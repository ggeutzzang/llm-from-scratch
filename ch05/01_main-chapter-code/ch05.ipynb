{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/llm-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45398736-7e89-4263-89c8-92153baff553",
      "metadata": {
        "id": "45398736-7e89-4263-89c8-92153baff553"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "세바스찬 라시카(Sebastian Raschka)가 쓴 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>의 번역서 예제 코드입니다.<br>\n",
        "<br>코드 저장소: <a href=\"https://github.com/rickiepark/llm-from-scratch\">https://github.com/rickiepark/llm-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
      "metadata": {
        "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
      },
      "source": [
        "# 5장: 레이블이 없는 데이터를 활용한 사전 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b989e9-da36-4159-b212-799184764dd9",
      "metadata": {
        "id": "92b989e9-da36-4159-b212-799184764dd9",
        "outputId": "315f9442-9407-44c4-f840-1d429eff91e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib version: 3.10.0\n",
            "numpy version: 2.0.2\n",
            "tiktoken version: 0.9.0\n",
            "torch version: 2.6.0+cu124\n",
            "tensorflow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"matplotlib\",\n",
        "        \"numpy\",\n",
        "        \"tiktoken\",\n",
        "        \"torch\",\n",
        "        \"tensorflow\" # OpenAI의 사전 훈련된 가중치를 위해서\n",
        "       ]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
      "metadata": {
        "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237"
      },
      "source": [
        "- 이 장에서 LLM을 사전 훈련하기 위해 훈련 루프를 구현하고 기본적인 모델 평가 방법을 알아 보겠습니다.\n",
        "- 이 장의 끝에서는 OpenAI의 사전 훈련된 가중치를 우리가 직접 구현한 모델에 로드해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
      "metadata": {
        "id": "efd27fcc-2886-47cb-b544-046c2c31f02a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d214765-7a73-42d5-95e9-302154b29db9",
      "metadata": {
        "id": "0d214765-7a73-42d5-95e9-302154b29db9"
      },
      "source": [
        "- 이 장에서 다루는 주제는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
      "metadata": {
        "id": "f67711d4-8391-4fee-aeef-07ea53dd5841"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
      "metadata": {
        "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
      },
      "source": [
        "## 5.1 텍스트 생성 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
      "metadata": {
        "id": "a3350f8c-5181-4f9b-a789-4523105e98f2"
      },
      "source": [
        "- 이전 장에 코드를 사용하여 GPT 모델을 초기화하는 방법을 간략히 정리합니다.\n",
        "- 그다음 LLM을 위한 기본적인 평가 지표를 소개합니다.\n",
        "- 이 절의 마지막에서 이 평가 지표를 훈련 세트와 검증 세트에 적용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
      "metadata": {
        "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
      },
      "source": [
        "### 5.1.1 GPT를 사용해 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
      "metadata": {
        "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc"
      },
      "source": [
        "- 이전 장의 코드를 사용하여 GPT 모델을 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 previous_chapters.py 파일을 다운로드합니다.\n",
        "!wget https://bit.ly/3HlFmc8 -O previous_chapters.py"
      ],
      "metadata": {
        "id": "oujnEbGCRVJe",
        "outputId": "31a6a031-5752-4cd8-b29c-6c99de90bbaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oujnEbGCRVJe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-02 06:13:24--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/previous_chapters.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9905 (9.7K) [text/plain]\n",
            "Saving to: ‘previous_chapters.py’\n",
            "\n",
            "previous_chapters.p 100%[===================>]   9.67K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-06-02 06:13:24 (14.5 MB/s) - ‘previous_chapters.py’ saved [9905/9905]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86000d74-624a-48f0-86da-f41926cb9e04",
      "metadata": {
        "id": "86000d74-624a-48f0-86da-f41926cb9e04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from previous_chapters import GPTModel\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # 어휘 사전 크기\n",
        "    \"context_length\": 256, # 짧은 문맥 길이 (원본 길이: 1024)\n",
        "    \"emb_dim\": 768,        # 임베딩 차원\n",
        "    \"n_heads\": 12,         # 어텐션 헤드 개수\n",
        "    \"n_layers\": 12,        # 층 개수\n",
        "    \"drop_rate\": 0.1,      # 드롭아웃 비율\n",
        "    \"qkv_bias\": False      # 쿼리-키-값 생성시 편향 사용 여부\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # 추론 시에는 드롭아웃을 비활성화합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
      "metadata": {
        "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c"
      },
      "source": [
        "- 위에서 드롭아웃을 0.1로 지정했지만 요즘에는 드롭아웃을 사용하지 않고 LLM을 훈련하는 경우가 많습니다.\n",
        "- 최신 LLM은 (초기 GPT 모델과 달리) 쿼리, 키, 값 행렬을 위한 `nn.Linear` 층에서 편향 벡터를 사용하지 않습니다. 그래서 `\"qkv_bias\": False`로 지정합니다.\n",
        "- 모델 훈련에 필요한 계산 자원을 절감하기 위해 문맥 길이(`context_length`)를 256 토큰으로 줄입니다. 원본 1억 2,400만 파라미터의 GPT-2 모델은 1024개의 토큰을 사용했습니다.\n",
        "  - 대부분의 독자들은 이 코드 예제를 랩탑 컴퓨터에서 실행하기 때문입니다.\n",
        "  - 하지만 `context_length`를 1,024개 토큰으로 늘려서 실험해도 괜찮습니다(어떤 코드도 바꿀 필요가 없습니다)\n",
        "  - 나중에 `context_length`가 1,024인 모델을 사전 훈련된 가중치에서 로드하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
      "metadata": {
        "id": "59f80895-be35-4bb5-81cb-f357ef7367fe"
      },
      "source": [
        "- 그 다음이 이전 장에서 만든 `generate_text_simple` 함수를 사용해 텍스트를 생성합니다.\n",
        "- 또한 두 개 유틸리티 함수 `text_to_token_ids`와 `token_ids_to_text`를 정의합니다. 이 장에서 토큰과 텍스트 표현 사이를 전환하는데 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
      "metadata": {
        "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=800px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
      "metadata": {
        "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
        "outputId": "a4c8a433-3cfe-4011-9725-443c7d3236b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "from previous_chapters import generate_text_simple\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # 배치 차원을 삭제합니다\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
      "metadata": {
        "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305"
      },
      "source": [
        "- 위해서 볼 수 있듯이 모델이 아직 훈련되지 않았기 때문에 좋은 텍스트를 생성하지 못합니다.\n",
        "- 훈련 과정에서 어떤 것이 좋은 텍스트인지 어떻게 정량적으로 측정할 수 있을까요?\n",
        "- 다음 절에서 훈련 과정을 모니터링할 수 있도록 생성된 출력의 손실을 계산하는 지표를 소개합니다.\n",
        "- LLM 미세 튜닝을 다루는 다음 장에서 모델의 품질을 측정하는 또 다른 방법을 소개하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
      "metadata": {
        "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
      },
      "source": [
        "### 5.1.2 텍스트 생성 손실 계산하기: 크로스 엔트로피와 혼잡도"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
      "metadata": {
        "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440"
      },
      "source": [
        "- 두 개의 훈련 샘플(행)에 대한 토큰 ID를 담고 있는 `inputs` 텐서가 있다고 가정해보죠.\n",
        "- `inputs`에 해당하는 `targets`은 모델이 생성 해야할 토큰 ID를 담고 있습니다.\n",
        "- 2장에서 데이터 로더를 구현할 때 설명했듯이 `targets`은 `inputs`에서 한 토큰씩 앞으로 이동한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
      "metadata": {
        "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
      "metadata": {
        "id": "33dc0645-ac2c-4973-9b40-6da40515bede"
      },
      "source": [
        "- `inputs`을 모델에 주입하면 각각 세 개의 토큰으로 구성된 두 개의 입력 샘플에 대한 로짓 벡터를 얻습니다.\n",
        "- 각각의 토큰는 어휘 사전 크기에 해당하는 50,257차원의 벡터입니다.\n",
        "- 소프트맥스 함수를 적용하여 로짓 텐서을 확률 점수를 담고 있는 동일 차원의 텐서로 바꿀 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
      "metadata": {
        "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
        "outputId": "5ff8d384-8253-400e-c6e5-e265c1371bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1) # 어휘 사전의 각 토큰에 대한 확률\n",
        "print(probas.shape) # 크기: (batch_size, num_tokens, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
      "metadata": {
        "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b"
      },
      "source": [
        "- 매우 작은 어휘 사전을 사용하는 아래 그림에서 확률 점수를 텍스트로 바꾸는 방법을 보여 줍니다. 이 장의 끝에서 이에 대해 논의하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
      "metadata": {
        "id": "384d86a9-0013-476c-bb6b-274fd5f20b29"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8480efd-d419-4954-9ecc-2876055334bd",
      "metadata": {
        "id": "e8480efd-d419-4954-9ecc-2876055334bd"
      },
      "source": [
        "- 이전 장에서 설명했듯이 `argmax` 함수를 적용하여 확률 점수를 토큰 ID 바꿀 수 있습니다.\n",
        "- 앞의 소프트맥스 함수는 각 토큰에 대해서 50,257 차원의 벡터를 생성합니다. `argmax` 함수는 이 벡터에서 가장 높은 확률을 가진 위치를 반환합니다. 이것이 주어진 토큰에 대한 예측 토큰의 아이디입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
      "metadata": {
        "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144"
      },
      "source": [
        "- 배치에는 각각 세 개 토큰으로 구성된 두 개의 입력 샘플이 있으므로 2x3 크기의 예측 토큰을 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
        "outputId": "ffd3291d-adc0-4125-c756-cfd3629b69a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n"
          ]
        }
      ],
      "source": [
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"토큰 ID:\\n\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee4072c-21ed-4df7-8721-dd2535362573",
      "metadata": {
        "id": "cee4072c-21ed-4df7-8721-dd2535362573"
      },
      "source": [
        "- 이 토큰을 디코딩하면 모델이 예측해야 할 토큰, 즉 타겟 토큰과 매우 다른 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
      "metadata": {
        "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
        "outputId": "c0659104-3292-44c9-cbf2-c7109aa707ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1:  Armed heNetflix\n"
          ]
        }
      ],
      "source": [
        "print(f\"첫 번째 샘플의 타깃: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"두 번째 샘플의 타깃: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
      "metadata": {
        "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2"
      },
      "source": [
        "- 이는 모델이 아직 훈련되지 않았기 때문입니다.\n",
        "- 모델을 훈련하려면 정답 예측(타깃)에서 얼만큼 떨어져 있는지 알아야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
      "metadata": {
        "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7251bf5-a079-4782-901d-68c9225d3157",
      "metadata": {
        "id": "c7251bf5-a079-4782-901d-68c9225d3157"
      },
      "source": [
        "- 타겟 인덱스에 해당하는 토큰 확률은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
        "outputId": "808cf759-21cd-48e9-ec0e-66f36fdc56d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
            "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
          ]
        }
      ],
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"텍스트 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"텍스트 2:\", target_probas_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
      "metadata": {
        "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf"
      },
      "source": [
        "- 확률이 1에 가까워지도록 이 값들을 최대화하는 것이 목표입니다.\n",
        "- 수학적 최적화에서는 확률 점수 자체를 최대화하는 것보다 확률 점수의 로그를 최대화하는 것이 쉽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
        "outputId": "02776db2-14a3-45d2-fb34-1e2fe66fd408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
          ]
        }
      ],
      "source": [
        "# 토큰 확률의 로그를 계산합니다.\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4261441-a511-4633-9c4c-67998af31b84",
      "metadata": {
        "id": "c4261441-a511-4633-9c4c-67998af31b84"
      },
      "source": [
        "- 그 다음 로그 확률의 평균을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b003797-161b-4d98-81dc-e68320e09fec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b003797-161b-4d98-81dc-e68320e09fec",
        "outputId": "d6e03b93-6273-4a93-d558-fd68a9744fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.7940)\n"
          ]
        }
      ],
      "source": [
        "# 각 토큰에 대한 평균 확률을 계산합니다.\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
      "metadata": {
        "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585"
      },
      "source": [
        "- 모델 가중치를 최적화하여 평균 로그 확률을 가능한 크게 만드는 것이 목표입니다.\n",
        "- 로그 때문에 가장 큰 가능한 값은 0이며, 현재는 0에서 부터 멀리 떨어져 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
      "metadata": {
        "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514"
      },
      "source": [
        "- 딥러닝 에서는 평균 로그 확률을 최대화하는 것 대신에 음의 평균 로그 확률을 최소화하는 것이 일반적입니다. 이 예제의 경우 -10.7722를 최대화하여 0에 가깝게 만드는 것 대신에 10.7722을 최소화하여 0에 가깝게 만듭니다.\n",
        "- -10.7722의 음수 값, 즉 10.7722을 딥러닝에서는 크로스 엔트로피 손실이라고 부릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
      "metadata": {
        "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
        "outputId": "7a5a500b-d21f-4cab-c946-a9802ded7377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7940)\n"
          ]
        }
      ],
      "source": [
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
      "metadata": {
        "id": "84eeb868-abd8-4028-82db-107546bf7c2c"
      },
      "source": [
        "- 파이토치는 이전 단계를 수행하는 `cross_entropy` 함수를 제공합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
      "metadata": {
        "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
      "metadata": {
        "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d"
      },
      "source": [
        "- `cross_entropy` 함수를 적용하기 전에 로짓과 타깃의 크기를 확인해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
        "outputId": "eab476fa-61f4-41e3-b16e-3f85441106d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# 로짓의 크기는 (batch_size, num_tokens, vocab_size)입니다.\n",
        "print(\"로짓 크기:\", logits.shape)\n",
        "\n",
        "# 타깃의 크기는 (batch_size, num_tokens)입니다.\n",
        "print(\"타깃 크기:\", targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
      "metadata": {
        "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06"
      },
      "source": [
        "- 파이토치의 `cross_entropy` 함수를 위해 배치 차원을 기준으로 합쳐서 텐서를 펼쳐야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
        "outputId": "aeb0699e-fb40-4678-af34-f21d9a11ed5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"펼친 로짓:\", logits_flat.shape)\n",
        "print(\"펼친 타깃:\", targets_flat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4921a57f-3a79-473e-a863-6d63b495010f",
      "metadata": {
        "id": "4921a57f-3a79-473e-a863-6d63b495010f"
      },
      "source": [
        "- 타깃은 토큰 ID이며, 최대화해야 할 로짓 텐서의 인덱스를 나타냅니다.\n",
        "- 파이토치의 `cross_entropy` 함수는 최대화할 토큰 인덱스에 대해 자동으로 소프트맥스와 로그 확률 계산을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
        "outputId": "7e12440b-4ed4-4c09-83da-62109132867f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7940)\n"
          ]
        }
      ],
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
      "metadata": {
        "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80"
      },
      "source": [
        "- 크로스 엔트로피 손실에 관련된 개념은 LLM의 혼잡도입니다.\n",
        "- 혼잡도는 크로스 엔트로피 손실에 지수 함수를 적용한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168952a1-b964-4aa7-8e49-966fa26add54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168952a1-b964-4aa7-8e49-966fa26add54",
        "outputId": "d7ccf3be-b888-4ce8-e44b-f82cbb2ed94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(48725.8203)\n"
          ]
        }
      ],
      "source": [
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
      "metadata": {
        "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7"
      },
      "source": [
        "- 혼잡도는 모델이 각 단계에서 불확실해하는 실제 어휘사전 크기를 나타내기 때문에 원시 손실 값보다 이해하기 더 쉽습니다(이 예에서는 48,725개 단어 또는 토큰).\n",
        "- 다른 말로 하면, 혼잡도는 모델이 예측한 확률 분포가 데이터셋에 있는 단어의 실제 분포와 얼마나 잘 맞는지를 측정합니다.\n",
        "- 손실과 비슷하게 낮은 혼잡도는 모델 예측이 실제 분포에 가깝다는 것을 나타냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
      "metadata": {
        "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
      },
      "source": [
        "### 5.1.3 훈련 세트와 검증 세트의 손실 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
      "metadata": {
        "id": "530da89e-2448-436c-8f1b-28e8a31ef85c"
      },
      "source": [
        "- LLM 훈련을 위해 비교적 작은 데이터셋을 사용합니다(사실 단편 소설 하나를 사용합니다).\n",
        "- 이유는 다음과 같습니다.\n",
        "  - 적절한 GPU가 없는 랩탑 컴퓨터에서 몇 분 안에 코드 예제가 실행되어야 합니다.\n",
        "  - 교육 목적을 위해 훈련이 비교적 빨리 끝나야 합니다(몇 주가 아니라 몇 분 만에).\n",
        "  - 사용 권리를 위반하지 않으며 깃허브 저장소에 저장할 수 있는 크기의 공개된 텍스트를 사용해야 합니다.\n",
        "- 예를 들어, Llama 2 7B는 2조 개의 토큰에서 훈련하기 위해 A100 GPU에서 184,320 GPU 시간이 필요합니다.\n",
        "  - 이 글을 쓰는 시점에, AWS의 8xA100 클라우드 서버의 시간당 가격은 약 \\$30입니다.\n",
        "  - 따라서 대략 계산하면 이 LLM을 훈련하는데 184,320 / 8 * \\$30 =  \\$690,000이 듭니다.\n",
        "- 아래에서는 2장에서 다루었던 데이터셋을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
      "metadata": {
        "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379330f1-80f4-4e34-8724-41d892b04cee",
      "metadata": {
        "id": "379330f1-80f4-4e34-8724-41d892b04cee"
      },
      "source": [
        "- 다운로드한 텍스트를 확인하기 위해 처음과 끝에서 100 개의 문자를 출력하여 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6kgJbe4ehI4q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kgJbe4ehI4q",
        "outputId": "ebc16efa-8dad-420d-afad-b4837ee85910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# 처음 100개 문자\n",
        "print(text_data[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2XPde_ThM_e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2XPde_ThM_e",
        "outputId": "de4e93a7-7087-497f-bf61-d8910a8134cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
          ]
        }
      ],
      "source": [
        "# 마지막 100개 문자\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
        "outputId": "49bcc08d-38bd-4de5-bf1d-94ebdbbce727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"문자:\", total_characters)\n",
        "print(\"토큰:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
      "metadata": {
        "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7"
      },
      "source": [
        "이 텍스트의 토큰이 5,145개 뿐이라 LLM을 훈련하기에 너무 적어 보일 수 있습니다. 하지만 이는 교육적인 목적을 위해서입니다(나중에 사전 훈련된 가중치를 로드하겠습니다)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
      "metadata": {
        "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f"
      },
      "source": [
        "- 그 다음 데이터셋을 훈련 세트와 검증 세트로 나누고 2장의 데이터 로더를 사용해 LLM 훈련을 위한 배치를 준비합니다.\n",
        "- 시각화때문에 아래 그림은 `max_length=6`라고 가정하지만, 훈련 데이터 로더에서 `max_length`는 LLM이 지원하는 문맥 길이와 같습니다.\n",
        "- 아래 그림은 간단하게 나타내려고 입력 토큰만 보여줍니다.\n",
        "  - LLM을 텍스트에 있는 다음 단어를 예측하도록 훈련하기 때문에 타깃은 한 토큰씩 이동한 것외에는 입력과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
      "metadata": {
        "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=800px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0959c855-f860-4358-8b98-bc654f047578",
      "metadata": {
        "id": "0959c855-f860-4358-8b98-bc654f047578"
      },
      "outputs": [],
      "source": [
        "from previous_chapters import create_dataloader_v1\n",
        "\n",
        "# 훈련 세트 비율\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
      "metadata": {
        "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e"
      },
      "outputs": [],
      "source": [
        "# 유효성 검사\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"훈련 데이터 로더에 토큰이 충분하지 않습니다. \"\n",
        "          \"`GPT_CONFIG_124M['context_length']`를 낮추거나 \"\n",
        "          \"`training_ratio`를 증가시키세요.\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"훈련 데이터 로더에 토큰이 충분하지 않습니다. \"\n",
        "          \"`GPT_CONFIG_124M['context_length']`를 낮추거나 \"\n",
        "          \"`training_ratio`를 증가시키세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
      "metadata": {
        "id": "e7ac3296-a4d1-4303-9ac5-376518960c33"
      },
      "source": [
        "- 컴퓨팅 자원을 절감하고 데이터셋이 매우 작기 때문에 비교적 작은 배치 크기를 사용합니다.\n",
        "- 예를 들어 Llama 2 7B는 배치 크기 1024에서 훈련되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
      "metadata": {
        "id": "a8e0514d-b990-4dc0-9afb-7721993284a0"
      },
      "source": [
        "- 데이터가 올바르게 로드되었는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
      "metadata": {
        "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
        "outputId": "fb5836a7-233e-4696-994a-ab04ea0d84f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "print(\"훈련 데이터 로더:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\n검증 데이터 로더:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
      "metadata": {
        "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed"
      },
      "source": [
        "- 토큰 크기가 예상 범위 안에 있는지 추가로 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb860488-5453-41d7-9870-23b723f742a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb860488-5453-41d7-9870-23b723f742a0",
        "outputId": "eaea1060-fdf9-4ed2-c6d2-4292454a50c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokens: 4608\n",
            "Validation tokens: 512\n",
            "All tokens: 5120\n"
          ]
        }
      ],
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"훈련 토큰 수:\", train_tokens)\n",
        "print(\"검증 토큰 수:\", val_tokens)\n",
        "print(\"모든 토큰 수:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
      "metadata": {
        "id": "5c3085e8-665e-48eb-bb41-cdde61537e06"
      },
      "source": [
        "- 주어진 배치에서 크로스 엔트로피 손실을 계산 하는 유틸리티 함수를 작성 합니다\n",
        "- 또한 데이터 로더에서 사용자가 지정한 배치 개수 만큼 추출하여 손실을 계산하는 두 번째 유틸리티 함수를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
      "metadata": {
        "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # num_batches가 데이터 로더에 있는 배치 개수보다 크면\n",
        "        # 배치 횟수를 데이터 로더에 있는 총 배치 개수로 맞춥니다.\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
      "metadata": {
        "id": "f0691332-84d0-48b3-b462-a885ddeb4fca"
      },
      "source": [
        "- CUDA를 지원하는 GPU를 가지고 있다면 어떤 코드도 변경할 필요 없이 GPU 에서 LLM을 훈련할 수 있습니다.\n",
        "- `device` 설정을 통해 데이터를 LLM 모델과 동일한 장치에 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
      "metadata": {
        "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
        "outputId": "61d847ff-712b-4fa8-c574-d039035d7755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583372328016\n",
            "Validation loss: 10.981104850769043\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 노트:\n",
        "# 애플 실리콘 칩에서 코드를 실행하는 경우 다음 주석을 해제하세요.\n",
        "# (M3 맥북 에어에서 측정하면) 애플 CPU보다 약 2배 빠릅니다. 하지만 손실 값은 조금 다를 수 있습니다.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # nn.Module classes의 경우 model = model.to(device)로 할당할 필요가 없습니다.\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # 데이터 로더에서 셔플링이 일어나므로 재현가능성을 위해 설정합니다.\n",
        "\n",
        "with torch.no_grad(): # 모델을 아직 훈련하지 않으므로 효율성을 위해 그레이디언트 추적을 끕니다.\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"훈련 손실:\", train_loss)\n",
        "print(\"검증 손실:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
      "metadata": {
        "id": "43875e95-190f-4b17-8f9a-35034ba649ec"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
      "metadata": {
        "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
      },
      "source": [
        "## 5.2 LLM 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
      "metadata": {
        "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd"
      },
      "source": [
        "- 이 절에서는 LLM 훈련을 위한 코드를 구현합니다.\n",
        "- 여기서는 간단한 훈련 함수를 만듭니다(이 훈련 함수를 학습률 워밍업, 코사인 어닐링, 그레이디언트 클리핑 같은 고급 기법으로 확장하고 싶다면 [부록 D](../../appendix-D/01_main-chapter-code)를 참고하세요).\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mtp4gY0ZO-qq",
      "metadata": {
        "id": "Mtp4gY0ZO-qq"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # 손실과 지금까지 처리한 토큰 수를 추적하기 위해 리스트를 초기화합니다.\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # 메인 훈련 루프를 시작합니다.\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # 모델을 훈련 모드로 설정합니다.\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # 이전 배치 반복에서 얻은 손실의 그레이디언트를 초기화합니다.\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # 손실의 그레이디언트를 계산합니다.\n",
        "            optimizer.step() # 손실의 그레이디언트를 사용하여 모델 가중치를 업데이트합니다.\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # 추가적인 평가 단계\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"에포크 {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"훈련 손실 {train_loss:.3f}, 검증 손실 {val_loss:.3f}\")\n",
        "\n",
        "        # 각 에포크 후에 샘플 텍스트를 출력합니다.\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # 간결한 출력 포맷을 위해\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
      "metadata": {
        "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47"
      },
      "source": [
        "- 위에 정의한 훈련 함수로 LLM을 훈련해 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3422000b-7aa2-485b-92df-99372cd22311",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3422000b-7aa2-485b-92df-99372cd22311",
        "outputId": "7d6c1b37-e2d6-4618-d556-aa9c34a9a0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.928\n",
            "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.335\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.622, Val loss 7.051\n",
            "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.600\n",
            "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
            "Ep 3 (Step 000020): Train loss 5.586, Val loss 6.477\n",
            "Ep 3 (Step 000025): Train loss 5.523, Val loss 6.399\n",
            "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
            "Ep 4 (Step 000030): Train loss 5.128, Val loss 6.366\n",
            "Ep 4 (Step 000035): Train loss 4.941, Val loss 6.366\n",
            "Every effort moves you a a to the a. Gisburn, and a. Gisburn. I had the of the of the of the of the of the of the of the of the of the of the of the of the of the. I had a\n",
            "Ep 5 (Step 000040): Train loss 4.340, Val loss 6.246\n",
            "Every effort moves you, with a, in the of the picture--as of the of the of the of the picture of his of the  \"I had been. \"Oh, in the donkey--and it was a little the man of the picture of\n",
            "Ep 6 (Step 000045): Train loss 3.967, Val loss 6.181\n",
            "Ep 6 (Step 000050): Train loss 3.451, Val loss 6.155\n",
            "Every effort moves you know the \"Oh, and.  \"Oh, and in a little: \"There, and in the    \"Oh, and I had been the donkey.            \n",
            "Ep 7 (Step 000055): Train loss 3.466, Val loss 6.195\n",
            "Ep 7 (Step 000060): Train loss 2.666, Val loss 6.134\n",
            "Every effort moves you know the picture.  \"I looked he was a little the last word.           \"I he was his pictures-c.             \n",
            "Ep 8 (Step 000065): Train loss 2.208, Val loss 6.141\n",
            "Ep 8 (Step 000070): Train loss 1.879, Val loss 6.228\n",
            "Every effort moves you know,\" was not that the picture.  \"I had the last word. Gisburn's an!  \"Oh, in the moment--as Jack himself, as he was his own the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 1.499, Val loss 6.230\n",
            "Ep 9 (Step 000080): Train loss 1.174, Val loss 6.250\n",
            "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fact, with a Mrs. Gisburn's open countenance. \"It's his pictures with a \"strongest,\" she was\n",
            "Ep 10 (Step 000085): Train loss 0.901, Val loss 6.328\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"         He placed them at my elbow and as I turned, and down the room, when I\n"
          ]
        }
      ],
      "source": [
        "# 노트:\n",
        "# 실행 시간을 계산하고 싶다면 다음 주석을 해제하세요.\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# 실행 시간을 계산하고 싶다면 다음 주석을 해제하세요.\n",
        "# end_time = time.time()\n",
        "# execution_time_minutes = (end_time - start_time) / 60\n",
        "# print(f\"훈련 소요 시간: {execution_time_minutes:.2f}분.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8b86f0-b07d-40d7-b9d3-a9218917f204",
      "metadata": {
        "id": "2e8b86f0-b07d-40d7-b9d3-a9218917f204"
      },
      "source": [
        "- 여러분의 결과와 손실 값이 조금 다를 수 있습니다. 대체적으로 비슷하다면 (훈련 손실은 1이하이고 검증 손실은 7이하이면) 걱정할 필요가 없습니다.\n",
        "- GPU 하드웨어와 CUDA 버전 또는 파이토치의 신버전에서 바뀐 변화 때문에 차이가 발생할 수 있습니다.\n",
        "- CPU에서 이 예제를 실행하더라도 작은 차이를 볼 수 있습니다. 이런 차이를 만드는 원인 중 하나는 파이토치가 컴파일된 운영체제에 따라 `nn.Dropout`의 동작 방식이 다르기 때문입니다. 자세한 내용은 파이토치 [깃허브 이슈](https://github.com/pytorch/pytorch/issues/121595)를 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0WSRu2i0iHJE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "0WSRu2i0iHJE",
        "outputId": "6d0fb7cc-d2fe-4ba0-82ec-8e2f5ddafd5a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # 에포크에 대한 훈련 손실과 검증 손실의 그래프를 그립니다.\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # 처리한 토큰 수에 대한 두 번째 x 축을 만듭니다.\n",
        "    ax2 = ax1.twiny()  # y 축을 공유하는 두 번째 x 축을 만듭니다.\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # 눈금을 정렬하기 위해 투명한 그래프를 만듭니다.\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
      "metadata": {
        "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995"
      },
      "source": [
        "- 위 결과를 보면 모델이 처음에는 이해할 수 없는 단어를 생성하지만 마지막으로 갈수록 문법적으로 어느 정도 정확한 문장을 생성합니다.\n",
        "- 하지만 훈련 세트 손실과 검증 세트 손실을 보면 모델이 과대적합되기 시작합니다.\n",
        "- 마지막 부분의 몇 문장을 확인하면 훈련 세트에 있는 내용이라는 것을 알 수 있습니다. 모델이 단순히 훈련 데이터를 암기한 것입니다.\n",
        "- 매우 작은 훈련 세트를 사용하고 모델을 여러 에포크에서 훈련하고 있기 때문에 과대적합이 일어납니다.\n",
        "  - 여기서는 교육적인 목적을 위해 LLM을 훈련합니다. 모델이 일관된 텍스트를 생성하는 방법을 학습할 수 있는지 확인하는 것이 주요 목적입니다.\n",
        "  - 대량의 고가 하드웨어에서 몇 주 또는 몇 달 동안 이런 모델을 훈련하는 대신에 나중에 사전 훈련된 가중치를 로드하여 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
      "metadata": {
        "id": "eb380c42-b31c-4ee1-b8b9-244094537272"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
      "metadata": {
        "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c"
      },
      "source": [
        "- 더 큰 훈련 데이터셋에서 오래 모델을 훈련하고 싶다면 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)을 참고하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
      "metadata": {
        "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
      },
      "source": [
        "## 5.3 무작위성을 제어하기 위한 디코딩 전략"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
      "metadata": {
        "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7"
      },
      "source": [
        "- 위에서 구현한 GPT 모델처럼 작은 LLM의 추론 비용은 비교적 저렴합니다. 따라서 훈련에 GPU를 사용했더라도 추론에서는 GPU를 사용할 필요가 없습니다.\n",
        "- 이전 장에서 만든 `generate_text_simple` 함수를 사용해 한 번에 하나의 단어(또는 토큰)씩 새로운 텍스트를 생성할 수 있습니다.\n",
        "- 5.1.2절에서 설명했듯이 생성된 다음 토큰은 어휘사전의 모든 토큰 중에서 확률 점수가 가장 높은 토큰입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
      "metadata": {
        "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
        "outputId": "4c65155d-b000-4dfd-dde0-70b4d3af736b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
          ]
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
      "metadata": {
        "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4"
      },
      "source": [
        "- `generate_text_simple` 함수를 여러번 실행하더라도 LLM은 항상 동일한 출력을 생성합니다.\n",
        "- `generate_text_simple`를 수정하기 위해 두 가지 디코딩 전략을 소개합니다. *온도 스케일링*과 *탑-k* 샘플링입니다.\n",
        "- 이를 사용해 모델이 생성된 텍스트의 무작위성과 다양성을 조절할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
      "metadata": {
        "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994"
      },
      "source": [
        "### 5.3.1 온도 스케일링"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
      "metadata": {
        "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa"
      },
      "source": [
        "- 이전에는 `torch.argmax`를 사용해 항상 가장 높은 확률을 가진 토큰을 다음 토큰으로 샘플링했습니다.\n",
        "- 다양성을 추가하기 위해 확률 분포에서 샘플링하도록 `torch.multinomial(probs, num_samples=1)`을 사용해 토큰을 샘플링할 수 있습니다.\n",
        "- 여기서 각 인덱스 선택 가능성은 입력 텐서에 있는 확률에 따라 결정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
      "metadata": {
        "id": "e7531bae-d5de-44c0-bc78-78fed077e22a"
      },
      "source": [
        "- 여기에서 다음 토큰 생성에 대해 간략히 정리해 보겠습니다. 설명을 위해 매우 작은 어휘사전을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
      "metadata": {
        "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
        "outputId": "d31c17a2-3d31-4026-e401-12e9d3d846f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward\n"
          ]
        }
      ],
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# 입력이 \"every effort moves you\"이고\n",
        "# LLM이 다음 토큰을 위해 아래와 같은 로짓을 반환했다고 가정해 보죠.\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "\n",
        "# 생성될 토큰은 다음과 같습니다.\n",
        "print(inverse_vocab[next_token_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
      "metadata": {
        "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
        "outputId": "8dd04ef7-7fb1-4014-e7a4-1e3c8f99d1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "print(inverse_vocab[next_token_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
      "metadata": {
        "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9"
      },
      "source": [
        "- `torch.argmax`로 가장 가능성이 높은 토큰을 결정하는 대신에 `torch.multinomial(probas, num_samples=1)`를 사용해 소프트맥스 분포에서 샘플링하여 가장 가능성이 높은 토큰을 결정할 수 있습니다.\n",
        "- 설명을 위해 원래 소프트맥스 분포에서 1,000번 토큰을 샘플링해 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
      "metadata": {
        "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
        "outputId": "741c6b1e-fdf3-4dda-c633-db28187a4e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "582 x forward\n",
            "2 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "343 x toward\n",
            "0 x you\n"
          ]
        }
      ],
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123) # 재현가능성을 위한 랜덤 시드\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
      "metadata": {
        "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832"
      },
      "source": [
        "- 온도 스케일링으로 분포와 선택 과정을 조절할 수 있습니다.\n",
        "- \"온도 스케일링\"은 로짓을 0보다 큰 숫자로 나누는 것을 의미합니다.\n",
        "- 1보다 큰 온도는 소프트맥스 함수를 적용한 후에 더 균등한 토큰 확률 분포를 만듭니다.\n",
        "- 1보다 작은 온도는 소프트맥스 함수를 적용한 후에 더 결정론적인 분포(더 날카롭거나 뾰족한 분포)를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
      "metadata": {
        "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d"
      },
      "outputs": [],
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "# 온도 값\n",
        "temperatures = [1, 0.1, 5]  # 원본, 낮은 온도, 높은 온도\n",
        "\n",
        "# 스케일을 조정한 확률 계산\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
      "metadata": {
        "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
        "outputId": "2fd2ffe3-1b03-4c21-fa26-bd42106782aa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 그래프 그리기\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"temperature-plot.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
      "metadata": {
        "id": "d750e989-842a-4cfa-a44b-cf44d6e49163"
      },
      "source": [
        "- 온도 0.1로 스케일을 조정하면 더 뾰족한 분포를 만들어, `torch.argmax`에 가까워져 항상 가장 가능성있는 토큰이 선택됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
      "metadata": {
        "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
        "outputId": "262092f0-b13b-4e6f-bd2e-d0f7dfb8bfcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "985 x forward\n",
            "0 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "15 x toward\n",
            "0 x you\n"
          ]
        }
      ],
      "source": [
        "print_sampled_tokens(scaled_probas[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
      "metadata": {
        "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b"
      },
      "source": [
        "- 온도 5로 스케일을 조정한 확률은 더 균등한 분포가 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
      "metadata": {
        "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
        "outputId": "1450c130-fd5c-4ce7-a155-a812ad7a9a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165 x closer\n",
            "75 x every\n",
            "42 x effort\n",
            "239 x forward\n",
            "71 x inches\n",
            "46 x moves\n",
            "32 x pizza\n",
            "227 x toward\n",
            "103 x you\n"
          ]
        }
      ],
      "source": [
        "print_sampled_tokens(scaled_probas[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
      "metadata": {
        "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7"
      },
      "source": [
        "- LLM 입력이 \"every effort moves you\"일 경우 위와 같은 방법을 사용하면 \"every effort moves you pizza\"와 같이 3.2%의 확률(1,000번 중에 32번)로 이따금 말이 안되는 텍스트를 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
      "metadata": {
        "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7"
      },
      "source": [
        "### 5.3.2 탑-k 샘플링"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
      "metadata": {
        "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df"
      },
      "source": [
        "- 높은 온도를 사용하여 출력의 다양성을 증가시키면서 말이 안되는 문장이 생성될 가능성을 낮추기 위해 가장 가능성있는 상위 k개 토큰으로 샘플링될 토큰을 제한할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
      "metadata": {
        "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=700px>\n",
        "\n",
        "- (이 그림의 숫자는 간단하게 나타내려고 소숫점 두자리 이후를 자른 값입니다. 소프트맥스 열의 값은 모두 더해서 1.0이 되어야 합니다)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
      "metadata": {
        "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c"
      },
      "source": [
        "- 코드로는 다음과 같이 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
      "metadata": {
        "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
        "outputId": "9db98281-70cc-4130-d245-3c814731f8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
            "Top positions: tensor([3, 7, 0])\n"
          ]
        }
      ],
      "source": [
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "print(\"탑-k 로짓:\", top_logits)\n",
        "print(\"탑-k 위치:\", top_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
      "metadata": {
        "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
        "outputId": "066282e6-1032-4bd8-8111-d282bffbc3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
          ]
        }
      ],
      "source": [
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits\n",
        ")\n",
        "\n",
        "print(new_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa6fa49-6e99-459d-a517-d7d0f51c4f00",
      "metadata": {
        "id": "dfa6fa49-6e99-459d-a517-d7d0f51c4f00"
      },
      "source": [
        "> 노트:  \n",
        ">\n",
        ">  이전 코드를 조금 더 효율적으로 구현하는 방법은 다음과 같습니다.\n",
        ">\n",
        "> ```python\n",
        "> new_logits = torch.full_like( # -inf 값을 담은 텐서를 만듭니다.\n",
        ">    next_token_logits, -torch.inf\n",
        ">)   \n",
        "> new_logits[top_pos] = next_token_logits[top_pos] # -inf 텐서에 상위 k개 값을 복사합니다.\n",
        "> ```\n",
        "> <br>\n",
        "> 자세한 내용은 다음을 참고하세요: https://github.com/rasbt/LLMs-from-scratch/discussions/326\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
      "metadata": {
        "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
        "outputId": "9ca21fc2-7eff-4907-f432-a39c4a13008f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(topk_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
      "metadata": {
        "id": "56056503-a15d-4315-a3ff-46647a4c7c45"
      },
      "source": [
        "### 5.3.3 텍스트 생성 함수 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34770423-473d-46f6-a5fa-6b2979564d26",
      "metadata": {
        "id": "34770423-473d-46f6-a5fa-6b2979564d26"
      },
      "source": [
        "- 이전 두 개의 절에서 온도 스케일링과 탑-k 샘플링을 소개했습니다.\n",
        "- 두 개념을 사용해 앞서 LLM으로 텍스트를 생성할 때 사용한 `generate_sample` 함수를 수정해 새로운 `generate` 함수를 만들어 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
      "metadata": {
        "id": "8e318891-bcc0-4d71-b147-33ce55febfa3"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # for 루프는 이전과 동일합니다. 로짓을 받아 마지막 타임 스텝만 사용합니다.\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # 탑-k 샘플링으로 로짓을 필터링합니다.\n",
        "        if top_k is not None:\n",
        "            # 탑-k 값만 유지합니다.\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # 온도 스케일링을 적용합니다.\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # 소프트맥스 함수를 적용하여 확률을 얻습니다.\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # 분포에서 샘플링합니다.\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # 온도 스케일링을 사용하지 않는 경우 이전처럼 그리디 샘플링을 사용해 다음 토큰을 선택합니다.\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # eos_id가 지정되어 있고 EoS 토큰을 만나면 생성을 중단합니다.\n",
        "            break\n",
        "\n",
        "        # 이전과 동일하게 샘플링된 인덱스를 현재 시퀀스 뒤에 추가합니다.\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
      "metadata": {
        "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
        "outputId": "be1affd1-d079-40d3-d8f1-3ab647a6e0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
      "metadata": {
        "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b"
      },
      "source": [
        "## 5.4 파이토치로 모델 로드하고 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc52676-f026-4566-a226-2a90269f9d53",
      "metadata": {
        "id": "0fc52676-f026-4566-a226-2a90269f9d53"
      },
      "source": [
        "- LLM 훈련에는 계산 비용이 많이 듭니다. 따라서 LLM 가중치를 저장하고 로드하는 것이 중요합니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
      "metadata": {
        "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82"
      },
      "source": [
        "- 파이토치에서는 `torch.save` 함수를 `.state_dict()` 메서드 결과에 적용해 소위 `state_dict`인 모델 가중치를 저장하는 것이 권장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
      "metadata": {
        "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
      "metadata": {
        "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e"
      },
      "source": [
        "- 그다음 모델 가중치를 새로운 `GPTModel` 클래스 인스턴스에 로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
      "metadata": {
        "id": "9d57d914-60a3-47f1-b499-5352f4c457cb"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
      "metadata": {
        "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1"
      },
      "source": [
        "- 일반적인 SGD 대신 Adam이나 AdamW와 같이 적응형 옵티마이저로 LLM을 훈련하는 것이 일반적입니다.\n",
        "- 이런 적응형 옵티마이저는 모델 가중치마다 추가적인 파라미터를 저장합니다. 나중에 사전 훈련을 계속하려면 이 파라미터도 저장하는 것이 맞습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
      "metadata": {
        "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0c7295-c822-43bf-9286-c45abc542868",
      "metadata": {
        "id": "8a0c7295-c822-43bf-9286-c45abc542868"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4194350e-0409-4a63-8ffd-d3a896509032",
      "metadata": {
        "id": "4194350e-0409-4a63-8ffd-d3a896509032"
      },
      "source": [
        "## 5.5 오픈AI에서 사전 훈련된 가중치 로드하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
      "metadata": {
        "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec"
      },
      "source": [
        "- 앞서 하나의 단편 소설로 구성된 작은 데이터셋을 사용해 소규모 GPT-2 모델을 훈련했습니다.\n",
        "- 구텐베르크 프로젝트에 있는 전체 책으로 더 오래 모델을 훈련하고 싶다면 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)을 참고하세요.\n",
        "- 다행히 오픈AI는 GPT-2 모델의 가중치를 공개적으로 제공하기 때문에 대규모 말뭉치에서 모델을 재훈련하기 위해 수만에서 수십만 달러를 쓸 필요가 없습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
      "metadata": {
        "id": "127ddbdb-3878-4669-9a39-d231fbdfb834"
      },
      "source": [
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "⚠️ **노트: 일부 사용자는 특히 윈도 운영체제에서 텐서플로의 호환성 때문에 문제가 발생할 수 있습니다. 원본 OpenAI GPT-2 가중치 파일을 로드하기 위해 텐서플로가 필요하며 그다음 파이토치로 변환합니다.\n",
        "텐서플로 관련 이슈가 발생한다면 이 절의 남은 코드를 실행하는 대신 아래 코드를 사용할 수 있습니다.\n",
        "아래 코드는 이전 절에 설명된 변환 과정을 사용해 미리 파이토치용으로 바꾼 가중치를 사용합니다. 자세한 내용은 다음 노트북을 참고하세요. [../02_alternative_weight_loading/weight-loading-pytorch.ipynb](../02_alternative_weight_loading/weight-loading-pytorch.ipynb).**\n",
        "\n",
        "```python\n",
        "file_name = \"gpt2-small-124M.pth\"\n",
        "# file_name = \"gpt2-medium-355M.pth\"\n",
        "# file_name = \"gpt2-large-774M.pth\"\n",
        "# file_name = \"gpt2-xl-1558M.pth\"\n",
        "\n",
        "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    urllib.request.urlretrieve(url, file_name)\n",
        "    print(f\"다운로드 파일: {file_name}\")\n",
        "\n",
        "gpt = GPTModel(BASE_CONFIG)\n",
        "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
        "gpt.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpt.to(device);\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cab892-a165-4f43-9601-f517bc212ab6",
      "metadata": {
        "id": "75cab892-a165-4f43-9601-f517bc212ab6"
      },
      "source": [
        "- 먼저 OpenAI에서 파일을 다운로드하고 파이썬으로 가중치를 로드하는 코드가 필요합니다.\n",
        "- OpenAI가 [텐서플로](https://www.tensorflow.org/)를 사용했기 때문에 가중치를 로드하기 위해 텐서플로를 설치하고 사용해야 합니다. [tqdm](https://github.com/tqdm/tqdm)은 진행 표시줄을 나타내기 위한 라이브러리입니다.\n",
        "- 필요한 라이브러리를 설치하려면 다음 코드의 주석을 제거하고 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
      "metadata": {
        "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8"
      },
      "outputs": [],
      "source": [
        "# pip install tensorflow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
      "metadata": {
        "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
        "outputId": "ceb2236f-a0b1-4d74-99d8-644895c8e558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "print(\"텐서플로 버전:\", version(\"tensorflow\"))\n",
        "print(\"tqdm 버전:\", version(\"tqdm\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bit.ly/4kSEn1v -O gpt_download.py"
      ],
      "metadata": {
        "id": "dvJjVQPWpg6B"
      },
      "id": "dvJjVQPWpg6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
      "metadata": {
        "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed"
      },
      "outputs": [],
      "source": [
        "from gpt_download import download_and_load_gpt2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
      "metadata": {
        "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc"
      },
      "source": [
        "---\n",
        "\n",
        "**노트**\n",
        "\n",
        "- 드물게 텐서플로 설치 이슈 때문에 위 코드 셀에서 `zsh: illegal hardware instruction python` 오류가 발생할 수 있습니다.\n",
        "- 이 문제를 해결하기 위해 `conda`를 사용해 텐서플로를 설치하는 방법은 [깃허브 이슈](https://github.com/rasbt/LLMs-from-scratch/discussions/273#discussioncomment-12367888)를 참고하세요.\n",
        "- conda 사용에 관한 추가적인 내용은 [Python setup tutorial](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences#option-2-using-conda)을 참고하세요.\n",
        "\n",
        "---\n",
        "\n",
        "- 다음처럼 1억 2,400만 파라미터를 가진 모델의 가중치를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
      "metadata": {
        "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
        "outputId": "a392c72d-7659-4a51-984f-cf2b51b13155"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "checkpoint: 100%|████████████████████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 63.1kiB/s]\n",
            "encoder.json: 100%|████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 4.69MiB/s]\n",
            "hparams.json: 100%|██████████████████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 59.7kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|████████████████████████████████████████████████████████████| 498M/498M [01:09<00:00, 7.15MiB/s]\n",
            "model.ckpt.index: 100%|████████████████████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 2.32MiB/s]\n",
            "model.ckpt.meta: 100%|███████████████████████████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 2.19MiB/s]\n",
            "vocab.bpe: 100%|█████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.47MiB/s]\n"
          ]
        }
      ],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
      "metadata": {
        "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
        "outputId": "6d08d181-ce12-413f-d526-af78ba49e826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
          ]
        }
      ],
      "source": [
        "print(\"설정:\", settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
      "metadata": {
        "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
        "outputId": "f7309d8d-c6b1-4e74-90d2-9c2ce0b3f300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "print(\"파라미터 딕셔너리 키:\", params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
      "metadata": {
        "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
        "outputId": "b1991170-38cc-454a-81d7-88021f8a281a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"토큰 임베딩 가중치 텐서의 차원:\", params[\"wte\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
      "metadata": {
        "id": "466e100c-294e-4afc-a70a-2f398ac4c104"
      },
      "source": [
        "- `model_size` 매개변수에 \"355M\", \"774M\", \"1558M\"도 지정할 수 있습니다.\n",
        "- 이런 모델의 차이점은 다음 그림에 요약되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
      "metadata": {
        "id": "20f19d32-5aae-4176-9f86-f391672c8f0d"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=700px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
      "metadata": {
        "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41"
      },
      "source": [
        "- 위에서 1억 2,400만 파라미터의 GPT-2 모델 가중치를 파이썬으로 로드했습니다. 이제 `GPTModel` 클래스의 인스턴스로 복사해야 합니다.\n",
        "- 먼저, 새로운 `GPTModel` 인스턴스를 초기화합니다.\n",
        "- 원본 GPT 모델은 멀티 헤드 어텐션 모듈의 쿼리, 키, 값 행렬을 위한 선형 층에서 편향 벡터를 사용합니다. 이것이 필수적이지는 않지만 사전 훈련된 가중치를 제대로 로드하기 위해서 `qkv_bias`를 `True`로 설정해야 합니다.\n",
        "- 또한 원본 GPT-2 모델에서 사용한 문맥 크기인 `1024` 토큰을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
      "metadata": {
        "id": "9fef90dd-0654-4667-844f-08e28339ef7d"
      },
      "outputs": [],
      "source": [
        "# 딕셔너리로 모델 설정을 저장합니다.\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# 기본 설정을 특정 값으로 업데이트합니다.\n",
        "model_name = \"gpt2-small (124M)\"  # 모델 이름\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
      "metadata": {
        "id": "272f29ac-8342-4b3d-a57d-9b0166ced314"
      },
      "source": [
        "- 다음 작업은 OpenAI 가중치를 `GPTModel` 인스턴스에 있는 가중치 텐서에 할당하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
      "metadata": {
        "id": "f9a92229-c002-49a6-8cfb-248297ad8296"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"크기가 다릅니다. left: {left.shape}, right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
      "metadata": {
        "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
      "metadata": {
        "id": "4f7472cb-54dc-4311-96d8-b2694f885cee"
      },
      "source": [
        "- 모델이 올바르게 로드되었다면 `generate` 함수를 사용해 새로운 텍스트를 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
      "metadata": {
        "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
        "outputId": "c1c2db4c-d451-4849-84a6-388f21e67c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you toward finding an ideal new way to practice something!\n",
            "\n",
            "What makes us want to be on top of that?\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
      "metadata": {
        "id": "6d079f98-a7c4-462e-8416-5a64f670861c"
      },
      "source": [
        "- 모델이 일관성 있는 텍스트를 생성했기 때문에 가중치가 올바르게 로드되었다고 확신할 수 있습니다. 이 과정에서 조금만 잘못되어도 모델이 제대로 텍스트를 생성하지 못합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
      "metadata": {
        "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44"
      },
      "source": [
        "- 허깅 페이스 허브에서 가중치를 로드하는 방법은 [../02_alternative_weight_loading](../02_alternative_weight_loading)에 있는 노트북을 참고하세요.\n",
        "- GPT 구조와 (메타에서 개발한) Llama 구조를 비교해 보려면 보너스 콘텐츠 [../07_gpt_to_llama](../07_gpt_to_llama)를 참고하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
      "metadata": {
        "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4"
      },
      "source": [
        "## 요약"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
      "metadata": {
        "id": "fc7ed189-a633-458c-bf12-4f70b42684b8"
      },
      "source": [
        "- [./gpt_train.py](./gpt_train.py)는 훈련 스크립트 파일입니다.\n",
        "- [./gpt_generate.py](./gpt_generate.py)는 OpenAI에서 사전 훈련된 가중치를 로드하여 프롬프트를 기반으로 텍스트를 생성합니다.\n",
        "- 연습문제 솔루션은 [./exercise-solutions.ipynb](./exercise-solutions.ipynb)에 있습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}