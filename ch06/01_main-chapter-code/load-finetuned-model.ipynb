{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/llm-from-scratch/blob/main/ch06/01_main-chapter-code/load-finetuned-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1545a16b-bc8d-4e49-b9a6-db6631e7483d",
      "metadata": {
        "id": "1545a16b-bc8d-4e49-b9a6-db6631e7483d"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "세바스찬 라시카(Sebastian Raschka)가 쓴 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>의 번역서 예제 코드입니다.<br>\n",
        "<br>코드 저장소: <a href=\"https://github.com/rickiepark/llm-from-scratch\">https://github.com/rickiepark/llm-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f83194-82b9-4478-9550-5ad793467bd0",
      "metadata": {
        "id": "f3f83194-82b9-4478-9550-5ad793467bd0"
      },
      "source": [
        "# 미세 튜닝된 모델 로딩 및 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e",
      "metadata": {
        "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e"
      },
      "source": [
        "이 노트북에는 6장의 [ch06.ipynb](ch06.ipynb)를 통해 생성 및 저장된 미세 튜닝된 모델을 로드하는 최소한의 코드가 포함되어 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
        "outputId": "f6eab753-5eae-4054-b71e-7322edc110c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken 버전: 0.9.0\n",
            "torch 버전: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"tiktoken\",    # 토크나이저\n",
        "    \"torch\",       # 딥러닝 라이브러리\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} 버전: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201",
      "metadata": {
        "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "finetuned_model_path = Path(\"review_classifier.pth\")\n",
        "if not finetuned_model_path.exists():\n",
        "    print(\n",
        "        f\"'{finetuned_model_path}'을(를) 찾을 수 없습니다.\\n\"\n",
        "        \"`ch06.ipynb` 노트북을 실행하여 미세 튜닝한 모델을 저장하세요.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bit.ly/4esl8dj -O previous_chapters.py"
      ],
      "metadata": {
        "id": "_EeGg79aYLaO",
        "outputId": "7ab2acca-3c37-47b8-f19c-831d643435ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_EeGg79aYLaO",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 04:14:13--  https://bit.ly/4esl8dj\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch06/01_main-chapter-code/previous_chapters.py [following]\n",
            "--2025-06-19 04:14:13--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch06/01_main-chapter-code/previous_chapters.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12067 (12K) [text/plain]\n",
            "Saving to: ‘previous_chapters.py’\n",
            "\n",
            "previous_chapters.p 100%[===================>]  11.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-19 04:14:13 (44.0 MB/s) - ‘previous_chapters.py’ saved [12067/12067]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb02584a-5e31-45d5-8377-794876907bc6",
      "metadata": {
        "id": "fb02584a-5e31-45d5-8377-794876907bc6"
      },
      "outputs": [],
      "source": [
        "from previous_chapters import GPTModel\n",
        "\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # 어휘사전 크기\n",
        "    \"context_length\": 1024,  # 문맥 길이\n",
        "    \"drop_rate\": 0.0,        # 드롭아웃 비율\n",
        "    \"qkv_bias\": True         # 쿼리-키-값 편향\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "# 기본 모델 초기화\n",
        "model = GPTModel(BASE_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94",
      "metadata": {
        "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 모델을 6.5절과 같이 분류기로 변환합니다.\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "\n",
        "# 그런 다음 사전 훈련된 가중치를 로드합니다.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"review_classifier.pth\", map_location=device, weights_only=True))\n",
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5",
      "metadata": {
        "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2",
      "metadata": {
        "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2"
      },
      "outputs": [],
      "source": [
        "# 이 함수는 ch06.ipynb에서 구현되었습니다.\n",
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # 모델에 대한 입력 준비\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # 너무 긴 시퀀스 자르기\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # 가장 긴 시퀀스에 맞게 패딩하기\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # 배치 차원 추가\n",
        "\n",
        "    # 모델 추론\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor.to(device))[:, -1, :]  # 마지막 출력 토큰의 로짓\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # 분류 결과 반환\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
        "outputId": "39a057b2-b562-40d7-fb4a-e8491652b95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ],
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=120\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "78472e05-cb4e-4ec4-82e8-23777aa90cf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78472e05-cb4e-4ec4-82e8-23777aa90cf8",
        "outputId": "c43a9e05-c09e-40ef-a96d-c5a51d990456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=120\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}