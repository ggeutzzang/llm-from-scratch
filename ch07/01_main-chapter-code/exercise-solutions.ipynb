{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/llm-from-scratch/blob/main/ch07/01_main-chapter-code/exercise-solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
      "metadata": {
        "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "세바스찬 라시카(Sebastian Raschka)가 쓴 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>의 번역서 예제 코드입니다.<br>\n",
        "<br>코드 저장소: <a href=\"https://github.com/rickiepark/llm-from-scratch\">https://github.com/rickiepark/llm-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
      "metadata": {
        "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14"
      },
      "source": [
        "# 7장 연습문제 솔루션\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
      "metadata": {
        "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c"
      },
      "source": [
        "## 연습문제 7.1: 프롬프트 스타일 변경\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be25a95-2a33-433b-a698-2365b5fc9357",
      "metadata": {
        "id": "6be25a95-2a33-433b-a698-2365b5fc9357"
      },
      "source": [
        "다음과 같은 데이터 항목이 있다고 가정해 보겠습니다.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Identify the correct spelling of the following word.\",\n",
        "  \"input\": \"Ocassion\",\n",
        "  \"output\": \"The correct spelling is 'Occasion.'\"\n",
        "}\n",
        "```\n",
        "\n",
        "본문에서는 Alpaca 스타일 프롬프트 템플릿에 따라 다음과 같이 형식을 지정했습니다.\n",
        "\n",
        "```\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the correct spelling of the following word.\n",
        "\n",
        "### Input:\n",
        "Occassion\n",
        "\n",
        "### Response:\n",
        "The correct spelling is 'Occasion.'\n",
        "```\n",
        "\n",
        "이 연습문제에서는 Phi-3 프롬프트 템플릿을 사용합니다. 이 템플릿은 데이터 항목의 형식을 다음과 같이 지정합니다.\n",
        "\n",
        "```\n",
        "<user>\n",
        "Identify the correct spelling of the following word: 'Occasion'\n",
        "\n",
        "<assistant>\n",
        "The correct spelling is 'Occasion'.\n",
        "```\n",
        "\n",
        "이 프롬프트 템플릿은 상당히 짧아 입력 프롬프트가 짧아지므로 LLM 미세 ㅌ닝 및 텍스트 생성을 위한 런타임 및 하드웨어 요구 사항이 줄어듭니다.\n",
        "이 변경을 위해 `format_input` 함수를 다음과 같이 업데이트합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a",
      "metadata": {
        "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"<|user|>\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50",
      "metadata": {
        "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50"
      },
      "source": [
        "두 개의 입력 샘플(하나는 `'input'` 필드에 내용이 있고 하나는 없는 샘플)에 적용하여 의도한 대로 작동하는지 확인해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "877a57e2-535f-4363-b32a-a093edd951b8",
      "metadata": {
        "id": "877a57e2-535f-4363-b32a-a093edd951b8",
        "outputId": "0bec744e-18fb-43d0-b55d-1cc4a4c4fb5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "Identify the correct spelling of the following word.\n",
            "Ocassion\n",
            "\n",
            "<|user|>\n",
            "What is an antonym of 'complicated'?\n"
          ]
        }
      ],
      "source": [
        "sample_data = [\n",
        "    {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"},\n",
        "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n",
        "]\n",
        "\n",
        "print(format_input(sample_data[0]))\n",
        "print()\n",
        "print(format_input(sample_data[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3",
      "metadata": {
        "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3"
      },
      "source": [
        "다음으로, 응답에 <|assistant|> 프롬프트 템플릿을 사용하도록 `InstructionDataset` 클래스도 업데이트합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f0d9c8-8f41-4455-b9ae-6b17de610cc3",
      "metadata": {
        "id": "81f0d9c8-8f41-4455-b9ae-6b17de610cc3"
      },
      "source": [
        "```python\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # 텍스트 토큰화\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "\n",
        "            ###################################################################\n",
        "            # 추가: `format_input_phi` 사용 및 응답 텍스트 템플릿 조정\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n<|assistant|>:\\n{entry['output']}\"\n",
        "            ###################################################################\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0650926-c39f-4442-8116-cb7494416f28",
      "metadata": {
        "id": "e0650926-c39f-4442-8116-cb7494416f28"
      },
      "source": [
        "마지막으로 테스트 세트 응답을 수집할 때 생성된 응답을 추출하는 방식도 업데이트해야 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb",
      "metadata": {
        "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb"
      },
      "source": [
        "```python\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "    tokenizer=tokenizer\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    # 추가: ###Response -> <|assistant|> 로 조정\n",
        "    response_text = generated_text[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cd557c-3838-45e4-a26a-baed4b11175a",
      "metadata": {
        "id": "29cd557c-3838-45e4-a26a-baed4b11175a"
      },
      "source": [
        "편의상 연습문제 솔루션은 [exercise_experiments.py](exercise_experiments.py) 스크립트에 구현되어 있으며, 다음과 같이 실행할 수 있습니다:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030",
      "metadata": {
        "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030"
      },
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution phi3_prompt\n",
        "```\n",
        "\n",
        "출력:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "Training set length: 935\n",
        "Validation set length: 55\n",
        "Test set length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded model: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   Training loss: 3.71630220413208\n",
        "   Validation loss: 3.6440994262695314\n",
        "Ep 1 (Step 000000): Train loss 2.633, Val loss 2.622\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.424, Val loss 0.928\n",
        "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.' <|assistant|>: The meal is prepared every day by the chef....\n",
        "Training completed in 1.50 minutes.\n",
        "Plot saved as loss-plot-phi3-prompt.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [00:11<00:00,  9.27it/s]\n",
        "Responses saved as instruction-data-with-response-phi3-prompt.json\n",
        "Model saved as gpt2-medium355M-sft-phi3-prompt.pth\n",
        "```\n",
        "\n",
        "비교를 위해, 원래 7장의 미세 튜닝 코드를 `python exercise_experiments.py --exercise_solution baseline`을 통해 실행할 수 있습니다.\n",
        "\n",
        "Nvidia L4 GPU에서 위의 코드는 Phi-3 프롬프트 템플릿을 사용하여 실행하는 데 1.5분이 걸립니다. 반면 Alpaca 스타일 템플릿은 실행하는 데 1.80분이 걸립니다. Phi-3 템플릿이 더 짧은 모델 입력을 생성하므로 약 17% 더 빠릅니다.\n",
        "\n",
        "응답이 올바르게 포맷팅되었는지 확인하기 위해 몇 가지 응답을 살펴보겠습니다.\n",
        "\n",
        "```json\n",
        "    {\n",
        "        \"instruction\": \"Rewrite the sentence using a simile.\",\n",
        "        \"input\": \"The car is very fast.\",\n",
        "        \"output\": \"The car is as fast as lightning.\",\n",
        "        \"model_response\": \"The car is as fast as a cheetah.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What type of cloud is typically associated with thunderstorms?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The type of cloud typically associated with thunderstorms is cumulonimbus.\",\n",
        "        \"model_response\": \"The type of cloud associated with thunderstorms is a cumulus cloud.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Name the author of 'Pride and Prejudice'.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"Jane Austen.\",\n",
        "        \"model_response\": \"The author of 'Pride and Prejudice' is Jane Austen.\"\n",
        "    },\n",
        "```\n",
        "\n",
        "Ollama Llama 3 방법을 사용하여 성능을 평가할 수 있습니다. 이 방법은 편의를 위해 `python exercise_experiments.py` 스크립트에도 구현되어 있으며 다음과 같이 실행할 수 있습니다.\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-phi3-prompt.json\n",
        "```\n",
        "\n",
        "출력:\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|████████████████████████| 110/110 [01:08<00:00,  1.60it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 48.87\n",
        "```\n",
        "\n",
        "점수는 50에 가깝습니다. 이는 이전에 Alpaca 스타일 프롬프트로 달성한 점수와 비슷합니다.\n",
        "\n",
        "Phi 프롬프트 스타일이 더 나은 본질적인 이점이나 근거는 없지만, 아래 *팁* 섹션에서 언급한 주의 사항을 제외하고는 더 간결하고 효율적일 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156bc574-3f3e-4479-8f58-c8c8c472416e",
      "metadata": {
        "id": "156bc574-3f3e-4479-8f58-c8c8c472416e"
      },
      "source": [
        "#### 팁: 특수 토큰 고려하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65cacf90-21c2-48f2-8f21-5c0c86749ff2",
      "metadata": {
        "id": "65cacf90-21c2-48f2-8f21-5c0c86749ff2"
      },
      "source": [
        "- Phi-3 프롬프트 템플릿에는 `<|user|>` 및 `<|assistant|>`와 같은 특수 토큰이 포함되어 있는데, 이는 GPT-2 토크나이저에 최적이 아닐 수 있습니다.\n",
        "- GPT-2 토크나이저는 `<|endoftext|>`를 특수 토큰(토큰 ID 50256으로 인코딩됨)으로 인식하지만, 앞서 언급한 것과 같은 다른 특수 토큰을 처리하는 데는 비효율적입니다.\n",
        "- 예를 들어, `<|user|>`는 5개의 개별 토큰 ID(27, 91, 7220, 91, 29)로 인코딩되는데, 이는 매우 비효율적입니다.\n",
        "- `tiktoken`에서 `allowed_special` 인수를 통해 `<|user|>`를 새로운 특수 토큰으로 추가할 수 있지만, GPT-2 어휘사전을 수정하지 않고는 이를 처리할 수 없다는 점을 유의하세요.\n",
        "- 토크나이저와 LLM이 특수 토큰을 처리하도록 확장하는 방법에 대해 궁금하다면, [extend-tiktoken.ipynb](../../ch05/09_extending-tokenizers/extend-tiktoken.ipynb) 보너스 자료를 참조하세요. (여기서는 필요하지 않지만, 호기심 많은 독자를 위한 내용입니다.)\n",
        "- 또한, 어휘사전을 통해 프롬프트 템플릿의 특수 토큰을 지원하는 모델은 더 효율적이고 전반적으로 더 나은 성능을 보일 수 있다고 가정할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
      "metadata": {
        "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e"
      },
      "source": [
        "&nbsp;\n",
        "## 연습문제 7.2: 명령어 및 입력 마스킹\n",
        "\n",
        "다음 그림과 같이 명령어를 마스킹하려면 `InstructionDataset` 클래스와 `custom_collate_fn`을 약간 수정해야 합니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp\" width=800px>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4405196a-db81-470b-be39-167a059587b6",
      "metadata": {
        "id": "4405196a-db81-470b-be39-167a059587b6"
      },
      "outputs": [],
      "source": [
        "# 이 `format_input` 함수는 원래 7장 코드에서 복사되었습니다.\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83658c09-af8a-425a-b940-eb1f06e43c0b",
      "metadata": {
        "id": "83658c09-af8a-425a-b940-eb1f06e43c0b"
      },
      "source": [
        "`InstructionDataset` 클래스를 수정하여 지시의 길이를 수집할 수 있습니다. 이 길이는 콜레이트 함수를 코딩할 때 타깃에서 지시 내용 위치를 찾는 데 사용됩니다. 다음과 같습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0",
      "metadata": {
        "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        ##########################################################################################\n",
        "        # 추가: 지시 길이를 위한 별도의 리스트\n",
        "        self.instruction_lengths = []\n",
        "        ##########################################################################################\n",
        "\n",
        "        self.encoded_texts = []\n",
        "\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "            ##########################################################################################\n",
        "            # 추가: 지시 길이 수집\n",
        "            instruction_length = len(tokenizer.encode(instruction_plus_input))\n",
        "            self.instruction_lengths.append(instruction_length)\n",
        "            ##########################################################################################\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 추가: 지시 길이와 텍스트를 모두 따로 반환\n",
        "        return self.instruction_lengths[index], self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb",
      "metadata": {
        "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a186394-4960-424d-bb6a-f58459dd5994",
      "metadata": {
        "id": "3a186394-4960-424d-bb6a-f58459dd5994"
      },
      "source": [
        "다음으로, `InstructionDataset` 데이터셋의 변경 사항으로 인해 각 `batch`가 이제 `item` 대신 `(instruction_length, item)` 튜플을 포함하도록 `custom_collate_fn`을 업데이트합니다. 또한 타깃 ID 리스트에서 지시 토큰을 마스킹합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d",
      "metadata": {
        "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # 배치에서 가장 긴 시퀀스 찾기\n",
        "    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # 추가: batch는 이제 튜플입니다.\n",
        "\n",
        "    # 입력과 타깃을 패딩하고 준비\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for instruction_length, item in batch:  # 추가: batch는 이제 튜플입니다.\n",
        "        new_item = item.copy()\n",
        "        # <|endoftext|> 토큰 추가\n",
        "        new_item += [pad_token_id]\n",
        "        # 시퀀스를 max_length까지 패딩\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "        inputs = torch.tensor(padded[:-1])  # 입력을 위해 마지막 토큰 자르기\n",
        "        targets = torch.tensor(padded[1:])  # 타깃을 위해 오른쪽으로 +1 이동\n",
        "\n",
        "        # 타깃에서 첫 번째를 제외한 모든 패딩 토큰을 ignore_index로 바꾸기\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        ##########################################################################################\n",
        "        # 추가: 타깃에서 모든 입력 및 지시 토큰 마스킹\n",
        "        targets[:instruction_length-1] = -100\n",
        "        ##########################################################################################\n",
        "\n",
        "        # 선택적으로 최대 시퀀스 길이로 자르기\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # 입력 및 타깃 리스트를 텐서로 변환하고 타깃 장치로 전송\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57",
      "metadata": {
        "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57"
      },
      "source": [
        "아래 샘플 데이터로 시험해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c",
      "metadata": {
        "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c"
      },
      "outputs": [],
      "source": [
        "sample_data = [\n",
        "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n",
        "    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n",
        "    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4",
      "metadata": {
        "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = InstructionDataset(sample_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=len(sample_data),\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
      "metadata": {
        "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
        "outputId": "5c33811c-3a76-4f70-af93-882b609dda93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 로더:\n",
            "torch.Size([3, 64]) torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "print(\"훈련 데이터 로더:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
      "metadata": {
        "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
        "outputId": "3ec06eab-5a39-43a6-f08e-4ebb5a212df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력:\n",
            " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
            "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
            "        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n",
            "          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n",
            "        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n",
            "        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n",
            "         1168, 37052, 50256, 50256])\n",
            "\n",
            "\n",
            "타깃:\n",
            " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n",
            "           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n",
            "        37052, 50256,  -100,  -100])\n"
          ]
        }
      ],
      "source": [
        "print(\"입력:\\n\", inputs[1])\n",
        "print(\"\\n\\n타깃:\\n\", targets[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628",
      "metadata": {
        "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628"
      },
      "source": [
        "`targets` 텐서를 기반으로 볼 수 있듯이, 지시와 패딩 토큰 모두 이제 -100 플레이스홀더 토큰을 사용하여 마스킹되었습니다.\n",
        "입력이 올바른지 확인하기 위해 디코딩해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
      "metadata": {
        "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
        "outputId": "a860acce-bc4a-4d46-f2dc-eb1b82fb2353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Sort the following list in alphabetical order.\n",
            "\n",
            "### Input:\n",
            "Zebra, Elephant, Crocodile\n",
            "\n",
            "### Response:\n",
            "Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(list(inputs[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd",
      "metadata": {
        "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd"
      },
      "source": [
        "다음으로, 마스킹되지 않은 타깃 토큰 ID를 디코딩해 보겠습니다:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
      "metadata": {
        "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
        "outputId": "96233c39-8b07-446e-d6e0-1ec0abcc3e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "### Response:\n",
            "Crocodile, Elephant, Zebra<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "non_masked_targets = targets[1][targets[1] != -100]\n",
        "\n",
        "print(tokenizer.decode(list(non_masked_targets)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6",
      "metadata": {
        "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6"
      },
      "source": [
        "위에서 보여준 것처럼 마스킹되지 않은 타깃 토큰에는 의도한 대로 `\"Instruction\"` 및 `\"Input\"` 필드가 제외됩니다. 이제 수정된 코드를 실행하여 이 마스킹 전략을 사용하여 미세 조정할 때 LLM이 얼마나 잘 수행되는지 확인할 수 있습니다.\n",
        "\n",
        "편의를 위해 다음과 같이 `exercise_experiments.py` 코드를 사용하여 비교를 실행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a76097-9114-479d-8803-443b0ff48581",
      "metadata": {
        "id": "56a76097-9114-479d-8803-443b0ff48581"
      },
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution mask_instructions\n",
        "```\n",
        "\n",
        "출력:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "Training set length: 935\n",
        "Validation set length: 55\n",
        "Test set length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded model: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   Training loss: 2.280539035797119\n",
        "   Validation loss: 2.262560224533081\n",
        "Ep 1 (Step 000000): Train loss 1.636, Val loss 1.620\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.143, Val loss 0.727\n",
        "...\n",
        "Training completed in 1.77 minutes.\n",
        "Plot saved as loss-plot-mask-instructions.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [02:10<00:00,  1.19s/it]\n",
        "Responses saved as instruction-data-with-response-mask-instructions.json\n",
        "Model saved as gpt2-medium355M-sft-mask-instructions.pth\n",
        "```\n",
        "\n",
        "다음으로, 결과 LLM의 성능을 평가해 보겠습니다.\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-mask-instructions.json\n",
        "```\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|██████████████████████████████████████████████████████████████████████████████████████| 110/110 [01:23<00:00,  1.31it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 47.73\n",
        "```\n",
        "\n",
        "점수를 기반으로 볼 때, 지시 마스킹은 \"Instruction Tuning With Loss Over Instructions\" 논문(https://arxiv.org/abs/2405.14394)에서 관찰된 것과 일치하게 약간 더 나쁜 성능을 보입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
      "metadata": {
        "id": "94a0f758-29da-44ee-b7af-32473b3c086e"
      },
      "source": [
        "&nbsp;\n",
        "## 연습문제 7.3: 원본 Alpaca 데이터셋에서 파인튜닝\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55",
      "metadata": {
        "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55"
      },
      "source": [
        "Stanford Alpaca 데이터셋([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca))에서 모델을 미세 튜닝하려면 파일 URL을 다음에서\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        "```\n",
        "\n",
        "다음으로 변경하기만 하면 됩니다.\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
        "```\n",
        "\n",
        "이 데이터셋에는 52,000개의 항목(7장에서 사용한 것보다 50배 더 많음)이 포함되어 있으며 각 항목의 길이는 7장에서 사용했던 것보다 더 깁니다.\n",
        "따라서 GPU에서 훈련을 실행하는 것이 좋습니다.\n",
        "\n",
        "메모리 부족 오류가 발생하면 배치 크기를 8에서 4, 2 또는 1로 줄이는 것을 고려하세요. 배치 크기를 줄이는 것 외에도 `allowed_max_length`를 1024에서 512 또는 256으로 줄이는 것을 고려할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c",
      "metadata": {
        "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c"
      },
      "source": [
        "편의를 위해 `exercise_experiments.py` 코드를 사용하여 52k Alpaca 데이터셋에서 배치 크기 4와 `allowed_max_length` 512로 모델을 미세 조정할 수 있습니다. 다음과 같습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52",
      "metadata": {
        "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52"
      },
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution alpaca_52k\n",
        "```\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "Training set length: 44201\n",
        "Validation set length: 2601\n",
        "Test set length: 5200\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded model: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   Training loss: 3.3681655883789063\n",
        "   Validation loss: 3.4122894287109373\n",
        "Ep 1 (Step 000000): Train loss 2.477, Val loss 2.750\n",
        "...\n",
        "Ep 2 (Step 022095): Train loss 0.761, Val loss 1.557\n",
        "...\n",
        "Training completed in 196.38 minutes.\n",
        "Plot saved as loss-plot-alpaca52k.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 5200/5200 [2:56:33<00:00,  2.04s/it]\n",
        "Responses saved as instruction-data-with-response-alpaca52k.json\n",
        "Model saved as gpt2-medium355M-sft-alpaca52k.pth\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7",
      "metadata": {
        "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7"
      },
      "source": [
        "아래는 Alpaca 데이터셋의 샘플 몇 개와 생성된 모델 응답입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8",
      "metadata": {
        "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8"
      },
      "source": [
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instruction\": \"Edit the following sentence to increase readability: \\\"He made a huge effort and was so successful.\\\"\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"He exerted a tremendous effort, and thus enjoyed great success.\",\n",
        "        \"model_response\": \"He put in an immense effort and was rewarded with success.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Rewrite the following sentence to make it more concise: \\\"I was displeased with the result of the experiment that I conducted.\\\"\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"I was unhappy with my experiment's outcome.\",\n",
        "        \"model_response\": \"I was displeased with the results of the experiment.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"How can we build a more efficient GPT model?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"We can build a more efficient GPT model by optimizing the architecture of the model, using smaller model sizes and training with fewer parameters. We can also leverage techniques such as knowledge distillation, transfer learning, dynamic sparsity and hybrid computing to further improve the efficiency of the model.\",\n",
        "        \"model_response\": \"Building a more efficient GPT model requires careful planning and optimization. First, it is important to identify the target language and the context in which the model is used. Then, it is important to select the appropriate model architecture, such as backpropagation, hyperparameters, and hyperparameters. Finally, it is important to select the appropriate model weights and optimizers, such as backpropagation, hyperparameters, and hyperparameters.\"\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840e2076-f2e6-44a2-86fd-f191f9674267",
      "metadata": {
        "id": "840e2076-f2e6-44a2-86fd-f191f9674267"
      },
      "source": [
        "마지막으로, `ollama_evaluate.py` 유틸리티를 사용하여 미세 튜닝된 LLM을 평가할 수 있습니다.\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-alpaca52k.json\n",
        "```\n",
        "\n",
        "```\n",
        "Scoring entries: 100%|████████████████████| 5200/5200 [1:07:52<00:00, 1.28it/s]\n",
        "Number of scores: 5188 of 5200\n",
        "Average score: 48.16\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4",
      "metadata": {
        "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4"
      },
      "source": [
        "이 점수는 이 장에서 사용했던 데이터셋에서 얻은 점수보다 약간 낮습니다. 그러나 Alpaca 테스트 세트에는 본문에서 사용했던 데이터셋보다 더 다양하고 부분적으로 더 어려운 지시가 포함되어 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30",
      "metadata": {
        "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30"
      },
      "source": [
        "## 연습문제 7.4: LoRA를 사용한 파라미터 효율적인 미세 튜닝\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01742cec-1f41-4415-8788-009d31b1ad38",
      "metadata": {
        "id": "01742cec-1f41-4415-8788-009d31b1ad38"
      },
      "source": [
        "LoRA를 사용하여 모델을 지시 미세 튜닝하려면 부록 E의 관련 클래스 및 함수를 사용하세요.\n",
        "\n",
        "```python\n",
        "from appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac",
      "metadata": {
        "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac"
      },
      "source": [
        "다음으로, 7.5절의 모델 로딩 코드 아래에 다음 코드 라인들을 추가합니다:\n",
        "\n",
        "\n",
        "```python\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")\n",
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"훈련 가능한 총 LoRA 파라미터 개수: {total_params:,}\")\n",
        "model.to(device)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b26b925-dc95-4b91-b050-9676dd9608a4",
      "metadata": {
        "id": "1b26b925-dc95-4b91-b050-9676dd9608a4"
      },
      "source": [
        "편의를 위해 `exercise_experiments.py` 코드를 사용하여 rank 16과 alpha 16인 LoRA를 사용해 모델을 미세 튜닝할 수 있습니다. 다음과 같습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f02c7e-3b15-44b8-bf41-7892cd755766",
      "metadata": {
        "id": "01f02c7e-3b15-44b8-bf41-7892cd755766"
      },
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution lora\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "Training set length: 935\n",
        "Validation set length: 55\n",
        "Test set length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
        "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
        "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
        "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
        "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
        "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
        "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
        "Loaded model: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Total trainable parameters before: 406,286,336\n",
        "Total trainable parameters after: 0\n",
        "Total trainable LoRA parameters: 7,898,384\n",
        "Initial losses\n",
        "   Training loss: 3.7684114456176756\n",
        "   Validation loss: 3.7619335651397705\n",
        "Ep 1 (Step 000000): Train loss 2.509, Val loss 2.519\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.308, Val loss 0.652\n",
        "...\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [01:52<00:00,  1.03s/it]\n",
        "Responses saved as instruction-data-with-response-lora.json\n",
        "Model saved as gpt2-medium355M-sft-lora.pth\n",
        "```\n",
        "\n",
        "비교를 위해 `python exercise_experiments.py --exercise_solution baseline` 명령으로 원래 7장의 미세 튜닝된 코드를 실행할 수 있습니다.\n",
        "\n",
        "Nvidia L4 GPU에서 LoRA를 사용하여 위 코드를 실행하면 1.3분이 걸리니다. 기본 모델은 1.8분이 걸립니다. 따라서 LoRA가 약 28% 빠릅니다.\n",
        "\n",
        "Ollama Llama 3 모델을 사용해 성능을 평가할 수 있습니다. 편의를 위해\n",
        "`python exercise_experiments.py`에 구현되어 있으며 다음처럼 실행할 수 있습니다.\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-lora.json\n",
        "```\n",
        "\n",
        "출력:\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|████████████████████████| 110/110 [01:13<00:00,  1.50it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 50.23\n",
        "```\n",
        "\n",
        "원래 모델과 비슷한 50점 정도의 점수입니다.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}