{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d003c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 재현 가능하게 시드 고정\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9272b1",
   "metadata": {},
   "source": [
    "### 1. 입력 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aaee916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  1️⃣ 입력 데이터 준비\n",
      "================================================================================\n",
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n",
      "입력 텐서 x: shape torch.Size([1, 3, 2])\n",
      "  토큰 0: tensor([1., 0.])\n",
      "  토큰 1: tensor([0.5000, 0.5000])\n",
      "  토큰 2: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print_section(\"1️⃣ 입력 데이터 준비\")\n",
    "\n",
    "batch_size = 1\n",
    "num_tokens = 3\n",
    "d_in = 2\n",
    "\n",
    "test_x = torch.tensor([[0, 1, 2], [1, 2, 3]])\n",
    "test_x2 = torch.tensor([[[0, 1, 2],[1, 2, 3]]])\n",
    "print(test_x.shape)\n",
    "print(test_x2.shape)\n",
    "\n",
    "# 입력 텐서: (1, 3, 2)\n",
    "x = torch.tensor([\n",
    "    [[1.0, 0.0],      # 토큰 0\n",
    "     [0.5, 0.5],      # 토큰 1\n",
    "     [0.0, 1.0]]      # 토큰 2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"입력 텐서 x: shape {x.shape}\")\n",
    "print(f\"  토큰 0: {x[0, 0, :]}\")\n",
    "print(f\"  토큰 1: {x[0, 1, :]}\")\n",
    "print(f\"  토큰 2: {x[0, 2, :]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88871844",
   "metadata": {},
   "source": [
    "### 2. Q, K, V 가중치 (수동으로 간단히 설정)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f50f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  2️⃣ Q, K, V 변환 가중치 설정\n",
      "================================================================================\n",
      "W_query shape: torch.Size([2, 2])\n",
      "W_query:\n",
      "tensor([[0.1000, 0.3000],\n",
      "        [0.2000, 0.4000]])\n",
      "\n",
      "W_key shape: torch.Size([2, 2])\n",
      "W_key:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1000, 0.3000]])\n",
      "\n",
      "W_value shape: torch.Size([2, 2])\n",
      "W_value:\n",
      "tensor([[0.1500, 0.3500],\n",
      "        [0.2500, 0.4500]])\n"
     ]
    }
   ],
   "source": [
    "print_section(\"2️⃣ Q, K, V 변환 가중치 설정\")\n",
    "\n",
    "d_out = 2\n",
    "num_heads = 2\n",
    "head_dim = d_out // num_heads  # = 1\n",
    "\n",
    "# 간단하게 고정된 가중치 사용\n",
    "W_query = torch.tensor([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4]\n",
    "], dtype=torch.float32).T  # (2, 2)\n",
    "\n",
    "W_key = torch.tensor([\n",
    "    [0.2, 0.1],\n",
    "    [0.4, 0.3]\n",
    "], dtype=torch.float32).T  # (2, 2)\n",
    "\n",
    "W_value = torch.tensor([\n",
    "    [0.15, 0.25],\n",
    "    [0.35, 0.45]\n",
    "], dtype=torch.float32).T  # (2, 2)\n",
    "\n",
    "print(f\"W_query shape: {W_query.shape}\")\n",
    "print(f\"W_query:\\n{W_query}\")\n",
    "print(f\"\\nW_key shape: {W_key.shape}\")\n",
    "print(f\"W_key:\\n{W_key}\")\n",
    "print(f\"\\nW_value shape: {W_value.shape}\")\n",
    "print(f\"W_value:\\n{W_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683df211",
   "metadata": {},
   "source": [
    "### 3. STEP 1: Q, K, V 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1ca99a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  3️⃣ STEP 1: Q, K, V 투영 계산 (x @ W)\n",
      "================================================================================\n",
      "Queries shape: torch.Size([1, 3, 2])\n",
      "Queries:\n",
      "tensor([[0.1000, 0.3000],\n",
      "        [0.1500, 0.3500],\n",
      "        [0.2000, 0.4000]])\n",
      "\n",
      "Keys shape: torch.Size([1, 3, 2])\n",
      "Keys:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1500, 0.3500],\n",
      "        [0.1000, 0.3000]])\n",
      "\n",
      "Values shape: torch.Size([1, 3, 2])\n",
      "Values:\n",
      "tensor([[0.1500, 0.3500],\n",
      "        [0.2000, 0.4000],\n",
      "        [0.2500, 0.4500]])\n",
      "\n",
      "[수동 계산 확인] Q 계산 (토큰 0):\n",
      "  [1.0, 0.0] @ [[0.1, 0.3], [0.2, 0.4]] = tensor([[0.1000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "print_section(\"3️⃣ STEP 1: Q, K, V 투영 계산 (x @ W)\")\n",
    "\n",
    "# Q = x @ W_query : (1, 3, 2) @ (2, 2) = (1, 3, 2)\n",
    "queries = x @ W_query\n",
    "keys = x @ W_key\n",
    "values = x @ W_value\n",
    "\n",
    "print(f\"Queries shape: {queries.shape}\")\n",
    "print(f\"Queries:\\n{queries[0]}\")\n",
    "print(f\"\\nKeys shape: {keys.shape}\")\n",
    "print(f\"Keys:\\n{keys[0]}\")\n",
    "print(f\"\\nValues shape: {values.shape}\")\n",
    "print(f\"Values:\\n{values[0]}\")\n",
    "\n",
    "# 수동 계산 확인\n",
    "print(\"\\n[수동 계산 확인] Q 계산 (토큰 0):\")\n",
    "print(f\"  [1.0, 0.0] @ [[0.1, 0.3], [0.2, 0.4]] = {x[0, 0:1] @ W_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe589ecd",
   "metadata": {},
   "source": [
    "### 4. STEP 2: 멀티헤드로 분할 (reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e89be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  4️⃣ STEP 2: 헤드로 분할 (reshape)\n",
      "================================================================================\n",
      "Queries 분할 후: torch.Size([1, 3, 2, 1])\n",
      "  (batch_size, num_tokens, num_heads, head_dim)\n",
      "  = (1, 3, 2, 1)\n",
      "\n",
      "Queries 분할 후 데이터:\n",
      "  헤드 0의 쿼리: tensor([0.1000, 0.1500, 0.2000])\n",
      "  헤드 1의 쿼리: tensor([0.3000, 0.3500, 0.4000])\n",
      "\n",
      "Keys 분할 후 데이터:\n",
      "  헤드 0의 키: tensor([0.2000, 0.1500, 0.1000])\n",
      "  헤드 1의 키: tensor([0.4000, 0.3500, 0.3000])\n",
      "\n",
      "Values 분할 후 데이터:\n",
      "  헤드 0의 값: tensor([0.1500, 0.2000, 0.2500])\n",
      "  헤드 1의 값: tensor([0.3500, 0.4000, 0.4500])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. STEP 2: 멀티헤드로 분할 (reshape)\n",
    "# ============================================\n",
    "print_section(\"4️⃣ STEP 2: 헤드로 분할 (reshape)\")\n",
    "\n",
    "# (1, 3, 2) -> (1, 3, 2, 1)\n",
    "# 2를 (num_heads=2, head_dim=1)로 분할\n",
    "queries_multi = queries.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "keys_multi = keys.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "values_multi = values.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "\n",
    "print(f\"Queries 분할 후: {queries_multi.shape}\")\n",
    "print(f\"  (batch_size, num_tokens, num_heads, head_dim)\")\n",
    "print(f\"  = (1, 3, 2, 1)\\n\")\n",
    "\n",
    "print(\"Queries 분할 후 데이터:\")\n",
    "print(f\"  헤드 0의 쿼리: {queries_multi[0, :, 0, 0]}\")\n",
    "print(f\"  헤드 1의 쿼리: {queries_multi[0, :, 1, 0]}\")\n",
    "print(f\"\\nKeys 분할 후 데이터:\")\n",
    "print(f\"  헤드 0의 키: {keys_multi[0, :, 0, 0]}\")\n",
    "print(f\"  헤드 1의 키: {keys_multi[0, :, 1, 0]}\")\n",
    "print(f\"\\nValues 분할 후 데이터:\")\n",
    "print(f\"  헤드 0의 값: {values_multi[0, :, 0, 0]}\")\n",
    "print(f\"  헤드 1의 값: {values_multi[0, :, 1, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1949a",
   "metadata": {},
   "source": [
    "### 5. STEP 3: 전치 (헤드 우선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ab3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  5️⃣ STEP 3: 전치 - 헤드를 우선 차원으로\n",
      "================================================================================\n",
      "전치 후 shape: torch.Size([1, 2, 3, 1])\n",
      "  (batch_size, num_heads, num_tokens, head_dim)\n",
      "  = (1, 2, 3, 1)\n",
      "\n",
      "전치 후 Queries:\n",
      "  헤드 0: tensor([0.1000, 0.1500, 0.2000])\n",
      "  헤드 1: tensor([0.3000, 0.3500, 0.4000])\n"
     ]
    }
   ],
   "source": [
    "print_section(\"5️⃣ STEP 3: 전치 - 헤드를 우선 차원으로\")\n",
    "\n",
    "\n",
    "# (1, 3, 2, 1) -> transpose(1,2) -> (1, 2, 3, 1)\n",
    "queries_T = queries_multi.transpose(1, 2)\n",
    "keys_T = keys_multi.transpose(1, 2)\n",
    "values_T = values_multi.transpose(1, 2)\n",
    "\n",
    "print(f\"전치 후 shape: {queries_T.shape}\")\n",
    "print(f\"  (batch_size, num_heads, num_tokens, head_dim)\")\n",
    "print(f\"  = (1, 2, 3, 1)\\n\")\n",
    "\n",
    "print(\"전치 후 Queries:\")\n",
    "print(f\"  헤드 0: {queries_T[0, 0, :, 0]}\")\n",
    "print(f\"  헤드 1: {queries_T[0, 1, :, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9017ec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  6️⃣ STEP 4: 어텐션 점수 = Q @ K^T\n",
      "================================================================================\n",
      "어텐션 점수 shape: torch.Size([1, 2, 3, 3])\n",
      "  (batch_size, num_heads, num_tokens, num_tokens)\n",
      "  = (1, 2, 3, 3)\n",
      "\n",
      "어텐션 점수 (각 토큰이 모든 토큰과의 유사도):\n",
      "헤드 0:\n",
      "tensor([[0.0200, 0.0150, 0.0100],\n",
      "        [0.0300, 0.0225, 0.0150],\n",
      "        [0.0400, 0.0300, 0.0200]])\n",
      "\n",
      "헤드 1:\n",
      "tensor([[0.1200, 0.1050, 0.0900],\n",
      "        [0.1400, 0.1225, 0.1050],\n",
      "        [0.1600, 0.1400, 0.1200]])\n",
      "\n",
      "[의미 해석]\n",
      "행: 쿼리 토큰 (현재 토큰)\n",
      "열: 키 토큰 (참조할 토큰)\n",
      "값: 유사도 점수 (높을수록 집중도 높음)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. STEP 4: 어텐션 점수 계산 (Q @ K^T)\n",
    "# ============================================\n",
    "print_section(\"6️⃣ STEP 4: 어텐션 점수 = Q @ K^T\")\n",
    "\n",
    "# (1, 2, 3, 1) @ (1, 2, 1, 3) = (1, 2, 3, 3)\n",
    "attn_scores = queries_T @ keys_T.transpose(2, 3)\n",
    "\n",
    "print(f\"어텐션 점수 shape: {attn_scores.shape}\")\n",
    "print(f\"  (batch_size, num_heads, num_tokens, num_tokens)\")\n",
    "print(f\"  = (1, 2, 3, 3)\\n\")\n",
    "\n",
    "print(\"어텐션 점수 (각 토큰이 모든 토큰과의 유사도):\")\n",
    "print(f\"헤드 0:\\n{attn_scores[0, 0, :, :]}\")\n",
    "print(f\"\\n헤드 1:\\n{attn_scores[0, 1, :, :]}\")\n",
    "\n",
    "print(\"\\n[의미 해석]\")\n",
    "print(\"행: 쿼리 토큰 (현재 토큰)\")\n",
    "print(\"열: 키 토큰 (참조할 토큰)\")\n",
    "print(\"값: 유사도 점수 (높을수록 집중도 높음)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5618fee",
   "metadata": {},
   "source": [
    "### 7. STEP 5: 인과적 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9748b303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  7️⃣ STEP 5: 인과적 마스크 적용 (미래 토큰 가리기)\n",
      "================================================================================\n",
      "마스크 패턴 (True = 마스킹할 위치):\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "설명:\n",
      "  토큰 0: 토큰 1, 2 마스킹 (미래 불가)\n",
      "  토큰 1: 토큰 2 마스킹\n",
      "  토큰 2: 마스킹 없음 (자신만 가능)\n",
      "\n",
      "마스킹 전 (헤드 0):\n",
      "tensor([[0.0200, 0.0150, 0.0100],\n",
      "        [0.0300, 0.0225, 0.0150],\n",
      "        [0.0400, 0.0300, 0.0200]])\n",
      "\n",
      "마스킹 후 (헤드 0):\n",
      "tensor([[0.0200,   -inf,   -inf],\n",
      "        [0.0300, 0.0225,   -inf],\n",
      "        [0.0400, 0.0300, 0.0200]])\n",
      "\n",
      "주의: -inf는 softmax에서 0으로 변환됨\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. STEP 5: 인과적 마스킹\n",
    "# ============================================\n",
    "print_section(\"7️⃣ STEP 5: 인과적 마스크 적용 (미래 토큰 가리기)\")\n",
    "\n",
    "# 상삼각 행렬 (미래 토큰을 True로 표시)\n",
    "mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1).bool()\n",
    "\n",
    "print(f\"마스크 패턴 (True = 마스킹할 위치):\\n{mask}\\n\")\n",
    "\n",
    "print(\"설명:\")\n",
    "print(\"  토큰 0: 토큰 1, 2 마스킹 (미래 불가)\")\n",
    "print(\"  토큰 1: 토큰 2 마스킹\")\n",
    "print(\"  토큰 2: 마스킹 없음 (자신만 가능)\")\n",
    "\n",
    "# 마스킹 전 점수\n",
    "print(f\"\\n마스킹 전 (헤드 0):\\n{attn_scores[0, 0, :, :]}\")\n",
    "\n",
    "# 마스킹 적용\n",
    "attn_scores_masked = attn_scores.clone()\n",
    "attn_scores_masked[0, :, mask] = -torch.inf\n",
    "\n",
    "print(f\"\\n마스킹 후 (헤드 0):\\n{attn_scores_masked[0, 0, :, :]}\")\n",
    "print(\"\\n주의: -inf는 softmax에서 0으로 변환됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9696a5",
   "metadata": {},
   "source": [
    "### 8. STEP 6: Softmax + 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0af66c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  8️⃣ STEP 6: Softmax 적용 (확률로 변환)\n",
      "================================================================================\n",
      "스케일링 팩터: sqrt(1) = 1.0\n",
      "  (head_dim이 1이므로 특별한 효과 없음)\n",
      "\n",
      "어텐션 가중치 shape: torch.Size([1, 2, 3, 3])\n",
      "어텐션 가중치 (헤드 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5019, 0.4981, 0.0000],\n",
      "        [0.3367, 0.3333, 0.3300]])\n",
      "\n",
      "각 행의 합 (1.0이어야 함):\n",
      "tensor([1., 1., 1.])\n",
      "\n",
      "[의미 해석]\n",
      "각 토큰이 다른 토큰들에 할당한 가중치\n",
      "값의 범위: 0 ~ 1, 합계: 1.0 (확률 분포)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8. STEP 6: Softmax + 스케일링\n",
    "# ============================================\n",
    "print_section(\"8️⃣ STEP 6: Softmax 적용 (확률로 변환)\")\n",
    "\n",
    "# 스케일링 팩터: sqrt(head_dim) = sqrt(1) = 1.0\n",
    "scale_factor = head_dim ** 0.5\n",
    "\n",
    "print(f\"스케일링 팩터: sqrt({head_dim}) = {scale_factor}\")\n",
    "print(f\"  (head_dim이 1이므로 특별한 효과 없음)\\n\")\n",
    "\n",
    "# Softmax 적용\n",
    "attn_weights = torch.softmax(attn_scores_masked / scale_factor, dim=-1)\n",
    "\n",
    "print(f\"어텐션 가중치 shape: {attn_weights.shape}\")\n",
    "print(f\"어텐션 가중치 (헤드 0):\\n{attn_weights[0, 0, :, :]}\")\n",
    "print(f\"\\n각 행의 합 (1.0이어야 함):\")\n",
    "print(attn_weights[0, 0, :, :].sum(dim=-1))\n",
    "\n",
    "print(\"\\n[의미 해석]\")\n",
    "print(\"각 토큰이 다른 토큰들에 할당한 가중치\")\n",
    "print(\"값의 범위: 0 ~ 1, 합계: 1.0 (확률 분포)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f648b0",
   "metadata": {},
   "source": [
    "### 9. STEP 7: Value와 곱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60a5903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  9️⃣ STEP 7: 가중합 계산 = Weights @ Values\n",
      "================================================================================\n",
      "컨텍스트 벡터 shape: torch.Size([1, 2, 3, 1])\n",
      "  (batch_size, num_heads, num_tokens, head_dim)\n",
      "  = (1, 2, 3, 1)\n",
      "\n",
      "컨텍스트 벡터:\n",
      "헤드 0 결과: tensor([0.1500, 0.1749, 0.1997])\n",
      "헤드 1 결과: tensor([0.3500, 0.3748, 0.3993])\n",
      "\n",
      "[수동 계산 예시] 헤드 0, 토큰 0:\n",
      "  가중치: tensor([1., 0., 0.])\n",
      "  값들: tensor([0.1500, 0.2000, 0.2500])\n",
      "  가중합: 0.150000\n",
      "tensor([[[[0.1500],\n",
      "          [0.1749],\n",
      "          [0.1997]],\n",
      "\n",
      "         [[0.3500],\n",
      "          [0.3748],\n",
      "          [0.3993]]]])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 9. STEP 7: Value와 곱하기\n",
    "# ============================================\n",
    "print_section(\"9️⃣ STEP 7: 가중합 계산 = Weights @ Values\")\n",
    "\n",
    "# (1, 2, 3, 3) @ (1, 2, 3, 1) = (1, 2, 3, 1)\n",
    "context_vectors = attn_weights @ values_T\n",
    "\n",
    "print(f\"컨텍스트 벡터 shape: {context_vectors.shape}\")\n",
    "print(f\"  (batch_size, num_heads, num_tokens, head_dim)\")\n",
    "print(f\"  = (1, 2, 3, 1)\\n\")\n",
    "\n",
    "print(\"컨텍스트 벡터:\")\n",
    "print(f\"헤드 0 결과: {context_vectors[0, 0, :, 0]}\")\n",
    "print(f\"헤드 1 결과: {context_vectors[0, 1, :, 0]}\")\n",
    "\n",
    "print(\"\\n[수동 계산 예시] 헤드 0, 토큰 0:\")\n",
    "print(f\"  가중치: {attn_weights[0, 0, 0, :]}\")\n",
    "print(f\"  값들: {values_T[0, 0, :, 0]}\")\n",
    "result = torch.dot(attn_weights[0, 0, 0, :], values_T[0, 0, :, 0])\n",
    "print(f\"  가중합: {result.item():.6f}\")\n",
    "\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f31070",
   "metadata": {},
   "source": [
    "### 10. STEP 8: 헤드 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f77e6978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  🔟 STEP 8: 헤드 결합 (concatenate)\n",
      "================================================================================\n",
      "전치 후 shape: torch.Size([1, 3, 2, 1])\n",
      "  (batch_size, num_tokens, num_heads, head_dim)\n",
      "  = (1, 3, 2, 1)\n",
      "\n",
      "최종 결합 후 shape: torch.Size([1, 3, 2])\n",
      "  (batch_size, num_tokens, d_out)\n",
      "  = (1, 3, 2)\n",
      "\n",
      "결합된 출력 (모든 헤드의 결과 연결):\n",
      "  토큰 0: tensor([0.1500, 0.3500])\n",
      "  토큰 1: tensor([0.1749, 0.3748])\n",
      "  토큰 2: tensor([0.1997, 0.3993])\n",
      "\n",
      "[의미]\n",
      "  첫 번째 값: 헤드 0의 결과\n",
      "  두 번째 값: 헤드 1의 결과\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10. STEP 8: 헤드 결합\n",
    "# ============================================\n",
    "print_section(\"🔟 STEP 8: 헤드 결합 (concatenate)\")\n",
    "\n",
    "# (1, 2, 3, 1) -> transpose(1,2) -> (1, 3, 2, 1) -> view -> (1, 3, 2)\n",
    "context_vectors_T = context_vectors.transpose(1, 2)\n",
    "output = context_vectors_T.contiguous().view(batch_size, num_tokens, d_out)\n",
    "\n",
    "print(f\"전치 후 shape: {context_vectors_T.shape}\")\n",
    "print(f\"  (batch_size, num_tokens, num_heads, head_dim)\")\n",
    "print(f\"  = (1, 3, 2, 1)\\n\")\n",
    "\n",
    "print(f\"최종 결합 후 shape: {output.shape}\")\n",
    "print(f\"  (batch_size, num_tokens, d_out)\")\n",
    "print(f\"  = (1, 3, 2)\\n\")\n",
    "\n",
    "print(\"결합된 출력 (모든 헤드의 결과 연결):\")\n",
    "print(f\"  토큰 0: {output[0, 0, :]}\")\n",
    "print(f\"  토큰 1: {output[0, 1, :]}\")\n",
    "print(f\"  토큰 2: {output[0, 2, :]}\")\n",
    "\n",
    "print(\"\\n[의미]\")\n",
    "print(\"  첫 번째 값: 헤드 0의 결과\")\n",
    "print(\"  두 번째 값: 헤드 1의 결과\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845cfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
