"""
ë©€í‹°í—¤ë“œ ì–´í…ì…˜ - ì´ˆê°„ë‹¨ ì˜ˆì œë¡œ ì™„ë²½íˆ ì´í•´í•˜ê¸°
=====================================

ì„¤ì •ì„ ìµœì†Œí•œìœ¼ë¡œ ì¤„ì—¬ì„œ:
- batch_size = 1  (ë°°ì¹˜ ì—†ì´ í•œ ìƒ˜í”Œë§Œ)
- num_tokens = 3  (3ê°œ í† í°ë§Œ)
- d_in = 2        (ì…ë ¥ ì°¨ì› 2)
- d_out = 2       (ì¶œë ¥ ì°¨ì› 2)
- num_heads = 2   (2ê°œ í—¤ë“œ)
- head_dim = 1    (í—¤ë“œë‹¹ 1ì°¨ì›)

ì´ì œ ëª¨ë“  í–‰ë ¬ì„ ì†ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# ì¬í˜„ ê°€ëŠ¥í•˜ê²Œ ì‹œë“œ ê³ ì •
torch.manual_seed(42)


def print_section(title):
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def simple_multihead_attention():
    """
    ê°€ì¥ ê°„ë‹¨í•œ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì˜ˆì œ
    """

    print_section("ğŸ¯ ì´ˆê°„ë‹¨ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì˜ˆì œ")

    # ============================================
    # 1. ê°„ë‹¨í•œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    # ============================================
    print_section("1ï¸âƒ£ ì…ë ¥ ë°ì´í„° ì¤€ë¹„")

    batch_size = 1
    num_tokens = 3
    d_in = 2

    # ì…ë ¥ í…ì„œ: (1, 3, 2)
    x = torch.tensor([
        [[1.0, 0.0],      # í† í° 0
         [0.5, 0.5],      # í† í° 1
         [0.0, 1.0]]      # í† í° 2
    ], dtype=torch.float32)

    print(f"ì…ë ¥ í…ì„œ x: shape {x.shape}")
    print(f"  í† í° 0: {x[0, 0, :]}")
    print(f"  í† í° 1: {x[0, 1, :]}")
    print(f"  í† í° 2: {x[0, 2, :]}")

    # ============================================
    # 2. Q, K, V ê°€ì¤‘ì¹˜ (ìˆ˜ë™ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ì •)
    # ============================================
    print_section("2ï¸âƒ£ Q, K, V ë³€í™˜ ê°€ì¤‘ì¹˜ ì„¤ì •")

    d_out = 2
    num_heads = 2
    head_dim = d_out // num_heads  # = 1

    # ê°„ë‹¨í•˜ê²Œ ê³ ì •ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    W_query = torch.tensor([
        [0.1, 0.2],
        [0.3, 0.4]
    ], dtype=torch.float32).T  # (2, 2)

    W_key = torch.tensor([
        [0.2, 0.1],
        [0.4, 0.3]
    ], dtype=torch.float32).T  # (2, 2)

    W_value = torch.tensor([
        [0.15, 0.25],
        [0.35, 0.45]
    ], dtype=torch.float32).T  # (2, 2)

    print(f"W_query shape: {W_query.shape}")
    print(f"W_query:\n{W_query}")
    print(f"\nW_key shape: {W_key.shape}")
    print(f"W_key:\n{W_key}")
    print(f"\nW_value shape: {W_value.shape}")
    print(f"W_value:\n{W_value}")

    # ============================================
    # 3. STEP 1: Q, K, V ê³„ì‚°
    # ============================================
    print_section("3ï¸âƒ£ STEP 1: Q, K, V íˆ¬ì˜ ê³„ì‚° (x @ W)")

    # Q = x @ W_query : (1, 3, 2) @ (2, 2) = (1, 3, 2)
    queries = x @ W_query
    keys = x @ W_key
    values = x @ W_value

    print(f"Queries shape: {queries.shape}")
    print(f"Queries:\n{queries[0]}")
    print(f"\nKeys shape: {keys.shape}")
    print(f"Keys:\n{keys[0]}")
    print(f"\nValues shape: {values.shape}")
    print(f"Values:\n{values[0]}")

    # ìˆ˜ë™ ê³„ì‚° í™•ì¸
    print("\n[ìˆ˜ë™ ê³„ì‚° í™•ì¸] Q ê³„ì‚° (í† í° 0):")
    print(f"  [1.0, 0.0] @ [[0.1, 0.3], [0.2, 0.4]] = {x[0, 0:1] @ W_query}")

    # ============================================
    # 4. STEP 2: ë©€í‹°í—¤ë“œë¡œ ë¶„í•  (reshape)
    # ============================================
    print_section("4ï¸âƒ£ STEP 2: í—¤ë“œë¡œ ë¶„í•  (reshape)")

    # (1, 3, 2) -> (1, 3, 2, 1)
    # 2ë¥¼ (num_heads=2, head_dim=1)ë¡œ ë¶„í• 
    queries_multi = queries.view(batch_size, num_tokens, num_heads, head_dim)
    keys_multi = keys.view(batch_size, num_tokens, num_heads, head_dim)
    values_multi = values.view(batch_size, num_tokens, num_heads, head_dim)

    print(f"Queries ë¶„í•  í›„: {queries_multi.shape}")
    print(f"  (batch_size, num_tokens, num_heads, head_dim)")
    print(f"  = (1, 3, 2, 1)\n")

    print("Queries ë¶„í•  í›„ ë°ì´í„°:")
    print(f"  í—¤ë“œ 0ì˜ ì¿¼ë¦¬: {queries_multi[0, :, 0, 0]}")
    print(f"  í—¤ë“œ 1ì˜ ì¿¼ë¦¬: {queries_multi[0, :, 1, 0]}")
    print(f"\nKeys ë¶„í•  í›„ ë°ì´í„°:")
    print(f"  í—¤ë“œ 0ì˜ í‚¤: {keys_multi[0, :, 0, 0]}")
    print(f"  í—¤ë“œ 1ì˜ í‚¤: {keys_multi[0, :, 1, 0]}")
    print(f"\nValues ë¶„í•  í›„ ë°ì´í„°:")
    print(f"  í—¤ë“œ 0ì˜ ê°’: {values_multi[0, :, 0, 0]}")
    print(f"  í—¤ë“œ 1ì˜ ê°’: {values_multi[0, :, 1, 0]}")

    # ============================================
    # 5. STEP 3: ì „ì¹˜ (í—¤ë“œ ìš°ì„ )
    # ============================================
    print_section("5ï¸âƒ£ STEP 3: ì „ì¹˜ - í—¤ë“œë¥¼ ìš°ì„  ì°¨ì›ìœ¼ë¡œ")

    # (1, 3, 2, 1) -> transpose(1,2) -> (1, 2, 3, 1)
    queries_T = queries_multi.transpose(1, 2)
    keys_T = keys_multi.transpose(1, 2)
    values_T = values_multi.transpose(1, 2)

    print(f"ì „ì¹˜ í›„ shape: {queries_T.shape}")
    print(f"  (batch_size, num_heads, num_tokens, head_dim)")
    print(f"  = (1, 2, 3, 1)\n")

    print("ì „ì¹˜ í›„ Queries:")
    print(f"  í—¤ë“œ 0: {queries_T[0, 0, :, 0]}")
    print(f"  í—¤ë“œ 1: {queries_T[0, 1, :, 0]}")

    # ============================================
    # 6. STEP 4: ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚° (Q @ K^T)
    # ============================================
    print_section("6ï¸âƒ£ STEP 4: ì–´í…ì…˜ ì ìˆ˜ = Q @ K^T")

    # (1, 2, 3, 1) @ (1, 2, 1, 3) = (1, 2, 3, 3)
    attn_scores = queries_T @ keys_T.transpose(2, 3)

    print(f"ì–´í…ì…˜ ì ìˆ˜ shape: {attn_scores.shape}")
    print(f"  (batch_size, num_heads, num_tokens, num_tokens)")
    print(f"  = (1, 2, 3, 3)\n")

    print("ì–´í…ì…˜ ì ìˆ˜ (ê° í† í°ì´ ëª¨ë“  í† í°ê³¼ì˜ ìœ ì‚¬ë„):")
    print(f"í—¤ë“œ 0:\n{attn_scores[0, 0, :, :]}")
    print(f"\ní—¤ë“œ 1:\n{attn_scores[0, 1, :, :]}")

    print("\n[ì˜ë¯¸ í•´ì„]")
    print("í–‰: ì¿¼ë¦¬ í† í° (í˜„ì¬ í† í°)")
    print("ì—´: í‚¤ í† í° (ì°¸ì¡°í•  í† í°)")
    print("ê°’: ìœ ì‚¬ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì§‘ì¤‘ë„ ë†’ìŒ)")

    # ============================================
    # 7. STEP 5: ì¸ê³¼ì  ë§ˆìŠ¤í‚¹
    # ============================================
    print_section("7ï¸âƒ£ STEP 5: ì¸ê³¼ì  ë§ˆìŠ¤í¬ ì ìš© (ë¯¸ë˜ í† í° ê°€ë¦¬ê¸°)")

    # ìƒì‚¼ê° í–‰ë ¬ (ë¯¸ë˜ í† í°ì„ Trueë¡œ í‘œì‹œ)
    mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1).bool()

    print(f"ë§ˆìŠ¤í¬ íŒ¨í„´ (True = ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜):\n{mask}\n")

    print("ì„¤ëª…:")
    print("  í† í° 0: í† í° 1, 2 ë§ˆìŠ¤í‚¹ (ë¯¸ë˜ ë¶ˆê°€)")
    print("  í† í° 1: í† í° 2 ë§ˆìŠ¤í‚¹")
    print("  í† í° 2: ë§ˆìŠ¤í‚¹ ì—†ìŒ (ìì‹ ë§Œ ê°€ëŠ¥)")

    # ë§ˆìŠ¤í‚¹ ì „ ì ìˆ˜
    print(f"\në§ˆìŠ¤í‚¹ ì „ (í—¤ë“œ 0):\n{attn_scores[0, 0, :, :]}")

    # ë§ˆìŠ¤í‚¹ ì ìš©
    attn_scores_masked = attn_scores.clone()
    attn_scores_masked[0, :, mask] = -torch.inf

    print(f"\në§ˆìŠ¤í‚¹ í›„ (í—¤ë“œ 0):\n{attn_scores_masked[0, 0, :, :]}")
    print("\nì£¼ì˜: -infëŠ” softmaxì—ì„œ 0ìœ¼ë¡œ ë³€í™˜ë¨")

    # ============================================
    # 8. STEP 6: Softmax + ìŠ¤ì¼€ì¼ë§
    # ============================================
    print_section("8ï¸âƒ£ STEP 6: Softmax ì ìš© (í™•ë¥ ë¡œ ë³€í™˜)")

    # ìŠ¤ì¼€ì¼ë§ íŒ©í„°: sqrt(head_dim) = sqrt(1) = 1.0
    scale_factor = head_dim ** 0.5

    print(f"ìŠ¤ì¼€ì¼ë§ íŒ©í„°: sqrt({head_dim}) = {scale_factor}")
    print(f"  (head_dimì´ 1ì´ë¯€ë¡œ íŠ¹ë³„í•œ íš¨ê³¼ ì—†ìŒ)\n")

    # Softmax ì ìš©
    attn_weights = torch.softmax(attn_scores_masked / scale_factor, dim=-1)

    print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ shape: {attn_weights.shape}")
    print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (í—¤ë“œ 0):\n{attn_weights[0, 0, :, :]}")
    print(f"\nê° í–‰ì˜ í•© (1.0ì´ì–´ì•¼ í•¨):")
    print(attn_weights[0, 0, :, :].sum(dim=-1))

    print("\n[ì˜ë¯¸ í•´ì„]")
    print("ê° í† í°ì´ ë‹¤ë¥¸ í† í°ë“¤ì— í• ë‹¹í•œ ê°€ì¤‘ì¹˜")
    print("ê°’ì˜ ë²”ìœ„: 0 ~ 1, í•©ê³„: 1.0 (í™•ë¥  ë¶„í¬)")

    # ============================================
    # 9. STEP 7: Valueì™€ ê³±í•˜ê¸°
    # ============================================
    print_section("9ï¸âƒ£ STEP 7: ê°€ì¤‘í•© ê³„ì‚° = Weights @ Values")

    # (1, 2, 3, 3) @ (1, 2, 3, 1) = (1, 2, 3, 1)
    context_vectors = attn_weights @ values_T

    print(f"ì»¨í…ìŠ¤íŠ¸ ë²¡í„° shape: {context_vectors.shape}")
    print(f"  (batch_size, num_heads, num_tokens, head_dim)")
    print(f"  = (1, 2, 3, 1)\n")

    print("ì»¨í…ìŠ¤íŠ¸ ë²¡í„°:")
    print(f"í—¤ë“œ 0 ê²°ê³¼: {context_vectors[0, 0, :, 0]}")
    print(f"í—¤ë“œ 1 ê²°ê³¼: {context_vectors[0, 1, :, 0]}")

    print("\n[ìˆ˜ë™ ê³„ì‚° ì˜ˆì‹œ] í—¤ë“œ 0, í† í° 0:")
    print(f"  ê°€ì¤‘ì¹˜: {attn_weights[0, 0, 0, :]}")
    print(f"  ê°’ë“¤: {values_T[0, 0, :, 0]}")
    result = torch.dot(attn_weights[0, 0, 0, :], values_T[0, 0, :, 0])
    print(f"  ê°€ì¤‘í•©: {result.item():.6f}")

    # ============================================
    # 10. STEP 8: í—¤ë“œ ê²°í•©
    # ============================================
    print_section("ğŸ”Ÿ STEP 8: í—¤ë“œ ê²°í•© (concatenate)")

    # (1, 2, 3, 1) -> transpose(1,2) -> (1, 3, 2, 1) -> view -> (1, 3, 2)
    context_vectors_T = context_vectors.transpose(1, 2)
    output = context_vectors_T.contiguous().view(batch_size, num_tokens, d_out)

    print(f"ì „ì¹˜ í›„ shape: {context_vectors_T.shape}")
    print(f"  (batch_size, num_tokens, num_heads, head_dim)")
    print(f"  = (1, 3, 2, 1)\n")

    print(f"ìµœì¢… ê²°í•© í›„ shape: {output.shape}")
    print(f"  (batch_size, num_tokens, d_out)")
    print(f"  = (1, 3, 2)\n")

    print("ê²°í•©ëœ ì¶œë ¥ (ëª¨ë“  í—¤ë“œì˜ ê²°ê³¼ ì—°ê²°):")
    print(f"  í† í° 0: {output[0, 0, :]}")
    print(f"  í† í° 1: {output[0, 1, :]}")
    print(f"  í† í° 2: {output[0, 2, :]}")

    print("\n[ì˜ë¯¸]")
    print("  ì²« ë²ˆì§¸ ê°’: í—¤ë“œ 0ì˜ ê²°ê³¼")
    print("  ë‘ ë²ˆì§¸ ê°’: í—¤ë“œ 1ì˜ ê²°ê³¼")

    return output, attn_weights, attn_scores_masked


def visualize_attention_flow():
    """
    ì–´í…ì…˜ì˜ íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸°
    """
    print_section("ğŸ“Š ì–´í…ì…˜ íë¦„ ì‹œê°í™”")

    print("""
    ì…ë ¥ 3ê°œ í† í°ì˜ ì²˜ë¦¬ ê³¼ì •:

    ì…ë ¥ í…ì„œ (1, 3, 2):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  í† í°0: [1.0, 0.0]  â”‚
    â”‚  í† í°1: [0.5, 0.5]  â”‚
    â”‚  í† í°2: [0.0, 1.0]  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (Q, K, V íˆ¬ì˜)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Q, K, V ê³„ì‚°      â”‚
    â”‚  shape: (1,3,2)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (reshape)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2ê°œ í—¤ë“œë¡œ ë¶„í•      â”‚
    â”‚  shape: (1,3,2,1)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (transpose)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  í—¤ë“œë³„ ì •ë ¬         â”‚
    â”‚  shape: (1,2,3,1)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (Q @ K^T)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ì–´í…ì…˜ ì ìˆ˜ (3x3)  â”‚
    â”‚  ê° í† í° ìŒì˜ ìœ ì‚¬ë„ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (ë§ˆìŠ¤í‚¹)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ë¯¸ë˜ í† í° ì œì™¸      â”‚
    â”‚  ì¸ê³¼ì  ë§ˆìŠ¤í¬ ì ìš©  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (softmax)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (3x3) â”‚
    â”‚  í™•ë¥  ë¶„í¬ (í–‰ì˜í•©=1)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (@ Values)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ì»¨í…ìŠ¤íŠ¸ ë²¡í„°      â”‚
    â”‚  ê° í† í°ì˜ ì§‘ì¤‘ ê²°ê³¼ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (í—¤ë“œ ê²°í•©)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ìµœì¢… ì¶œë ¥ (1,3,2)  â”‚
    â”‚  2ê°œ í—¤ë“œ ê²°í•©      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def compare_heads():
    """
    ë‘ í—¤ë“œê°€ ì–´ë–»ê²Œ ë‹¤ë¥´ê²Œ ì§‘ì¤‘í•˜ëŠ”ì§€ ë¹„êµ
    """
    print_section("ğŸ§  í—¤ë“œ ê°„ ì§‘ì¤‘ íŒ¨í„´ ë¹„êµ")

    output, attn_weights, attn_scores = simple_multihead_attention()

    print("\n" + "=" * 80)
    print("í—¤ë“œ 0 vs í—¤ë“œ 1ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜")
    print("=" * 80)

    print("\n[í—¤ë“œ 0] ì–´í…ì…˜ ê°€ì¤‘ì¹˜:")
    print(attn_weights[0, 0, :, :])

    print("\n[í—¤ë“œ 1] ì–´í…ì…˜ ê°€ì¤‘ì¹˜:")
    print(attn_weights[0, 1, :, :])

    print("\n[í•´ì„]")
    print("- ê° í—¤ë“œëŠ” ë‹¤ë¥¸ íŒ¨í„´ìœ¼ë¡œ ì§‘ì¤‘!")
    print("- í—¤ë“œ 0: ì–´ë–¤ í† í°ì— ì§‘ì¤‘í• ê¹Œ?")
    print("- í—¤ë“œ 1: ë‹¤ë¥¸ í† í°ì— ì§‘ì¤‘í• ê¹Œ?")
    print("- ì—¬ëŸ¬ í—¤ë“œë¥¼ í•©ì¹˜ë©´ ë” í’ë¶€í•œ í‘œí˜„ ê°€ëŠ¥!")


if __name__ == "__main__":
    print("\n" * 2)
    print("â–ˆ" * 80)
    print("â–ˆ  ë©€í‹°í—¤ë“œ ì–´í…ì…˜ - ì†ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ì„œ ë°°ìš°ê¸°")
    print("â–ˆ" * 80)

    # ë©”ì¸ ì˜ˆì œ ì‹¤í–‰
    output, attn_weights, attn_scores = simple_multihead_attention()

    # íë¦„ ì‹œê°í™”
    visualize_attention_flow()

    # í—¤ë“œ ë¹„êµ
    compare_heads()

    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ! ëª¨ë“  ë‹¨ê³„ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤!")
    print("=" * 80 + "\n")
