{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고차원 텐서 완전 가이드\n",
    "\n",
    "## LLM 프로젝트에서 자주 사용되는 텐서 연산 마스터하기\n",
    "\n",
    "이 노트북에서는 PyTorch 텐서의 핵심 개념과 연산들을 단계별로 학습합니다:\n",
    "- 차원 분해 및 Shape 이해\n",
    "- Squeeze/Unsqueeze로 차원 조작\n",
    "- **전치(Transpose)**: 차원 재배열\n",
    "- **점곱(Dot Product)**: 벡터 및 행렬 연산\n",
    "- Broadcasting 규칙\n",
    "- LLM 실전 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.8.0+cu128\n",
      "CUDA 사용 가능: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ 차원 분해 (Shape Breakdown)\n",
    "\n",
    "고차원 텐서의 각 차원이 **무엇을 의미하는지** 명확히 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 어텐션 출력 텐서 ===\n",
      "Shape: torch.Size([2, 1024, 768])\n",
      "차원 0 (배치): 2개 문서\n",
      "차원 1 (시퀀스): 1024개 토큰\n",
      "차원 2 (임베딩): 768차원 벡터\n",
      "\n",
      "총 원소 수: 1572864\n",
      "각 원소 크기: 4 바이트\n",
      "메모리 크기: 6.00 MB\n"
     ]
    }
   ],
   "source": [
    "# 예: GPT의 어텐션 출력\n",
    "batch_size = 2\n",
    "seq_length = 1024\n",
    "emb_dim = 768\n",
    "\n",
    "attention_output = torch.randn(batch_size, seq_length, emb_dim)\n",
    "\n",
    "print(\"=== 어텐션 출력 텐서 ===\")\n",
    "print(f\"Shape: {attention_output.shape}\")\n",
    "print(f\"차원 0 (배치): {attention_output.shape[0]}개 문서\")\n",
    "print(f\"차원 1 (시퀀스): {attention_output.shape[1]}개 토큰\")\n",
    "print(f\"차원 2 (임베딩): {attention_output.shape[2]}차원 벡터\")\n",
    "print(f\"\\n총 원소 수: {attention_output.numel()}\")\n",
    "print(f\"각 원소 크기: {attention_output.element_size()} 바이트\")\n",
    "print(f\"메모리 크기: {attention_output.element_size() * attention_output.numel() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ 특정 차원에 집중 (Indexing)\n",
    "\n",
    "한 번에 하나의 차원만 선택해서 생각하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작은 텐서로 실험\n",
    "x = torch.randn(2, 5, 3)  # (배치=2, 토큰=5, 차원=3)\n",
    "print(\"원본 텐서:\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"내용:\\n{x}\")\n",
    "\n",
    "print(\"\\n=== 배치 0만 선택 ===\")\n",
    "batch_0 = x[0]  # (5, 3)\n",
    "print(f\"Shape: {batch_0.shape}\")\n",
    "print(f\"내용:\\n{batch_0}\")\n",
    "\n",
    "print(\"\\n=== 배치 0의 첫 토큰 ===\")\n",
    "first_token = x[0, 0]  # (3,)\n",
    "print(f\"Shape: {first_token.shape}\")\n",
    "print(f\"내용: {first_token}\")\n",
    "\n",
    "print(\"\\n=== 모든 배치의 첫 토큰 ===\")\n",
    "first_tokens = x[:, 0, :]  # (2, 3)\n",
    "print(f\"Shape: {first_tokens.shape}\")\n",
    "print(f\"내용:\\n{first_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Squeeze & Unsqueeze (차원 제거/추가)\n",
    "\n",
    "불필요한 차원을 제거하거나 새 차원을 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze: 크기가 1인 차원 제거\n",
    "x_1 = torch.randn(1, 10, 768)  # (배치=1, 토큰=10, 임베딩=768)\n",
    "print(\"=== Squeeze (차원 제거) ===\")\n",
    "print(f\"원본 shape: {x_1.shape}\")\n",
    "x_1_squeezed = x_1.squeeze()  # 모든 크기 1인 차원 제거\n",
    "print(f\"squeeze() 후: {x_1_squeezed.shape}\")\n",
    "\n",
    "# 특정 차원만 제거\n",
    "x_1_squeezed_0 = x_1.squeeze(0)  # 차원 0만 제거\n",
    "print(f\"squeeze(0) 후: {x_1_squeezed_0.shape}\")\n",
    "\n",
    "# Unsqueeze: 새 차원 추가\n",
    "print(\"\\n=== Unsqueeze (차원 추가) ===\")\n",
    "x_2 = torch.randn(10, 768)  # (토큰=10, 임베딩=768)\n",
    "print(f\"원본 shape: {x_2.shape}\")\n",
    "x_2_unsqueezed_0 = x_2.unsqueeze(0)  # 맨 앞에 차원 추가\n",
    "print(f\"unsqueeze(0) 후: {x_2_unsqueezed_0.shape}\")  # (1, 10, 768)\n",
    "x_2_unsqueezed_1 = x_2.unsqueeze(1)  # 중간에 차원 추가\n",
    "print(f\"unsqueeze(1) 후: {x_2_unsqueezed_1.shape}\")  # (10, 1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ 전치 (Transpose/Permute) 🔄\n",
    "\n",
    "텐서의 차원 순서를 바꾸기. 어텐션, 행렬 곱셈 등에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 행렬의 전치\n",
    "print(\"=== 2D 행렬 전치 ===\")\n",
    "matrix = torch.arange(6).reshape(2, 3)\n",
    "print(f\"원본 (2, 3):\\n{matrix}\")\n",
    "transposed = matrix.T  # 또는 matrix.transpose(0, 1)\n",
    "print(f\"\\n전치 후 (3, 2):\\n{transposed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차원 텐서의 전치 (Permute)\n",
    "print(\"\\n=== 고차원 텐서 전치 (Permute) ===\")\n",
    "tensor = torch.randn(2, 3, 4, 5)  # (배치, 토큰, 헤드, 차원)\n",
    "print(f\"원본 shape: {tensor.shape}\")\n",
    "\n",
    "# 차원 0과 2를 교환\n",
    "permuted = tensor.permute(0, 2, 1, 3)  # (배치, 헤드, 토큰, 차원)\n",
    "print(f\"permute(0, 2, 1, 3) 후: {permuted.shape}\")\n",
    "\n",
    "# 모든 차원을 역순으로\n",
    "reversed_dims = tensor.permute(3, 2, 1, 0)  # (차원, 헤드, 토큰, 배치)\n",
    "print(f\"permute(3, 2, 1, 0) 후: {reversed_dims.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실전: 멀티헤드 어텐션에서의 전치\n",
    "print(\"\\n=== LLM 실전 예시: 멀티헤드 어텐션 ===\")\n",
    "batch_size, seq_len, emb_dim, n_heads = 2, 4, 8, 2\n",
    "head_dim = emb_dim // n_heads\n",
    "\n",
    "# Q, K, V를 head 형태로 변환: (batch, seq, emb_dim) → (batch, seq, n_heads, head_dim)\n",
    "Q = torch.randn(batch_size, seq_len, emb_dim)\n",
    "print(f\"Q 원본: {Q.shape}\")\n",
    "\n",
    "Q_heads = Q.reshape(batch_size, seq_len, n_heads, head_dim)\n",
    "print(f\"Q reshape 후: {Q_heads.shape}\")\n",
    "\n",
    "# 헤드 계산을 위해 차원 재배열: (batch, seq, n_heads, head_dim) → (batch, n_heads, seq, head_dim)\n",
    "Q_heads = Q_heads.permute(0, 2, 1, 3)\n",
    "print(f\"Q permute(0, 2, 1, 3) 후: {Q_heads.shape}\")\n",
    "print(\"\\n이제 각 헤드별로 병렬로 어텐션 연산 가능!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ 점곱 (Dot Product) 🔹\n",
    "\n",
    "두 벡터 또는 행렬의 내적. 어텐션 메커니즘의 핵심입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1D 벡터 점곱 ===\n",
      "v1: tensor([1., 2., 3.])\n",
      "v2: tensor([4., 5., 6.])\n",
      "torch.dot(v1, v2) = 32.0\n",
      "수동 계산: 1×4 + 2×5 + 3×6 = 32\n",
      "v1 @ v2 = 32.0\n"
     ]
    }
   ],
   "source": [
    "# 기본: 1D 벡터의 점곱\n",
    "print(\"=== 1D 벡터 점곱 ===\")\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# 방법 1: dot()\n",
    "dot_result = torch.dot(v1, v2)\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"torch.dot(v1, v2) = {dot_result}\")\n",
    "print(f\"수동 계산: 1×4 + 2×5 + 3×6 = {1*4 + 2*5 + 3*6}\")\n",
    "\n",
    "# 방법 2: @ 연산자\n",
    "dot_result_2 = v1 @ v2\n",
    "print(f\"v1 @ v2 = {dot_result_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 행렬 곱셈 (Matrix Multiplication)\n",
    "print(\"\\n=== 2D 행렬 곱셈 ===\")\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])  # (3, 2)\n",
    "B = torch.tensor([[7.0, 8.0, 9.0],\n",
    "                   [10.0, 11.0, 12.0]])  # (2, 3)\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"A @ B shape: {(A @ B).shape}\")\n",
    "\n",
    "C = A @ B  # (3, 2) @ (2, 3) = (3, 3)\n",
    "print(f\"\\nA @ B =\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차원 배치 행렬 곱셈\n",
    "print(\"\\n=== 배치 행렬 곱셈 (Batch Matrix Multiplication) ===\")\n",
    "X = torch.randn(4, 3, 2)  # (배치=4, 시퀀스=3, 특성=2)\n",
    "W = torch.randn(2, 5)      # (입력=2, 출력=5)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "\n",
    "Y = X @ W  # (4, 3, 2) @ (2, 5) = (4, 3, 5)\n",
    "print(f\"X @ W shape: {Y.shape}\")\n",
    "print(f\"\\n배치와 시퀀스는 유지되고, 마지막 차원만 변환됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실전: 어텐션 스코어 계산\n",
    "print(\"\\n=== LLM 실전 예시: 어텐션 스코어 ===\")\n",
    "batch_size, seq_len, head_dim = 2, 4, 3\n",
    "\n",
    "# Query와 Key 텐서 (단순화)\n",
    "Q = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\n",
    "K = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "\n",
    "# Key를 전치\n",
    "K_T = K.transpose(-2, -1)  # (2, 3, 4) - 마지막 두 차원을 바꿈\n",
    "print(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\n",
    "\n",
    "# 어텐션 스코어 = Q @ K^T\n",
    "attention_scores = Q @ K_T  # (2, 4, 3) @ (2, 3, 4) = (2, 4, 4)\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(f\"\\n각 쿼리가 모든 키와의 유사도를 계산했습니다!\")\n",
    "print(f\"shape [2, 4, 4] = [배치, 쿼리_위치, 키_위치]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Broadcasting 규칙\n",
    "\n",
    "크기가 다른 텐서들을 자동으로 맞춰서 연산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting 예시\n",
    "print(\"=== Broadcasting 규칙 ===\")\n",
    "\n",
    "# 예1: 스칼라 vs 벡터\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = 10\n",
    "print(f\"a shape: {a.shape}, b shape: {b}\")\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"→ b가 (1,)에서 (3,)으로 자동 확장됨\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 예2: (3, 1) vs (1, 5)\n",
    "c = torch.randn(3, 1)\n",
    "d = torch.randn(1, 5)\n",
    "print(f\"c shape: {c.shape}, d shape: {d.shape}\")\n",
    "result = c + d\n",
    "print(f\"c + d shape: {result.shape}\")\n",
    "print(f\"→ (3, 1) + (1, 5) = (3, 5)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 예3: (batch, seq, emb) vs (emb,)\n",
    "token_embeddings = torch.randn(2, 10, 768)  # (배치, 토큰, 임베딩)\n",
    "bias = torch.randn(768)\n",
    "print(f\"token_embeddings shape: {token_embeddings.shape}\")\n",
    "print(f\"bias shape: {bias.shape}\")\n",
    "result = token_embeddings + bias\n",
    "print(f\"결과 shape: {result.shape}\")\n",
    "print(f\"→ 모든 배치, 모든 토큰에 같은 bias 적용됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Reshape vs View vs Permute 비교\n",
    "\n",
    "텐서 형태를 변경하는 다양한 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape vs View\n",
    "print(\"=== Reshape vs View ===\")\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(f\"원본: {x}, shape: {x.shape}\")\n",
    "\n",
    "# reshape: 메모리 레이아웃 무시하고 재구성\n",
    "reshaped = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{reshaped}\")\n",
    "\n",
    "# view: 메모리 연속성 필요 (같은 메모리, 다른 해석)\n",
    "viewed = x.view(3, 4)\n",
    "print(f\"\\nview(3, 4):\\n{viewed}\")\n",
    "\n",
    "print(\"\\n일반적으로 reshape()을 사용하는 것이 안전합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 가지 방법의 차이\n",
    "print(\"\\n=== 세 가지 변형 메서드 비교 ===\")\n",
    "original = torch.randn(2, 3, 4)\n",
    "print(f\"원본 shape: {original.shape}\")\n",
    "\n",
    "# 1. reshape: 자유로운 재구성\n",
    "reshaped = original.reshape(2, 12)  # (2, 3, 4) → (2, 12)\n",
    "print(f\"reshape(2, 12): {reshaped.shape}\")\n",
    "\n",
    "# 2. view: 메모리 연속성 필요 (같은 데이터 해석)\n",
    "viewed = original.view(2, 12)\n",
    "print(f\"view(2, 12): {viewed.shape}\")\n",
    "\n",
    "# 3. permute: 차원 순서 변경 (데이터 이동)\n",
    "permuted = original.permute(1, 0, 2)  # (2, 3, 4) → (3, 2, 4)\n",
    "print(f\"permute(1, 0, 2): {permuted.shape}\")\n",
    "\n",
    "print(\"\\n핵심: reshape/view는 숫자 배열, permute는 차원 순서 변경\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ 종합 실전 예시: GPT 어텐션 메커니즘\n",
    "\n",
    "지금까지 배운 모든 개념을 적용한 실제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=== 간단한 멀티헤드 어텐션 구현 ===\")\n",
    "\n",
    "# 설정\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "emb_dim = 8\n",
    "n_heads = 2\n",
    "head_dim = emb_dim // n_heads\n",
    "\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "print(f\"시퀀스 길이: {seq_length}\")\n",
    "print(f\"임베딩 차원: {emb_dim}\")\n",
    "print(f\"헤드 수: {n_heads}\")\n",
    "print(f\"헤드 차원: {head_dim}\\n\")\n",
    "\n",
    "# 입력 데이터\n",
    "X = torch.randn(batch_size, seq_length, emb_dim)\n",
    "print(f\"입력 X shape: {X.shape}\")\n",
    "\n",
    "# 선형 변환 (Q, K, V 계산)\n",
    "W_q = torch.randn(emb_dim, emb_dim)\n",
    "W_k = torch.randn(emb_dim, emb_dim)\n",
    "W_v = torch.randn(emb_dim, emb_dim)\n",
    "\n",
    "# 1단계: 선형 변환\n",
    "Q = X @ W_q  # (2, 4, 8)\n",
    "K = X @ W_k  # (2, 4, 8)\n",
    "V = X @ W_v  # (2, 4, 8)\n",
    "print(f\"\\n1단계: 선형 변환\")\n",
    "print(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n",
    "\n",
    "# 2단계: 헤드로 분할\n",
    "Q = Q.reshape(batch_size, seq_length, n_heads, head_dim)  # (2, 4, 2, 4)\n",
    "K = K.reshape(batch_size, seq_length, n_heads, head_dim)\n",
    "V = V.reshape(batch_size, seq_length, n_heads, head_dim)\n",
    "print(f\"\\n2단계: 헤드로 분할\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "\n",
    "# 3단계: 헤드별로 계산하기 위해 순서 변경\n",
    "Q = Q.permute(0, 2, 1, 3)  # (2, 2, 4, 4) - [배치, 헤드, 시퀀스, 헤드_차원]\n",
    "K = K.permute(0, 2, 1, 3)\n",
    "V = V.permute(0, 2, 1, 3)\n",
    "print(f\"\\n3단계: 차원 재배열\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"이제 각 헤드별로 병렬 계산 가능!\")\n",
    "\n",
    "# 4단계: 어텐션 스코어 계산\n",
    "# scores = Q @ K^T / sqrt(head_dim)\n",
    "K_T = K.transpose(-2, -1)  # (2, 2, 4, 4) 마지막 두 차원 전치\n",
    "scores = Q @ K_T / (head_dim ** 0.5)  # (2, 2, 4, 4)\n",
    "print(f\"\\n4단계: 어텐션 스코어\")\n",
    "print(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\n",
    "print(f\"scores = Q @ K^T / sqrt(head_dim)\")\n",
    "print(f\"scores shape: {scores.shape}\")\n",
    "\n",
    "# 5단계: Softmax\n",
    "weights = F.softmax(scores, dim=-1)  # (2, 2, 4, 4)\n",
    "print(f\"\\n5단계: Softmax 가중치\")\n",
    "print(f\"weights shape: {weights.shape}\")\n",
    "\n",
    "# 6단계: 값에 가중치 적용\n",
    "output = weights @ V  # (2, 2, 4, 4) @ (2, 2, 4, 4) = (2, 2, 4, 4)\n",
    "print(f\"\\n6단계: 값 가중치 적용\")\n",
    "print(f\"output = weights @ V\")\n",
    "print(f\"output shape: {output.shape}\")\n",
    "\n",
    "# 7단계: 헤드 다시 합치기\n",
    "output = output.permute(0, 2, 1, 3)  # (2, 4, 2, 4)\n",
    "print(f\"\\n7단계: 차원 재배열\")\n",
    "print(f\"output.permute(0, 2, 1, 3) shape: {output.shape}\")\n",
    "\n",
    "# 8단계: 헤드 연결\n",
    "output = output.reshape(batch_size, seq_length, emb_dim)  # (2, 4, 8)\n",
    "print(f\"\\n8단계: 헤드 연결\")\n",
    "print(f\"output.reshape(...) shape: {output.shape}\")\n",
    "print(f\"\\n최종 결과 shape: {output.shape} (입력과 동일!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ 연산 복잡도 비교\n",
    "\n",
    "다양한 연산의 효율성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=== 연산 속도 비교 ===\")\n",
    "\n",
    "# 큰 텐서 생성\n",
    "big_tensor = torch.randn(1000, 1000, 100)\n",
    "print(f\"테스트 텐서 shape: {big_tensor.shape}\")\n",
    "\n",
    "# 1. reshape\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = big_tensor.reshape(1000, 100000)\n",
    "reshape_time = (time.time() - start) * 1000\n",
    "print(f\"reshape (100회): {reshape_time:.2f}ms\")\n",
    "\n",
    "# 2. view\n",
    "linear_tensor = big_tensor.clone()\n",
    "if linear_tensor.is_contiguous():\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = linear_tensor.view(1000, 100000)\n",
    "    view_time = (time.time() - start) * 1000\n",
    "    print(f\"view (100회): {view_time:.2f}ms\")\n",
    "\n",
    "# 3. permute\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = big_tensor.permute(2, 0, 1)\n",
    "permute_time = (time.time() - start) * 1000\n",
    "print(f\"permute (100회): {permute_time:.2f}ms\")\n",
    "\n",
    "print(\"\\n💡 Insight: reshape와 view는 메타데이터만 변경해서 매우 빠르지만,\")\n",
    "print(\"   permute는 실제 데이터를 이동시켜 시간이 걸립니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔟 체크리스트: 텐서 연산 마스터하기\n",
    "\n",
    "다음을 확인하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 개념 확인 체크리스트 ===\")\n",
    "print()\n",
    "\n",
    "checklist = [\n",
    "    (\"✓\", \"Shape 읽기: (2, 4, 8)이 무엇을 의미하는지 설명 가능\"),\n",
    "    (\"✓\", \"Indexing: x[0, :, :] vs x[:, 0, :] 차이점 이해\"),\n",
    "    (\"✓\", \"Squeeze/Unsqueeze: 차원 제거/추가 목적 이해\"),\n",
    "    (\"✓\", \"Transpose: 2D 행렬 전치와 permute로 고차원 전치 가능\"),\n",
    "    (\"✓\", \"Dot Product: 벡터, 행렬, 배치 연산 모두 가능\"),\n",
    "    (\"✓\", \"Broadcasting: 크기가 다른 텐서 연산 규칙 이해\"),\n",
    "    (\"✓\", \"Reshape vs View: 언제 어떤 것을 사용할지 판단\"),\n",
    "    (\"✓\", \"멀티헤드 어텐션: 8단계 프로세스 이해\"),\n",
    "]\n",
    "\n",
    "for status, item in checklist:\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "print(\"\\n모두 이해했다면 LLM 코드도 쉬워집니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고: ch03, ch04의 코드에서 이런 개념들이 실제로 사용되는 곳\n",
    "\n",
    "1. **ch03 - 어텐션 메커니즘**\n",
    "   - `torch.matmul()` 및 `@`: 점곱으로 어텐션 스코어 계산\n",
    "   - `transpose()`: Query와 Key 전치\n",
    "   - Broadcasting: 스칼라 연산 (온도 조절 등)\n",
    "\n",
    "2. **ch04 - GPT 모델**\n",
    "   - `reshape()`: 멀티헤드 분할\n",
    "   - `permute()`: 헤드 차원 재배열\n",
    "   - `squeeze()`: 배치 1일 때 차원 제거\n",
    "   - `unsqueeze()`: 마스크 차원 추가\n",
    "\n",
    "3. **ch05 - 훈련**\n",
    "   - 배치 처리 시 배치 차원 자동 확장\n",
    "   - 손실 계산 시 배치/시퀀스 평균화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
