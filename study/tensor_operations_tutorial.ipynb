{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고차원 텐서 완전 가이드\n",
    "\n",
    "## LLM 프로젝트에서 자주 사용되는 텐서 연산 마스터하기\n",
    "\n",
    "이 노트북에서는 PyTorch 텐서의 핵심 개념과 연산들을 단계별로 학습합니다:\n",
    "- 차원 분해 및 Shape 이해\n",
    "- Squeeze/Unsqueeze로 차원 조작\n",
    "- **전치(Transpose)**: 차원 재배열\n",
    "- **점곱(Dot Product)**: 벡터 및 행렬 연산\n",
    "- Broadcasting 규칙\n",
    "- LLM 실전 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ 차원 분해 (Shape Breakdown)\n",
    "\n",
    "고차원 텐서의 각 차원이 **무엇을 의미하는지** 명확히 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예: GPT의 어텐션 출력\n",
    "batch_size = 2\n",
    "seq_length = 1024\n",
    "emb_dim = 768\n",
    "\n",
    "attention_output = torch.randn(batch_size, seq_length, emb_dim)\n",
    "\n",
    "print(\"=== 어텐션 출력 텐서 ===\")\n",
    "print(f\"Shape: {attention_output.shape}\")\n",
    "print(f\"차원 0 (배치): {attention_output.shape[0]}개 문서\")\n",
    "print(f\"차원 1 (시퀀스): {attention_output.shape[1]}개 토큰\")\n",
    "print(f\"차원 2 (임베딩): {attention_output.shape[2]}차원 벡터\")\n",
    "print(f\"\\n총 원소 수: {attention_output.numel()}\")\n",
    "print(f\"메모리 크기: {attention_output.element_size() * attention_output.numel() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ 특정 차원에 집중 (Indexing)\n",
    "\n",
    "한 번에 하나의 차원만 선택해서 생각하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작은 텐서로 실험\n",
    "x = torch.randn(2, 5, 3)  # (배치=2, 토큰=5, 차원=3)\n",
    "print(\"원본 텐서:\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"내용:\\n{x}\")\n",
    "\n",
    "print(\"\\n=== 배치 0만 선택 ===\")\n",
    "batch_0 = x[0]  # (5, 3)\n",
    "print(f\"Shape: {batch_0.shape}\")\n",
    "print(f\"내용:\\n{batch_0}\")\n",
    "\n",
    "print(\"\\n=== 배치 0의 첫 토큰 ===\")\n",
    "first_token = x[0, 0]  # (3,)\n",
    "print(f\"Shape: {first_token.shape}\")\n",
    "print(f\"내용: {first_token}\")\n",
    "\n",
    "print(\"\\n=== 모든 배치의 첫 토큰 ===\")\n",
    "first_tokens = x[:, 0, :]  # (2, 3)\n",
    "print(f\"Shape: {first_tokens.shape}\")\n",
    "print(f\"내용:\\n{first_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Squeeze & Unsqueeze (차원 제거/추가)\n",
    "\n",
    "불필요한 차원을 제거하거나 새 차원을 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze: 크기가 1인 차원 제거\n",
    "x_1 = torch.randn(1, 10, 768)  # (배치=1, 토큰=10, 임베딩=768)\n",
    "print(\"=== Squeeze (차원 제거) ===\")\n",
    "print(f\"원본 shape: {x_1.shape}\")\n",
    "x_1_squeezed = x_1.squeeze()  # 모든 크기 1인 차원 제거\n",
    "print(f\"squeeze() 후: {x_1_squeezed.shape}\")\n",
    "\n",
    "# 특정 차원만 제거\n",
    "x_1_squeezed_0 = x_1.squeeze(0)  # 차원 0만 제거\n",
    "print(f\"squeeze(0) 후: {x_1_squeezed_0.shape}\")\n",
    "\n",
    "# Unsqueeze: 새 차원 추가\n",
    "print(\"\\n=== Unsqueeze (차원 추가) ===\")\n",
    "x_2 = torch.randn(10, 768)  # (토큰=10, 임베딩=768)\n",
    "print(f\"원본 shape: {x_2.shape}\")\n",
    "x_2_unsqueezed_0 = x_2.unsqueeze(0)  # 맨 앞에 차원 추가\n",
    "print(f\"unsqueeze(0) 후: {x_2_unsqueezed_0.shape}\")  # (1, 10, 768)\n",
    "x_2_unsqueezed_1 = x_2.unsqueeze(1)  # 중간에 차원 추가\n",
    "print(f\"unsqueeze(1) 후: {x_2_unsqueezed_1.shape}\")  # (10, 1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ 전치 (Transpose/Permute) 🔄\n",
    "\n",
    "텐서의 차원 순서를 바꾸기. 어텐션, 행렬 곱셈 등에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 행렬의 전치\n",
    "print(\"=== 2D 행렬 전치 ===\")\n",
    "matrix = torch.arange(6).reshape(2, 3)\n",
    "print(f\"원본 (2, 3):\\n{matrix}\")\n",
    "transposed = matrix.T  # 또는 matrix.transpose(0, 1)\n",
    "print(f\"\\n전치 후 (3, 2):\\n{transposed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차원 텐서의 전치 (Permute)\n",
    "print(\"\\n=== 고차원 텐서 전치 (Permute) ===\")\n",
    "tensor = torch.randn(2, 3, 4, 5)  # (배치, 토큰, 헤드, 차원)\n",
    "print(f\"원본 shape: {tensor.shape}\")\n",
    "\n",
    "# 차원 0과 2를 교환\n",
    "permuted = tensor.permute(0, 2, 1, 3)  # (배치, 헤드, 토큰, 차원)\n",
    "print(f\"permute(0, 2, 1, 3) 후: {permuted.shape}\")\n",
    "\n",
    "# 모든 차원을 역순으로\n",
    "reversed_dims = tensor.permute(3, 2, 1, 0)  # (차원, 헤드, 토큰, 배치)\n",
    "print(f\"permute(3, 2, 1, 0) 후: {reversed_dims.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실전: 멀티헤드 어텐션에서의 전치\n",
    "print(\"\\n=== LLM 실전 예시: 멀티헤드 어텐션 ===\")\nbatch_size, seq_len, emb_dim, n_heads = 2, 4, 8, 2\nhead_dim = emb_dim // n_heads\n\n# Q, K, V를 head 형태로 변환: (batch, seq, emb_dim) → (batch, seq, n_heads, head_dim)\nQ = torch.randn(batch_size, seq_len, emb_dim)\nprint(f\"Q 원본: {Q.shape}\")\n\nQ_heads = Q.reshape(batch_size, seq_len, n_heads, head_dim)\nprint(f\"Q reshape 후: {Q_heads.shape}\")\n\n# 헤드 계산을 위해 차원 재배열: (batch, seq, n_heads, head_dim) → (batch, n_heads, seq, head_dim)\nQ_heads = Q_heads.permute(0, 2, 1, 3)\nprint(f\"Q permute(0, 2, 1, 3) 후: {Q_heads.shape}\")\nprint(\"\\n이제 각 헤드별로 병렬로 어텐션 연산 가능!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ 점곱 (Dot Product) 🔹\n",
    "\n",
    "두 벡터 또는 행렬의 내적. 어텐션 메커니즘의 핵심입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본: 1D 벡터의 점곱\n",
    "print(\"=== 1D 벡터 점곱 ===\")\nv1 = torch.tensor([1.0, 2.0, 3.0])\nv2 = torch.tensor([4.0, 5.0, 6.0])\n\n# 방법 1: dot()\ndot_result = torch.dot(v1, v2)\nprint(f\"v1: {v1}\")\nprint(f\"v2: {v2}\")\nprint(f\"torch.dot(v1, v2) = {dot_result}\")\nprint(f\"수동 계산: 1×4 + 2×5 + 3×6 = {1*4 + 2*5 + 3*6}\")\n\n# 방법 2: @ 연산자\ndot_result_2 = v1 @ v2\nprint(f\"v1 @ v2 = {dot_result_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 행렬 곱셈 (Matrix Multiplication)\n",
    "print(\"\\n=== 2D 행렬 곱셈 ===\")\nA = torch.tensor([[1.0, 2.0],\n                   [3.0, 4.0],\n                   [5.0, 6.0]])  # (3, 2)\nB = torch.tensor([[7.0, 8.0, 9.0],\n                   [10.0, 11.0, 12.0]])  # (2, 3)\n\nprint(f\"A shape: {A.shape}\")\nprint(f\"B shape: {B.shape}\")\nprint(f\"A @ B shape: {(A @ B).shape}\")\n\nC = A @ B  # (3, 2) @ (2, 3) = (3, 3)\nprint(f\"\\nA @ B =\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차원 배치 행렬 곱셈\n",
    "print(\"\\n=== 배치 행렬 곱셈 (Batch Matrix Multiplication) ===\")\nX = torch.randn(4, 3, 2)  # (배치=4, 시퀀스=3, 특성=2)\nW = torch.randn(2, 5)      # (입력=2, 출력=5)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"W shape: {W.shape}\")\n\nY = X @ W  # (4, 3, 2) @ (2, 5) = (4, 3, 5)\nprint(f\"X @ W shape: {Y.shape}\")\nprint(f\"\\n배치와 시퀀스는 유지되고, 마지막 차원만 변환됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실전: 어텐션 스코어 계산\n",
    "print(\"\\n=== LLM 실전 예시: 어텐션 스코어 ===\")\nbatch_size, seq_len, head_dim = 2, 4, 3\n\n# Query와 Key 텐서 (단순화)\nQ = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\nK = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\n\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"K shape: {K.shape}\")\n\n# Key를 전치\nK_T = K.transpose(-2, -1)  # (2, 3, 4) - 마지막 두 차원을 바꿈\nprint(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\n\n# 어텐션 스코어 = Q @ K^T\nattention_scores = Q @ K_T  # (2, 4, 3) @ (2, 3, 4) = (2, 4, 4)\nprint(f\"Attention scores shape: {attention_scores.shape}\")\nprint(f\"\\n각 쿼리가 모든 키와의 유사도를 계산했습니다!\")\nprint(f\"shape [2, 4, 4] = [배치, 쿼리_위치, 키_위치]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Broadcasting 규칙\n",
    "\n",
    "크기가 다른 텐서들을 자동으로 맞춰서 연산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting 예시\n",
    "print(\"=== Broadcasting 규칙 ===\")\n",
    "\n",
    "# 예1: 스칼라 vs 벡터\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = 10\n",
    "print(f\"a shape: {a.shape}, b shape: {b}\")\nprint(f\"a + b = {a + b}\")\nprint(f\"→ b가 (1,)에서 (3,)으로 자동 확장됨\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# 예2: (3, 1) vs (1, 5)\nc = torch.randn(3, 1)\nd = torch.randn(1, 5)\nprint(f\"c shape: {c.shape}, d shape: {d.shape}\")\nresult = c + d\nprint(f\"c + d shape: {result.shape}\")\nprint(f\"→ (3, 1) + (1, 5) = (3, 5)\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# 예3: (batch, seq, emb) vs (emb,)\ntoken_embeddings = torch.randn(2, 10, 768)  # (배치, 토큰, 임베딩)\nbias = torch.randn(768)\nprint(f\"token_embeddings shape: {token_embeddings.shape}\")\nprint(f\"bias shape: {bias.shape}\")\nresult = token_embeddings + bias\nprint(f\"결과 shape: {result.shape}\")\nprint(f\"→ 모든 배치, 모든 토큰에 같은 bias 적용됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Reshape vs View vs Permute 비교\n",
    "\n",
    "텐서 형태를 변경하는 다양한 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape vs View\n",
    "print(\"=== Reshape vs View ===\")\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\nprint(f\"원본: {x}, shape: {x.shape}\")\n",
    "\n",
    "# reshape: 메모리 레이아웃 무시하고 재구성\n",
    "reshaped = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{reshaped}\")\n",
    "\n",
    "# view: 메모리 연속성 필요 (같은 메모리, 다른 해석)\n",
    "viewed = x.view(3, 4)\n",
    "print(f\"\\nview(3, 4):\\n{viewed}\")\n",
    "\n",
    "print(\"\\n일반적으로 reshape()을 사용하는 것이 안전합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 가지 방법의 차이\n",
    "print(\"\\n=== 세 가지 변형 메서드 비교 ===\")\noriginal = torch.randn(2, 3, 4)\nprint(f\"원본 shape: {original.shape}\")\n\n# 1. reshape: 자유로운 재구성\nreshaped = original.reshape(2, 12)  # (2, 3, 4) → (2, 12)\nprint(f\"reshape(2, 12): {reshaped.shape}\")\n\n# 2. view: 메모리 연속성 필요 (같은 데이터 해석)\nviewed = original.view(2, 12)\nprint(f\"view(2, 12): {viewed.shape}\")\n\n# 3. permute: 차원 순서 변경 (데이터 이동)\npermuted = original.permute(1, 0, 2)  # (2, 3, 4) → (3, 2, 4)\nprint(f\"permute(1, 0, 2): {permuted.shape}\")\n\nprint(\"\\n핵심: reshape/view는 숫자 배열, permute는 차원 순서 변경\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ 종합 실전 예시: GPT 어텐션 메커니즘\n",
    "\n",
    "지금까지 배운 모든 개념을 적용한 실제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn.functional as F\n\nprint(\"=== 간단한 멀티헤드 어텐션 구현 ===\")\n\n# 설정\nbatch_size = 2\nseq_length = 4\nemb_dim = 8\nn_heads = 2\nhead_dim = emb_dim // n_heads\n\nprint(f\"배치 크기: {batch_size}\")\nprint(f\"시퀀스 길이: {seq_length}\")\nprint(f\"임베딩 차원: {emb_dim}\")\nprint(f\"헤드 수: {n_heads}\")\nprint(f\"헤드 차원: {head_dim}\\n\")\n\n# 입력 데이터\nX = torch.randn(batch_size, seq_length, emb_dim)\nprint(f\"입력 X shape: {X.shape}\")\n\n# 선형 변환 (Q, K, V 계산)\nW_q = torch.randn(emb_dim, emb_dim)\nW_k = torch.randn(emb_dim, emb_dim)\nW_v = torch.randn(emb_dim, emb_dim)\n\n# 1단계: 선형 변환\nQ = X @ W_q  # (2, 4, 8)\nK = X @ W_k  # (2, 4, 8)\nV = X @ W_v  # (2, 4, 8)\nprint(f\"\\n1단계: 선형 변환\")\nprint(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n\n# 2단계: 헤드로 분할\nQ = Q.reshape(batch_size, seq_length, n_heads, head_dim)  # (2, 4, 2, 4)\nK = K.reshape(batch_size, seq_length, n_heads, head_dim)\nV = V.reshape(batch_size, seq_length, n_heads, head_dim)\nprint(f\"\\n2단계: 헤드로 분할\")\nprint(f\"Q shape: {Q.shape}\")\n\n# 3단계: 헤드별로 계산하기 위해 순서 변경\nQ = Q.permute(0, 2, 1, 3)  # (2, 2, 4, 4) - [배치, 헤드, 시퀀스, 헤드_차원]\nK = K.permute(0, 2, 1, 3)\nV = V.permute(0, 2, 1, 3)\nprint(f\"\\n3단계: 차원 재배열\")\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"이제 각 헤드별로 병렬 계산 가능!\")\n\n# 4단계: 어텐션 스코어 계산\n# scores = Q @ K^T / sqrt(head_dim)\nK_T = K.transpose(-2, -1)  # (2, 2, 4, 4) 마지막 두 차원 전치\nscores = Q @ K_T / (head_dim ** 0.5)  # (2, 2, 4, 4)\nprint(f\"\\n4단계: 어텐션 스코어\")\nprint(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\nprint(f\"scores = Q @ K^T / sqrt(head_dim)\")\nprint(f\"scores shape: {scores.shape}\")\n\n# 5단계: Softmax\nweights = F.softmax(scores, dim=-1)  # (2, 2, 4, 4)\nprint(f\"\\n5단계: Softmax 가중치\")\nprint(f\"weights shape: {weights.shape}\")\n\n# 6단계: 값에 가중치 적용\noutput = weights @ V  # (2, 2, 4, 4) @ (2, 2, 4, 4) = (2, 2, 4, 4)\nprint(f\"\\n6단계: 값 가중치 적용\")\nprint(f\"output = weights @ V\")\nprint(f\"output shape: {output.shape}\")\n\n# 7단계: 헤드 다시 합치기\noutput = output.permute(0, 2, 1, 3)  # (2, 4, 2, 4)\nprint(f\"\\n7단계: 차원 재배열\")\nprint(f\"output.permute(0, 2, 1, 3) shape: {output.shape}\")\n\n# 8단계: 헤드 연결\noutput = output.reshape(batch_size, seq_length, emb_dim)  # (2, 4, 8)\nprint(f\"\\n8단계: 헤드 연결\")\nprint(f\"output.reshape(...) shape: {output.shape}\")\nprint(f\"\\n최종 결과 shape: {output.shape} (입력과 동일!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ 연산 복잡도 비교\n",
    "\n",
    "다양한 연산의 효율성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n\nprint(\"=== 연산 속도 비교 ===\")\n\n# 큰 텐서 생성\nbig_tensor = torch.randn(1000, 1000, 100)\nprint(f\"테스트 텐서 shape: {big_tensor.shape}\")\n\n# 1. reshape\nstart = time.time()\nfor _ in range(100):\n    _ = big_tensor.reshape(1000, 100000)\nreshape_time = (time.time() - start) * 1000\nprint(f\"reshape (100회): {reshape_time:.2f}ms\")\n\n# 2. view\nlinear_tensor = big_tensor.clone()\nif linear_tensor.is_contiguous():\n    start = time.time()\n    for _ in range(100):\n        _ = linear_tensor.view(1000, 100000)\n    view_time = (time.time() - start) * 1000\n    print(f\"view (100회): {view_time:.2f}ms\")\n\n# 3. permute\nstart = time.time()\nfor _ in range(100):\n    _ = big_tensor.permute(2, 0, 1)\npermute_time = (time.time() - start) * 1000\nprint(f\"permute (100회): {permute_time:.2f}ms\")\n\nprint(\"\\n💡 Insight: reshape와 view는 메타데이터만 변경해서 매우 빠르지만,\")\nprint(\"   permute는 실제 데이터를 이동시켜 시간이 걸립니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔟 체크리스트: 텐서 연산 마스터하기\n",
    "\n",
    "다음을 확인하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 개념 확인 체크리스트 ===\")\nprint()\n\nchecklist = [\n    (\"✓\", \"Shape 읽기: (2, 4, 8)이 무엇을 의미하는지 설명 가능\"),\n    (\"✓\", \"Indexing: x[0, :, :] vs x[:, 0, :] 차이점 이해\"),\n    (\"✓\", \"Squeeze/Unsqueeze: 차원 제거/추가 목적 이해\"),\n    (\"✓\", \"Transpose: 2D 행렬 전치와 permute로 고차원 전치 가능\"),\n    (\"✓\", \"Dot Product: 벡터, 행렬, 배치 연산 모두 가능\"),\n    (\"✓\", \"Broadcasting: 크기가 다른 텐서 연산 규칙 이해\"),\n    (\"✓\", \"Reshape vs View: 언제 어떤 것을 사용할지 판단\"),\n    (\"✓\", \"멀티헤드 어텐션: 8단계 프로세스 이해\"),\n]\n\nfor status, item in checklist:\n    print(f\"{status} {item}\")\n\nprint(\"\\n모두 이해했다면 LLM 코드도 쉬워집니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고: ch03, ch04의 코드에서 이런 개념들이 실제로 사용되는 곳\n",
    "\n",
    "1. **ch03 - 어텐션 메커니즘**\n",
    "   - `torch.matmul()` 및 `@`: 점곱으로 어텐션 스코어 계산\n",
    "   - `transpose()`: Query와 Key 전치\n",
    "   - Broadcasting: 스칼라 연산 (온도 조절 등)\n",
    "\n",
    "2. **ch04 - GPT 모델**\n",
    "   - `reshape()`: 멀티헤드 분할\n",
    "   - `permute()`: 헤드 차원 재배열\n",
    "   - `squeeze()`: 배치 1일 때 차원 제거\n",
    "   - `unsqueeze()`: 마스크 차원 추가\n",
    "\n",
    "3. **ch05 - 훈련**\n",
    "   - 배치 처리 시 배치 차원 자동 확장\n",
    "   - 손실 계산 시 배치/시퀀스 평균화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
