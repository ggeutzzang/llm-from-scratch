{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê³ ì°¨ì› í…ì„œ ì™„ì „ ê°€ì´ë“œ\n",
    "\n",
    "## LLM í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ì„œ ì—°ì‚° ë§ˆìŠ¤í„°í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” PyTorch í…ì„œì˜ í•µì‹¬ ê°œë…ê³¼ ì—°ì‚°ë“¤ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤:\n",
    "- ì°¨ì› ë¶„í•´ ë° Shape ì´í•´\n",
    "- Squeeze/Unsqueezeë¡œ ì°¨ì› ì¡°ì‘\n",
    "- **ì „ì¹˜(Transpose)**: ì°¨ì› ì¬ë°°ì—´\n",
    "- **ì ê³±(Dot Product)**: ë²¡í„° ë° í–‰ë ¬ ì—°ì‚°\n",
    "- Broadcasting ê·œì¹™\n",
    "- LLM ì‹¤ì „ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ì°¨ì› ë¶„í•´ (Shape Breakdown)\n",
    "\n",
    "ê³ ì°¨ì› í…ì„œì˜ ê° ì°¨ì›ì´ **ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€** ëª…í™•íˆ ì´í•´í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆ: GPTì˜ ì–´í…ì…˜ ì¶œë ¥\n",
    "batch_size = 2\n",
    "seq_length = 1024\n",
    "emb_dim = 768\n",
    "\n",
    "attention_output = torch.randn(batch_size, seq_length, emb_dim)\n",
    "\n",
    "print(\"=== ì–´í…ì…˜ ì¶œë ¥ í…ì„œ ===\")\n",
    "print(f\"Shape: {attention_output.shape}\")\n",
    "print(f\"ì°¨ì› 0 (ë°°ì¹˜): {attention_output.shape[0]}ê°œ ë¬¸ì„œ\")\n",
    "print(f\"ì°¨ì› 1 (ì‹œí€€ìŠ¤): {attention_output.shape[1]}ê°œ í† í°\")\n",
    "print(f\"ì°¨ì› 2 (ì„ë² ë”©): {attention_output.shape[2]}ì°¨ì› ë²¡í„°\")\n",
    "print(f\"\\nì´ ì›ì†Œ ìˆ˜: {attention_output.numel()}\")\n",
    "print(f\"ë©”ëª¨ë¦¬ í¬ê¸°: {attention_output.element_size() * attention_output.numel() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ íŠ¹ì • ì°¨ì›ì— ì§‘ì¤‘ (Indexing)\n",
    "\n",
    "í•œ ë²ˆì— í•˜ë‚˜ì˜ ì°¨ì›ë§Œ ì„ íƒí•´ì„œ ìƒê°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì€ í…ì„œë¡œ ì‹¤í—˜\n",
    "x = torch.randn(2, 5, 3)  # (ë°°ì¹˜=2, í† í°=5, ì°¨ì›=3)\n",
    "print(\"ì›ë³¸ í…ì„œ:\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"ë‚´ìš©:\\n{x}\")\n",
    "\n",
    "print(\"\\n=== ë°°ì¹˜ 0ë§Œ ì„ íƒ ===\")\n",
    "batch_0 = x[0]  # (5, 3)\n",
    "print(f\"Shape: {batch_0.shape}\")\n",
    "print(f\"ë‚´ìš©:\\n{batch_0}\")\n",
    "\n",
    "print(\"\\n=== ë°°ì¹˜ 0ì˜ ì²« í† í° ===\")\n",
    "first_token = x[0, 0]  # (3,)\n",
    "print(f\"Shape: {first_token.shape}\")\n",
    "print(f\"ë‚´ìš©: {first_token}\")\n",
    "\n",
    "print(\"\\n=== ëª¨ë“  ë°°ì¹˜ì˜ ì²« í† í° ===\")\n",
    "first_tokens = x[:, 0, :]  # (2, 3)\n",
    "print(f\"Shape: {first_tokens.shape}\")\n",
    "print(f\"ë‚´ìš©:\\n{first_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Squeeze & Unsqueeze (ì°¨ì› ì œê±°/ì¶”ê°€)\n",
    "\n",
    "ë¶ˆí•„ìš”í•œ ì°¨ì›ì„ ì œê±°í•˜ê±°ë‚˜ ìƒˆ ì°¨ì›ì„ ì¶”ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze: í¬ê¸°ê°€ 1ì¸ ì°¨ì› ì œê±°\n",
    "x_1 = torch.randn(1, 10, 768)  # (ë°°ì¹˜=1, í† í°=10, ì„ë² ë”©=768)\n",
    "print(\"=== Squeeze (ì°¨ì› ì œê±°) ===\")\n",
    "print(f\"ì›ë³¸ shape: {x_1.shape}\")\n",
    "x_1_squeezed = x_1.squeeze()  # ëª¨ë“  í¬ê¸° 1ì¸ ì°¨ì› ì œê±°\n",
    "print(f\"squeeze() í›„: {x_1_squeezed.shape}\")\n",
    "\n",
    "# íŠ¹ì • ì°¨ì›ë§Œ ì œê±°\n",
    "x_1_squeezed_0 = x_1.squeeze(0)  # ì°¨ì› 0ë§Œ ì œê±°\n",
    "print(f\"squeeze(0) í›„: {x_1_squeezed_0.shape}\")\n",
    "\n",
    "# Unsqueeze: ìƒˆ ì°¨ì› ì¶”ê°€\n",
    "print(\"\\n=== Unsqueeze (ì°¨ì› ì¶”ê°€) ===\")\n",
    "x_2 = torch.randn(10, 768)  # (í† í°=10, ì„ë² ë”©=768)\n",
    "print(f\"ì›ë³¸ shape: {x_2.shape}\")\n",
    "x_2_unsqueezed_0 = x_2.unsqueeze(0)  # ë§¨ ì•ì— ì°¨ì› ì¶”ê°€\n",
    "print(f\"unsqueeze(0) í›„: {x_2_unsqueezed_0.shape}\")  # (1, 10, 768)\n",
    "x_2_unsqueezed_1 = x_2.unsqueeze(1)  # ì¤‘ê°„ì— ì°¨ì› ì¶”ê°€\n",
    "print(f\"unsqueeze(1) í›„: {x_2_unsqueezed_1.shape}\")  # (10, 1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ì „ì¹˜ (Transpose/Permute) ğŸ”„\n",
    "\n",
    "í…ì„œì˜ ì°¨ì› ìˆœì„œë¥¼ ë°”ê¾¸ê¸°. ì–´í…ì…˜, í–‰ë ¬ ê³±ì…ˆ ë“±ì—ì„œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D í–‰ë ¬ì˜ ì „ì¹˜\n",
    "print(\"=== 2D í–‰ë ¬ ì „ì¹˜ ===\")\n",
    "matrix = torch.arange(6).reshape(2, 3)\n",
    "print(f\"ì›ë³¸ (2, 3):\\n{matrix}\")\n",
    "transposed = matrix.T  # ë˜ëŠ” matrix.transpose(0, 1)\n",
    "print(f\"\\nì „ì¹˜ í›„ (3, 2):\\n{transposed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì°¨ì› í…ì„œì˜ ì „ì¹˜ (Permute)\n",
    "print(\"\\n=== ê³ ì°¨ì› í…ì„œ ì „ì¹˜ (Permute) ===\")\n",
    "tensor = torch.randn(2, 3, 4, 5)  # (ë°°ì¹˜, í† í°, í—¤ë“œ, ì°¨ì›)\n",
    "print(f\"ì›ë³¸ shape: {tensor.shape}\")\n",
    "\n",
    "# ì°¨ì› 0ê³¼ 2ë¥¼ êµí™˜\n",
    "permuted = tensor.permute(0, 2, 1, 3)  # (ë°°ì¹˜, í—¤ë“œ, í† í°, ì°¨ì›)\n",
    "print(f\"permute(0, 2, 1, 3) í›„: {permuted.shape}\")\n",
    "\n",
    "# ëª¨ë“  ì°¨ì›ì„ ì—­ìˆœìœ¼ë¡œ\n",
    "reversed_dims = tensor.permute(3, 2, 1, 0)  # (ì°¨ì›, í—¤ë“œ, í† í°, ë°°ì¹˜)\n",
    "print(f\"permute(3, 2, 1, 0) í›„: {reversed_dims.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì „: ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ ì „ì¹˜\n",
    "print(\"\\n=== LLM ì‹¤ì „ ì˜ˆì‹œ: ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ===\")\nbatch_size, seq_len, emb_dim, n_heads = 2, 4, 8, 2\nhead_dim = emb_dim // n_heads\n\n# Q, K, Vë¥¼ head í˜•íƒœë¡œ ë³€í™˜: (batch, seq, emb_dim) â†’ (batch, seq, n_heads, head_dim)\nQ = torch.randn(batch_size, seq_len, emb_dim)\nprint(f\"Q ì›ë³¸: {Q.shape}\")\n\nQ_heads = Q.reshape(batch_size, seq_len, n_heads, head_dim)\nprint(f\"Q reshape í›„: {Q_heads.shape}\")\n\n# í—¤ë“œ ê³„ì‚°ì„ ìœ„í•´ ì°¨ì› ì¬ë°°ì—´: (batch, seq, n_heads, head_dim) â†’ (batch, n_heads, seq, head_dim)\nQ_heads = Q_heads.permute(0, 2, 1, 3)\nprint(f\"Q permute(0, 2, 1, 3) í›„: {Q_heads.shape}\")\nprint(\"\\nì´ì œ ê° í—¤ë“œë³„ë¡œ ë³‘ë ¬ë¡œ ì–´í…ì…˜ ì—°ì‚° ê°€ëŠ¥!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ì ê³± (Dot Product) ğŸ”¹\n",
    "\n",
    "ë‘ ë²¡í„° ë˜ëŠ” í–‰ë ¬ì˜ ë‚´ì . ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸: 1D ë²¡í„°ì˜ ì ê³±\n",
    "print(\"=== 1D ë²¡í„° ì ê³± ===\")\nv1 = torch.tensor([1.0, 2.0, 3.0])\nv2 = torch.tensor([4.0, 5.0, 6.0])\n\n# ë°©ë²• 1: dot()\ndot_result = torch.dot(v1, v2)\nprint(f\"v1: {v1}\")\nprint(f\"v2: {v2}\")\nprint(f\"torch.dot(v1, v2) = {dot_result}\")\nprint(f\"ìˆ˜ë™ ê³„ì‚°: 1Ã—4 + 2Ã—5 + 3Ã—6 = {1*4 + 2*5 + 3*6}\")\n\n# ë°©ë²• 2: @ ì—°ì‚°ì\ndot_result_2 = v1 @ v2\nprint(f\"v1 @ v2 = {dot_result_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D í–‰ë ¬ ê³±ì…ˆ (Matrix Multiplication)\n",
    "print(\"\\n=== 2D í–‰ë ¬ ê³±ì…ˆ ===\")\nA = torch.tensor([[1.0, 2.0],\n                   [3.0, 4.0],\n                   [5.0, 6.0]])  # (3, 2)\nB = torch.tensor([[7.0, 8.0, 9.0],\n                   [10.0, 11.0, 12.0]])  # (2, 3)\n\nprint(f\"A shape: {A.shape}\")\nprint(f\"B shape: {B.shape}\")\nprint(f\"A @ B shape: {(A @ B).shape}\")\n\nC = A @ B  # (3, 2) @ (2, 3) = (3, 3)\nprint(f\"\\nA @ B =\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì°¨ì› ë°°ì¹˜ í–‰ë ¬ ê³±ì…ˆ\n",
    "print(\"\\n=== ë°°ì¹˜ í–‰ë ¬ ê³±ì…ˆ (Batch Matrix Multiplication) ===\")\nX = torch.randn(4, 3, 2)  # (ë°°ì¹˜=4, ì‹œí€€ìŠ¤=3, íŠ¹ì„±=2)\nW = torch.randn(2, 5)      # (ì…ë ¥=2, ì¶œë ¥=5)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"W shape: {W.shape}\")\n\nY = X @ W  # (4, 3, 2) @ (2, 5) = (4, 3, 5)\nprint(f\"X @ W shape: {Y.shape}\")\nprint(f\"\\në°°ì¹˜ì™€ ì‹œí€€ìŠ¤ëŠ” ìœ ì§€ë˜ê³ , ë§ˆì§€ë§‰ ì°¨ì›ë§Œ ë³€í™˜ë¨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì „: ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "print(\"\\n=== LLM ì‹¤ì „ ì˜ˆì‹œ: ì–´í…ì…˜ ìŠ¤ì½”ì–´ ===\")\nbatch_size, seq_len, head_dim = 2, 4, 3\n\n# Queryì™€ Key í…ì„œ (ë‹¨ìˆœí™”)\nQ = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\nK = torch.randn(batch_size, seq_len, head_dim)  # (2, 4, 3)\n\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"K shape: {K.shape}\")\n\n# Keyë¥¼ ì „ì¹˜\nK_T = K.transpose(-2, -1)  # (2, 3, 4) - ë§ˆì§€ë§‰ ë‘ ì°¨ì›ì„ ë°”ê¿ˆ\nprint(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\n\n# ì–´í…ì…˜ ìŠ¤ì½”ì–´ = Q @ K^T\nattention_scores = Q @ K_T  # (2, 4, 3) @ (2, 3, 4) = (2, 4, 4)\nprint(f\"Attention scores shape: {attention_scores.shape}\")\nprint(f\"\\nê° ì¿¼ë¦¬ê°€ ëª¨ë“  í‚¤ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤!\")\nprint(f\"shape [2, 4, 4] = [ë°°ì¹˜, ì¿¼ë¦¬_ìœ„ì¹˜, í‚¤_ìœ„ì¹˜]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Broadcasting ê·œì¹™\n",
    "\n",
    "í¬ê¸°ê°€ ë‹¤ë¥¸ í…ì„œë“¤ì„ ìë™ìœ¼ë¡œ ë§ì¶°ì„œ ì—°ì‚°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting ì˜ˆì‹œ\n",
    "print(\"=== Broadcasting ê·œì¹™ ===\")\n",
    "\n",
    "# ì˜ˆ1: ìŠ¤ì¹¼ë¼ vs ë²¡í„°\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = 10\n",
    "print(f\"a shape: {a.shape}, b shape: {b}\")\nprint(f\"a + b = {a + b}\")\nprint(f\"â†’ bê°€ (1,)ì—ì„œ (3,)ìœ¼ë¡œ ìë™ í™•ì¥ë¨\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# ì˜ˆ2: (3, 1) vs (1, 5)\nc = torch.randn(3, 1)\nd = torch.randn(1, 5)\nprint(f\"c shape: {c.shape}, d shape: {d.shape}\")\nresult = c + d\nprint(f\"c + d shape: {result.shape}\")\nprint(f\"â†’ (3, 1) + (1, 5) = (3, 5)\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# ì˜ˆ3: (batch, seq, emb) vs (emb,)\ntoken_embeddings = torch.randn(2, 10, 768)  # (ë°°ì¹˜, í† í°, ì„ë² ë”©)\nbias = torch.randn(768)\nprint(f\"token_embeddings shape: {token_embeddings.shape}\")\nprint(f\"bias shape: {bias.shape}\")\nresult = token_embeddings + bias\nprint(f\"ê²°ê³¼ shape: {result.shape}\")\nprint(f\"â†’ ëª¨ë“  ë°°ì¹˜, ëª¨ë“  í† í°ì— ê°™ì€ bias ì ìš©ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Reshape vs View vs Permute ë¹„êµ\n",
    "\n",
    "í…ì„œ í˜•íƒœë¥¼ ë³€ê²½í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape vs View\n",
    "print(\"=== Reshape vs View ===\")\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\nprint(f\"ì›ë³¸: {x}, shape: {x.shape}\")\n",
    "\n",
    "# reshape: ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ë¬´ì‹œí•˜ê³  ì¬êµ¬ì„±\n",
    "reshaped = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{reshaped}\")\n",
    "\n",
    "# view: ë©”ëª¨ë¦¬ ì—°ì†ì„± í•„ìš” (ê°™ì€ ë©”ëª¨ë¦¬, ë‹¤ë¥¸ í•´ì„)\n",
    "viewed = x.view(3, 4)\n",
    "print(f\"\\nview(3, 4):\\n{viewed}\")\n",
    "\n",
    "print(\"\\nì¼ë°˜ì ìœ¼ë¡œ reshape()ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¸ ê°€ì§€ ë°©ë²•ì˜ ì°¨ì´\n",
    "print(\"\\n=== ì„¸ ê°€ì§€ ë³€í˜• ë©”ì„œë“œ ë¹„êµ ===\")\noriginal = torch.randn(2, 3, 4)\nprint(f\"ì›ë³¸ shape: {original.shape}\")\n\n# 1. reshape: ììœ ë¡œìš´ ì¬êµ¬ì„±\nreshaped = original.reshape(2, 12)  # (2, 3, 4) â†’ (2, 12)\nprint(f\"reshape(2, 12): {reshaped.shape}\")\n\n# 2. view: ë©”ëª¨ë¦¬ ì—°ì†ì„± í•„ìš” (ê°™ì€ ë°ì´í„° í•´ì„)\nviewed = original.view(2, 12)\nprint(f\"view(2, 12): {viewed.shape}\")\n\n# 3. permute: ì°¨ì› ìˆœì„œ ë³€ê²½ (ë°ì´í„° ì´ë™)\npermuted = original.permute(1, 0, 2)  # (2, 3, 4) â†’ (3, 2, 4)\nprint(f\"permute(1, 0, 2): {permuted.shape}\")\n\nprint(\"\\ní•µì‹¬: reshape/viewëŠ” ìˆ«ì ë°°ì—´, permuteëŠ” ì°¨ì› ìˆœì„œ ë³€ê²½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ì¢…í•© ì‹¤ì „ ì˜ˆì‹œ: GPT ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ê°œë…ì„ ì ìš©í•œ ì‹¤ì œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn.functional as F\n\nprint(\"=== ê°„ë‹¨í•œ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ êµ¬í˜„ ===\")\n\n# ì„¤ì •\nbatch_size = 2\nseq_length = 4\nemb_dim = 8\nn_heads = 2\nhead_dim = emb_dim // n_heads\n\nprint(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}\")\nprint(f\"ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_length}\")\nprint(f\"ì„ë² ë”© ì°¨ì›: {emb_dim}\")\nprint(f\"í—¤ë“œ ìˆ˜: {n_heads}\")\nprint(f\"í—¤ë“œ ì°¨ì›: {head_dim}\\n\")\n\n# ì…ë ¥ ë°ì´í„°\nX = torch.randn(batch_size, seq_length, emb_dim)\nprint(f\"ì…ë ¥ X shape: {X.shape}\")\n\n# ì„ í˜• ë³€í™˜ (Q, K, V ê³„ì‚°)\nW_q = torch.randn(emb_dim, emb_dim)\nW_k = torch.randn(emb_dim, emb_dim)\nW_v = torch.randn(emb_dim, emb_dim)\n\n# 1ë‹¨ê³„: ì„ í˜• ë³€í™˜\nQ = X @ W_q  # (2, 4, 8)\nK = X @ W_k  # (2, 4, 8)\nV = X @ W_v  # (2, 4, 8)\nprint(f\"\\n1ë‹¨ê³„: ì„ í˜• ë³€í™˜\")\nprint(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n\n# 2ë‹¨ê³„: í—¤ë“œë¡œ ë¶„í• \nQ = Q.reshape(batch_size, seq_length, n_heads, head_dim)  # (2, 4, 2, 4)\nK = K.reshape(batch_size, seq_length, n_heads, head_dim)\nV = V.reshape(batch_size, seq_length, n_heads, head_dim)\nprint(f\"\\n2ë‹¨ê³„: í—¤ë“œë¡œ ë¶„í• \")\nprint(f\"Q shape: {Q.shape}\")\n\n# 3ë‹¨ê³„: í—¤ë“œë³„ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ ìˆœì„œ ë³€ê²½\nQ = Q.permute(0, 2, 1, 3)  # (2, 2, 4, 4) - [ë°°ì¹˜, í—¤ë“œ, ì‹œí€€ìŠ¤, í—¤ë“œ_ì°¨ì›]\nK = K.permute(0, 2, 1, 3)\nV = V.permute(0, 2, 1, 3)\nprint(f\"\\n3ë‹¨ê³„: ì°¨ì› ì¬ë°°ì—´\")\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"ì´ì œ ê° í—¤ë“œë³„ë¡œ ë³‘ë ¬ ê³„ì‚° ê°€ëŠ¥!\")\n\n# 4ë‹¨ê³„: ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°\n# scores = Q @ K^T / sqrt(head_dim)\nK_T = K.transpose(-2, -1)  # (2, 2, 4, 4) ë§ˆì§€ë§‰ ë‘ ì°¨ì› ì „ì¹˜\nscores = Q @ K_T / (head_dim ** 0.5)  # (2, 2, 4, 4)\nprint(f\"\\n4ë‹¨ê³„: ì–´í…ì…˜ ìŠ¤ì½”ì–´\")\nprint(f\"K.transpose(-2, -1) shape: {K_T.shape}\")\nprint(f\"scores = Q @ K^T / sqrt(head_dim)\")\nprint(f\"scores shape: {scores.shape}\")\n\n# 5ë‹¨ê³„: Softmax\nweights = F.softmax(scores, dim=-1)  # (2, 2, 4, 4)\nprint(f\"\\n5ë‹¨ê³„: Softmax ê°€ì¤‘ì¹˜\")\nprint(f\"weights shape: {weights.shape}\")\n\n# 6ë‹¨ê³„: ê°’ì— ê°€ì¤‘ì¹˜ ì ìš©\noutput = weights @ V  # (2, 2, 4, 4) @ (2, 2, 4, 4) = (2, 2, 4, 4)\nprint(f\"\\n6ë‹¨ê³„: ê°’ ê°€ì¤‘ì¹˜ ì ìš©\")\nprint(f\"output = weights @ V\")\nprint(f\"output shape: {output.shape}\")\n\n# 7ë‹¨ê³„: í—¤ë“œ ë‹¤ì‹œ í•©ì¹˜ê¸°\noutput = output.permute(0, 2, 1, 3)  # (2, 4, 2, 4)\nprint(f\"\\n7ë‹¨ê³„: ì°¨ì› ì¬ë°°ì—´\")\nprint(f\"output.permute(0, 2, 1, 3) shape: {output.shape}\")\n\n# 8ë‹¨ê³„: í—¤ë“œ ì—°ê²°\noutput = output.reshape(batch_size, seq_length, emb_dim)  # (2, 4, 8)\nprint(f\"\\n8ë‹¨ê³„: í—¤ë“œ ì—°ê²°\")\nprint(f\"output.reshape(...) shape: {output.shape}\")\nprint(f\"\\nìµœì¢… ê²°ê³¼ shape: {output.shape} (ì…ë ¥ê³¼ ë™ì¼!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ ì—°ì‚° ë³µì¡ë„ ë¹„êµ\n",
    "\n",
    "ë‹¤ì–‘í•œ ì—°ì‚°ì˜ íš¨ìœ¨ì„± ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n\nprint(\"=== ì—°ì‚° ì†ë„ ë¹„êµ ===\")\n\n# í° í…ì„œ ìƒì„±\nbig_tensor = torch.randn(1000, 1000, 100)\nprint(f\"í…ŒìŠ¤íŠ¸ í…ì„œ shape: {big_tensor.shape}\")\n\n# 1. reshape\nstart = time.time()\nfor _ in range(100):\n    _ = big_tensor.reshape(1000, 100000)\nreshape_time = (time.time() - start) * 1000\nprint(f\"reshape (100íšŒ): {reshape_time:.2f}ms\")\n\n# 2. view\nlinear_tensor = big_tensor.clone()\nif linear_tensor.is_contiguous():\n    start = time.time()\n    for _ in range(100):\n        _ = linear_tensor.view(1000, 100000)\n    view_time = (time.time() - start) * 1000\n    print(f\"view (100íšŒ): {view_time:.2f}ms\")\n\n# 3. permute\nstart = time.time()\nfor _ in range(100):\n    _ = big_tensor.permute(2, 0, 1)\npermute_time = (time.time() - start) * 1000\nprint(f\"permute (100íšŒ): {permute_time:.2f}ms\")\n\nprint(\"\\nğŸ’¡ Insight: reshapeì™€ viewëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ë³€ê²½í•´ì„œ ë§¤ìš° ë¹ ë¥´ì§€ë§Œ,\")\nprint(\"   permuteëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ì´ë™ì‹œì¼œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ ì²´í¬ë¦¬ìŠ¤íŠ¸: í…ì„œ ì—°ì‚° ë§ˆìŠ¤í„°í•˜ê¸°\n",
    "\n",
    "ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ê°œë… í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===\")\nprint()\n\nchecklist = [\n    (\"âœ“\", \"Shape ì½ê¸°: (2, 4, 8)ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì„¤ëª… ê°€ëŠ¥\"),\n    (\"âœ“\", \"Indexing: x[0, :, :] vs x[:, 0, :] ì°¨ì´ì  ì´í•´\"),\n    (\"âœ“\", \"Squeeze/Unsqueeze: ì°¨ì› ì œê±°/ì¶”ê°€ ëª©ì  ì´í•´\"),\n    (\"âœ“\", \"Transpose: 2D í–‰ë ¬ ì „ì¹˜ì™€ permuteë¡œ ê³ ì°¨ì› ì „ì¹˜ ê°€ëŠ¥\"),\n    (\"âœ“\", \"Dot Product: ë²¡í„°, í–‰ë ¬, ë°°ì¹˜ ì—°ì‚° ëª¨ë‘ ê°€ëŠ¥\"),\n    (\"âœ“\", \"Broadcasting: í¬ê¸°ê°€ ë‹¤ë¥¸ í…ì„œ ì—°ì‚° ê·œì¹™ ì´í•´\"),\n    (\"âœ“\", \"Reshape vs View: ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ íŒë‹¨\"),\n    (\"âœ“\", \"ë©€í‹°í—¤ë“œ ì–´í…ì…˜: 8ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ì´í•´\"),\n]\n\nfor status, item in checklist:\n    print(f\"{status} {item}\")\n\nprint(\"\\nëª¨ë‘ ì´í•´í–ˆë‹¤ë©´ LLM ì½”ë“œë„ ì‰¬ì›Œì§‘ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì°¸ê³ : ch03, ch04ì˜ ì½”ë“œì—ì„œ ì´ëŸ° ê°œë…ë“¤ì´ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ê³³\n",
    "\n",
    "1. **ch03 - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**\n",
    "   - `torch.matmul()` ë° `@`: ì ê³±ìœ¼ë¡œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "   - `transpose()`: Queryì™€ Key ì „ì¹˜\n",
    "   - Broadcasting: ìŠ¤ì¹¼ë¼ ì—°ì‚° (ì˜¨ë„ ì¡°ì ˆ ë“±)\n",
    "\n",
    "2. **ch04 - GPT ëª¨ë¸**\n",
    "   - `reshape()`: ë©€í‹°í—¤ë“œ ë¶„í• \n",
    "   - `permute()`: í—¤ë“œ ì°¨ì› ì¬ë°°ì—´\n",
    "   - `squeeze()`: ë°°ì¹˜ 1ì¼ ë•Œ ì°¨ì› ì œê±°\n",
    "   - `unsqueeze()`: ë§ˆìŠ¤í¬ ì°¨ì› ì¶”ê°€\n",
    "\n",
    "3. **ch05 - í›ˆë ¨**\n",
    "   - ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì°¨ì› ìë™ í™•ì¥\n",
    "   - ì†ì‹¤ ê³„ì‚° ì‹œ ë°°ì¹˜/ì‹œí€€ìŠ¤ í‰ê· í™”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
