{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch `view()` 함수 - 완벽 이해하기\n",
    "\n",
    "`view()`는 텐서의 **형태를 바꾸는 함수**입니다. 멀티헤드 어텐션에서 중요하게 사용됩니다.\n",
    "\n",
    "## 🎯 기본 원칙\n",
    "\n",
    "```python\n",
    "tensor.view(new_shape)\n",
    "# 전체 원소 개수는 유지하면서 형태만 변경\n",
    "```\n",
    "\n",
    "**핵심**: 원소 개수가 같으면 어떤 형태든 변경 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch view() 함수 튜토리얼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch view() 함수 튜토리얼\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 1️⃣: 1D → 2D 변환\n",
    "\n",
    "가장 간단한 1차원 배열을 2차원으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텐서: tensor([1, 2, 3, 4, 5, 6])\n",
      "원본 shape: torch.Size([6])\n",
      "원소 개수: 6개\n",
      "\n",
      "변경 후 (2, 3):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "새로운 shape: torch.Size([2, 3])\n",
      "원소 개수: 6개 (동일!)\n"
     ]
    }
   ],
   "source": [
    "# 원본 (6개 원소)\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(f\"원본 텐서: {x}\")\n",
    "print(f\"원본 shape: {x.shape}\")\n",
    "print(f\"원소 개수: {x.numel()}개\")\n",
    "\n",
    "# 2D로 변경\n",
    "x_2d = x.view(2, 3)\n",
    "print(f\"\\n변경 후 (2, 3):\")\n",
    "print(x_2d)\n",
    "print(f\"새로운 shape: {x_2d.shape}\")\n",
    "print(f\"원소 개수: {x_2d.numel()}개 (동일!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**시각적으로:**\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6]  (6개 원소)\n",
    "       ↓\n",
    "[[1, 2, 3],\n",
    " [4, 5, 6]]         (2×3 = 6개 원소)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 2️⃣: 2D → 3D 변환\n",
    "\n",
    "더 복잡한 형태로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: shape=torch.Size([12]), 원소=12개\n",
      "\n",
      "view(2, 6):\n",
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12]])\n",
      "\n",
      "view(2, 2, 3):\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "shape: torch.Size([2, 2, 3])\n",
      "\n",
      "view(3, 2, 2):\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n",
      "shape: torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 12개 원소를 가진 1D 배열\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(f\"원본: shape={x.shape}, 원소={x.numel()}개\")\n",
    "\n",
    "# (2, 6) 형태로\n",
    "x_2d = x.view(2, 6)\n",
    "print(f\"\\nview(2, 6):\")\n",
    "print(x_2d)\n",
    "\n",
    "# (2, 2, 3) 형태로 (12개)\n",
    "x_3d = x.view(2, 2, 3)\n",
    "print(f\"\\nview(2, 2, 3):\")\n",
    "print(x_3d)\n",
    "print(f\"shape: {x_3d.shape}\")\n",
    "\n",
    "# (3, 2, 2) 형태로\n",
    "x_3d_alt = x.view(3, 2, 2)\n",
    "print(f\"\\nview(3, 2, 2):\")\n",
    "print(x_3d_alt)\n",
    "print(f\"shape: {x_3d_alt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**중요한 관찰:**\n",
    "- 같은 12개 원소도 (2,3), (2,2,3), (3,2,2) 등 다양한 형태로 변환 가능\n",
    "- 원소 개수: 2×6 = 2×2×3 = 3×2×2 = 12개 (모두 동일)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 3️⃣: -1 사용 (자동 계산)\n",
    "\n",
    "`-1`은 \"나머지 차원을 자동으로 계산해줘\"라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: shape=torch.Size([2, 3, 4])\n",
      "원소 개수: 24개\n",
      "\n",
      "view(-1, 6): shape=torch.Size([4, 6])\n",
      "계산: 24 = ? × 6 → ? = 4\n",
      "\n",
      "view(-1): shape=torch.Size([24])\n",
      "모든 원소가 1D로 펴짐\n",
      "\n",
      "❌ 두 개의 -1 사용 불가: only one dimension can be inferred\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(f\"원본: shape={x.shape}\")\n",
    "print(f\"원소 개수: {x.numel()}개\\n\")\n",
    "\n",
    "# -1로 자동 계산\n",
    "x_reshaped = x.view(-1, 6)  # (?, 6)\n",
    "print(f\"view(-1, 6): shape={x_reshaped.shape}\")\n",
    "print(f\"계산: 24 = ? × 6 → ? = 4\\n\")\n",
    "\n",
    "# 1D로 평탄화\n",
    "x_flat = x.view(-1)\n",
    "print(f\"view(-1): shape={x_flat.shape}\")\n",
    "print(f\"모든 원소가 1D로 펴짐\\n\")\n",
    "\n",
    "# 여러 -1 사용 (한 번만 가능)\n",
    "try:\n",
    "    x.view(-1, -1)  # 두 개의 -1 ❌\n",
    "except RuntimeError as e:\n",
    "    print(f\"❌ 두 개의 -1 사용 불가: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 4️⃣: 멀티헤드 분할 (코드의 실제 사용)\n",
    "\n",
    "멀티헤드 어텐션에서 사용되는 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 queries:\n",
      "shape: torch.Size([1, 3, 2])  (batch=1, tokens=3, d_out=2)\n",
      "원소 개수: 6개\n",
      "\n",
      "분할 후 queries_multi:\n",
      "shape: torch.Size([1, 3, 2, 1])  (batch=1, tokens=3, heads=2, head_dim=1)\n",
      "원소 개수: 6개 (동일!)\n",
      "\n",
      "토큰 0을 헤드별로 확인:\n",
      "헤드 0: 0.10000000149011612\n",
      "헤드 1: 0.30000001192092896\n",
      "\n",
      "원본에서: tensor([0.1000, 0.3000])\n",
      "→ 첫 값(0.1)은 헤드 0, 두 번째 값(0.3)은 헤드 1으로 분할됨!\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 벡터 (멀티헤드 어텐션)\n",
    "queries = torch.tensor([[[0.1, 0.3],\n",
    "                         [0.2, 0.4],\n",
    "                         [0.3, 0.5]]], dtype=torch.float32)\n",
    "\n",
    "print(\"원본 queries:\")\n",
    "print(f\"shape: {queries.shape}  (batch=1, tokens=3, d_out=2)\")\n",
    "print(f\"원소 개수: {queries.numel()}개\\n\")\n",
    "\n",
    "# 멀티헤드로 분할\n",
    "# d_out=2를 (num_heads=2, head_dim=1)로 분할\n",
    "queries_multi = queries.view(1, 3, 2, 1)\n",
    "\n",
    "print(\"분할 후 queries_multi:\")\n",
    "print(f\"shape: {queries_multi.shape}  (batch=1, tokens=3, heads=2, head_dim=1)\")\n",
    "print(f\"원소 개수: {queries_multi.numel()}개 (동일!)\\n\")\n",
    "\n",
    "print(\"토큰 0을 헤드별로 확인:\")\n",
    "print(f\"헤드 0: {queries_multi[0, 0, 0, 0].item()}\")\n",
    "print(f\"헤드 1: {queries_multi[0, 0, 1, 0].item()}\")\n",
    "print(f\"\\n원본에서: {queries[0, 0, :]}\")\n",
    "print(\"→ 첫 값(0.1)은 헤드 0, 두 번째 값(0.3)은 헤드 1으로 분할됨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ 주의사항 1️⃣: 원소 개수 확인\n",
    "\n",
    "원소 개수가 맞지 않으면 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: shape=torch.Size([2, 3]), 원소=6개\n",
      "\n",
      "✓ view(3, 2): shape=torch.Size([3, 2]) (6개 = 6개)\n",
      "\n",
      "❌ view(2, 4) 에러:\n",
      "   shape '[2, 4]' is invalid for input of size 6\n",
      "\n",
      "   이유: 6개 원소를 8개 크기로 변환 불가!\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)  # 6개 원소\n",
    "print(f\"원본: shape={x.shape}, 원소={x.numel()}개\\n\")\n",
    "\n",
    "# 올바른 변환\n",
    "x_ok = x.view(3, 2)  # 3×2 = 6개 ✓\n",
    "print(f\"✓ view(3, 2): shape={x_ok.shape} (6개 = 6개)\")\n",
    "\n",
    "# 잘못된 변환\n",
    "try:\n",
    "    x_error = x.view(2, 4)  # 2×4 = 8개 ❌\n",
    "    print(\"이 줄은 실행되지 않음\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n❌ view(2, 4) 에러:\")\n",
    "    print(f\"   {e}\")\n",
    "    print(f\"\\n   이유: 6개 원소를 8개 크기로 변환 불가!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ 주의사항 2️⃣: contiguous() - 메모리 배치\n",
    "\n",
    "transpose 후에는 메모리 배치가 바뀌어 view()가 실패할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "print(f\"원본: shape={x.shape}\")\n",
    "print(f\"is_contiguous: {x.is_contiguous()}\\n\")\n",
    "\n",
    "# transpose 후\n",
    "y = x.transpose(0, 1)  # (4, 3, 5)\n",
    "print(f\"transpose(0,1) 후: shape={y.shape}\")\n",
    "print(f\"is_contiguous: {y.is_contiguous()}  ← 이제 불연속!\\n\")\n",
    "\n",
    "# 직접 view() 시도 (실패할 수 있음)\n",
    "try:\n",
    "    z = y.view(12, 5)  # 시도\n",
    "    print(f\"✓ view(12, 5) 성공: {z.shape}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"❌ view(12, 5) 실패: {e}\\n\")\n",
    "    print(\"해결책: contiguous() 사용\")\n",
    "    y_contiguous = y.contiguous()\n",
    "    z = y_contiguous.view(12, 5)\n",
    "    print(f\"✓ contiguous().view(12, 5) 성공: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 비교: view() vs reshape() vs flatten()\n",
    "\n",
    "각 함수의 특징과 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(f\"원본: shape={x.shape}\\n\")\n",
    "\n",
    "# 1. view() - 빠름, 연속 메모리 필요\n",
    "try:\n",
    "    x_view = x.view(6, 4)\n",
    "    print(f\"✓ view(6, 4): {x_view.shape}\")\n",
    "except RuntimeError:\n",
    "    print(f\"❌ view(6, 4): 실패\")\n",
    "\n",
    "# 2. reshape() - 느림, 자동 contiguous 처리\n",
    "x_reshape = x.reshape(6, 4)\n",
    "print(f\"✓ reshape(6, 4): {x_reshape.shape}\")\n",
    "\n",
    "# 3. flatten() - 1D로 평탄화만\n",
    "x_flatten = x.flatten()\n",
    "print(f\"✓ flatten(): {x_flatten.shape}\")\n",
    "\n",
    "print(\"\\n특징 비교:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'함수':<15} {'속도':<10} {'특징'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'view()':<15} {'빠름':<10} '연속 메모리 필요'\")\n",
    "print(f\"{'reshape()':<15} {'느림':<10} '자동 처리, 권장'\")\n",
    "print(f\"{'flatten()':<15} {'중간':<10} '1D로만 변환'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 종합 실습\n",
    "\n",
    "여러 시나리오를 한 번에 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"다양한 head_dim 시나리오 비교\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scenarios = [\n",
    "    (\"현재 코드\", 2, 2, 1),\n",
    "    (\"변형 1\", 4, 2, 2),\n",
    "    (\"변형 2\", 8, 4, 2),\n",
    "    (\"GPT-124M\", 768, 12, 64),\n",
    "]\n",
    "\n",
    "for name, d_out, num_heads, expected_head_dim in scenarios:\n",
    "    head_dim = d_out // num_heads\n",
    "    assert head_dim == expected_head_dim, f\"head_dim 계산 오류: {head_dim} != {expected_head_dim}\"\n",
    "    \n",
    "    # 더미 데이터 생성\n",
    "    batch_size = 1\n",
    "    num_tokens = 3\n",
    "    queries = torch.randn(batch_size, num_tokens, d_out)\n",
    "    \n",
    "    # 분할\n",
    "    queries_multi = queries.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "    \n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"  d_out={d_out}, num_heads={num_heads}, head_dim={head_dim}\")\n",
    "    print(f\"  분할 공식: {d_out} = {num_heads} × {head_dim}\")\n",
    "    print(f\"  분할 전: {queries.shape}\")\n",
    "    print(f\"  분할 후: {queries_multi.shape}\")\n",
    "    print(f\"  원소 개수: {queries.numel()} → {queries_multi.numel()} (동일 ✓)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 핵심 정리\n",
    "\n",
    "### view() 함수의 3가지 원칙\n",
    "\n",
    "1. **원소 개수 유지**\n",
    "   - view() 전후로 총 원소 개수는 같아야 함\n",
    "   - (2, 3, 4) = 24개, view(6, 4) = 24개 ✓\n",
    "\n",
    "2. **메모리 연속성 확인**\n",
    "   - transpose() 후에는 contiguous() 필수\n",
    "   - 그렇지 않으면 RuntimeError 발생 가능\n",
    "\n",
    "3. **멀티헤드 분할에 사용**\n",
    "   - d_out을 (num_heads × head_dim)으로 분할\n",
    "   - 각 헤드가 독립적으로 동작하게 함\n",
    "\n",
    "### 사용 권장사항\n",
    "\n",
    "| 상황 | 추천 함수 |\n",
    "|------|----------|\n",
    "| 메모리 배치 확실함 | `view()` |\n",
    "| 메모리 배치 불명확 | `reshape()` |\n",
    "| 1D로만 변환 | `flatten()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 확인: 원본과 같은 값인지 검증\n",
    "print(\"\\n최종 검증: 분할 후 복원\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "original = torch.randn(1, 3, 8)\n",
    "print(f\"원본: shape={original.shape}\")\n",
    "\n",
    "# 8 = 4 헤드 × 2 head_dim\n",
    "reshaped = original.view(1, 3, 4, 2)\n",
    "print(f\"분할: shape={reshaped.shape}\")\n",
    "\n",
    "# 다시 복원\n",
    "restored = reshaped.view(1, 3, 8)\n",
    "print(f\"복원: shape={restored.shape}\")\n",
    "\n",
    "# 값이 동일한지 확인\n",
    "is_same = torch.allclose(original, restored)\n",
    "print(f\"\\n원본과 복원된 값이 동일? {is_same} ✓\")\n",
    "\n",
    "if is_same:\n",
    "    print(\"✅ view()는 단순히 형태만 변경하고 값은 보존합니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
